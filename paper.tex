\documentclass{llncs}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage[dvips]{epsfig}
\usepackage[latin1]{inputenc}

\def\CC{{C\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}~}
\addtolength{\textfloatsep}{-0.5cm}
\addtolength{\intextsep}{-0.5cm}


%%%%%%%%%%%%%%%% Titulo %%%%%%%%%%%%%%%
\title{Predicting sales of newly published books} 

%%%%%%%%%%%%%%%% autores %%%%%%%%%%%%%%%
\author {
P.A. Castillo, A.M. Mora, J.J. Merelo, P. Garc\'{\i}a-S\'anchez, \\ A.J. Fern\'andez-Ares, M.G. Arenas, P. de las Cuevas
}
\institute{Department of Architecture and Computer Technology. CITIC \\
           University of Granada (Spain) \\
~\\
           e-mail: {\tt pacv@ugr.es}}

\date{} 

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
When a new book is published the publisher faces the problem of
how many books should be printed for delivery to bookstores. Printing too many is the main issue,
since it implies a loss of investment due to inventory excess. In this
paper we are tackling the problem of predicting total sales in order to 
print the right amount of books, doing so even before the book has
reached shelves. 
In this paper, which works with several years of data on published books in
Spain, we first apply a data visualization and preprocessing strategy
in order to find
out what are the variables that have some impact on sales, and
then use standard data mining software to predict sales, and from them
deduce print-run, based on those variables. Our work focuses then in  discovering which
are the most relevant variables when predicting sales and is
eventually able to predict sales with remarkable accuracy. In fact,
this method has been validated by the company that provided the data 
and incorporated in a business intelligence tool for publishers,
since the obtained models provide guidance on the decision process of
publishing a book and do not require human intervention either for the operation or for
interpretation.
\end{abstract}


%********************************************************************************
\section{Introduction}


Publishing a book, as releasing any other product, implies several
types of risks and costs due to
the complexity and expenses of the production and distribution processes. 
% tienes que enmarcar el problema. Se trata de productos culturales
% con soporte físico y con ventas relativamente pequeñas. Libros,
% discos, no sé si alguna cosa más. - JJ
A newly launched product is also new to the company, and sales forecasts can
only be based on the experience of similar products. 
In this scope, having accurate estimates of future sales of the new
product is extremely important \cite{ChingChin2010} so that production
runs do not exceed predicted sales. 
Indeed, several studies have demonstrated that improving sale predictions 
(a reduction of forecasting errors) results in an improvement in the 
production \cite{Bayraktar2008,Fildes2010,Saeed2008,Zhao2002}.

% In this sense, the publishing industry is based on very particular and
% established production process, and publishing a book requires several
% steps, between which the distribution is important and plays an
% important role. The publishing process and individual decisions at each step are very
% important. One of those decisions is the number of copies to print. 
  % ¿Esto que tiene que ver con la frase anterior? - JJ
% Thus, a suitable prediction system for new books should take into
% account many details about how the editorial process works. % y? - JJ

% Implementing a good forecasting tool requires not only a good study of
% the state of the art in this field, but also an accurate knowledge of
% the operations and challenges facing the publishing industry and its
% supply chain. % ¿eso es lo que vamos a hacer? - JJ

Book publishers, as well as other content producers, release new {\em
  products} more frequently than other industries and thus face the
problem of predicting sales quite often. The issue, in this case, is to print an
adequate number of copies but not too many, as the unsold volumes will
lead to losses and sunk inventory cost, whereas if the number of printed copies was not
enough, you can always print more. In this sense, errors in one or the
other direction do not have the same importance, although a temporal
scarcity of a book that is selling well is also a problem.

Thus, it is extremely important using predictive tools to optimize 
production and improve the publishing company profits or minimize losses \cite{Zhao2001}
by forecasting the sales a new book might achieve. This is the
objective we have in this paper.
An example is the optimization of sales \cite{Mattila2002} by
improving profit margins by selling the whole print-run. %publishing roll.  
% [Pedro] "publishing roll" es como he encontrado "tirada editorial" traducido...
% Antonio propone traducirlo como "print run". Lo pongo así.

However, accurately using a predictive tool is a difficult task as
book sales are influenced by various factors that make
them fluctuate and usually they might be not controllable and
sometimes unknown. 
Some have to do with decisions on purchases, others with freight %FERGU: El Some have to do me suena raro
traffic, and others with the distribution strategy \cite{Little1998}. 

Thus, it is very important to know the product and the variables that
might affect the sales, and how an expert manager can eventually 
take them into account \cite{Armstrong2001,SThomassey2014}. 
                                % ¿Esto también pasa en la industria de la
                                % moda. ¿Qué vamos a usar de esto? - JJ
                                % [Pedro] esto iba referido a la Industria Editorial.
Their impact is difficult to estimate and it is not constant over time, 
hence the difficulty to adequately identify and measure their influence \cite{DeToni2000}.
In addition, some may have correlations between them. 
However, these correlations can be detected using data analysis methods, 
such as Self-Organizing Maps (SOM) \cite{kohonen1998} 
or stepwise regression analysis (SRA) \cite{Chang2009}. % What do they do? What are the results?

In this research, the problem of how many books should be printed 
when a new book is published is faced using data provided by the 
Spanish publishing company Trevenque Editorial \footnote{{\tt http://editorial.trevenque.es}},
a company that provides management systems and services for libraries, 
publishers and distributors. 
Usually, the company's expert sales managers analyzed historical data 
and applied their knowledge using external variables to calculate
appropriate predictions, for example, increasing or decreasing the 
sales prediction based on season or sales period.
                        % varias citas - JJ
                        % [Pedro] lo modifico para comentar quienes nos facilitaron los datos
                        %  y que así era como funcionaban hasta ese momento.
%In any case, in general companies usually use a forecasting model and then, 
%an expert applies his knowledge, as the company might require an 
%interpretable and understandable model. 
However, carrying out the analysis and forecasting process as described,
may lead to an imprecise analysis and may not take all the variables into account. 
In addition, the process might be tedious, especially when the amount
of data is large, and even, different experts can reach different
predictions from the same data set \cite{ChingChin2010,Sanders1994}.

For these reasons, the industry requires the best suited forecasting
techniques to be adopted so that, estimating sales task is carried out
properly and in an automatic way. 

With this in mind, in this paper a data visualization and preprocessing strategy 
to find out what are the relevant variables describing a book in the prediction 
of book sales is proposed; then several standard data mining models 
for sales forecasting are evaluated so a publishing company 
can deduce and optimize print-run.

The rest of this paper is structured as follows: next, 
Section \ref{sec:soa} presents a comprehensive review of the
approaches found in the bibliography related to the prediction of
sales from product characteristics. 
Then, Section \ref{sec:problem} describes the 
problem of how many books should be printed when a new book is published, 
        % no es el problema de publicación, es el problema de venta
        % de libros sin publicar. Habrá que buscar un nombre - JJ
        %[Pedro] cierto, lo cambio, aunque me ha quedado un nombre muy largo  :)
 followed by Section \ref{sec:methodology} where the
experimental setup and the methodology considered in the study is
presented.  %FERGU: mejor described para no repetir
Section \ref{sec:experiments} reports and analyzes the obtained results with a 
real data set. Finally, conclusions and future work are presented in Section
\ref{sec:conclusionsAndFutureWork}. 


%********************************************************************************
\section{State of the art on sales forecasting} %FERGU: State of the art on sales forecasting?  ok  ;)
\label{sec:soa}

% [Pedro] no se encuentra gran cosa sobre predicción de ventas de libros
%  por eso quiero comenzar hablando de la aplicación de métodos
%  de predicción en la industria en general, que sí hay bastante bibliografía.

% In the literature several proposals to perform the prediction of sales in different fields can be found. 
In the last years, different analysis and forecasting methods, such as
regression models \cite{Papalexopoulos1990}, neural networks \cite{Yoo1999} 
or fuzzy systems \cite{Mastorocostas2001} 
have been applied in different industry sectors, i.e. electricity
consumption, traffic flow or sales forecasting. % ¿Y qué? Esta frase
                                % no aporta nada. La primera frase
                                % tiene que enmarcar el estado del
                                % arte - JJ

Specifically, using time series prediction methods \cite{Chu2003,Brown1959,Winters1960,Box1969,Papalexopoulos1990} 
is perhaps the most used technique to tackle the sales forecasting problems. 
%FERGU: terminar esta frase con punto en vez de coma para que no sea larga.   OK
The efficiency of these techniques depends largely on the field of application 
and the problem data as they require a large amount of data for predicting 
sales of new products are not the most suitable for this task \cite{ChingChin2010}.

This is the new book sales forecasting case, a problem with issues such as 
dealing with limited data, finding adequate forecasting methods and selecting 
the best method to use \cite{ChingChin2010}.
In the case of new unpublished-books, there is no historical data to define 
patterns, and only some descriptive data about the new book is available 
\cite{ChingChin2010,FaderHardie2005,Madsen2008}.
Thus, not any timeseries-based forecasting method can be used \cite{Madsen2008,ChernWSF15}. 

%This is the new book sales forecasting case, for which no historical data to make predictions is available \cite{ChingChin2010,FaderHardie2005,Madsen2008} and only some descriptive variables relative to the new unpublished-book are available.

%Forecasting sales of new books is a problem with issues such as dealing with limited data, finding adequate forecasting methods and selecting the best method to use \cite{ChingChin2010}.

%In the case of already published books, available historical data might help to generate predictive models.
%However, in the case of new books there is no historical data to define patterns, and only some descriptive data about the new book is available. 
%In these cases not any timeseries-based forecasting method can be used \cite{Madsen2008,ChernWSF15}. 

%An additional issue is the difficulty to compare our model with classical forecasting methods, such as time series prediction methods, since books do not have specific sales time series.

Methods used to predict book sales have been generally based on experts' 
experience who analyzed data about sales, and taking into account their knowledge 
in industry experience and their perception about trends, could make decisions
on the companies production (how many books should be printed when a new book is published).
This a complex problem as the predictions are influenced by external variables 
that must be taken into account, such as seasonality, promotions or fashions 
that expert managers might subjectively apply \cite{Lapide1999,ChingChin2010,ChernWSF15}.

  %[pedro] sobre la industria editorial no hay trabajos sobre predicción de ventas.
  % Lo habitual ha sido usar el conocimiento de expertos para realizar predicciones 
  %  o aconsejar acerca de la cantidad de libros a imprimir.
  % Lo más parecido que he encontrado son dos trabajos (uno en realidad) de Moon et al. 
  %  sobre "Use Blog Information As Book Sales Prediction":
  %     Moon2010ICSSSM
  %     Moon2010ICEC
Recently, some works propose analyzing relationships between on-line information 
and book sales \cite{Moon2010ICSSSM,Moon2010ICEC}.
Specifically, authors propose using the number of blog references as an indicator 
of book sales.
Different sales patterns are observed by analyzing data obtained.
In any case, Moon et al \cite{Moon2010ICSSSM,Moon2010ICEC} used historical data from published books, 
%FERGU: a qué research se refiere? A la nuestra o a la de la referencia? Dejarlo más claro, por si acaso.   ok
and the proposed methodology can not be usable in the problem of unpublished-books 
sales prediction.

%
% pasar a contar los casos de la predicción de ventas en la 
% industria de la moda (SThomassey2014) y de nuevos productos (ChingChin2010)
%
% Son problemas muy similares, ya que no hay datos históricos de los nuevos productos,
% al igual que no los hay sobre ventas, y en los que además influyen mucho las variables 
% externas (estacionalidad, modas, fama del author, etc).
% Hay que echar mano de productos similares o simplemente de los datos que describen el nuevo producto.
%

Some authors have faced the problem of forecasting sales of new products using 
different data mining methods \cite{Hammond1990,Chang2009,ChingChin2010,Thomassey2012,Xia2012}. 
Thomassey et al \cite{SThomassey2014} propose using a clustering method and 
decision trees for sales forecasting in textile-apparel fashion industry, %FERGU: appared? apparel, creo.  ¡sí!
a very similar issue to the new book publishing problem.
The same problem was previously faced by Xia et al \cite{Xia2012} using a 
forecasting method based on extreme learning machine with adaptive metrics of inputs.
Complexity of this problem is due to the lack of historical data, short lifetime 
of the large number of items, and due to the influence of variables such as 
promotions, fashions, or economic environment \cite{Thomassey2012,Xia2012,SThomassey2014}.

  %[Pedro] otro caso muy similar es el paper de ChingChin2010 sobre:
  % New tea product
  % New cosmetic product
  % New soft drink product
In \cite{ChingChin2010}, a decision-support system for new product sales %FERGU: The work of Chinchin et al.?
forecasting is proposed to solve three real-world sales forecasting problems 
involving new tea, cosmetic, and soft drink products taking into account 
quantitative variables. 
In this study, it is assumed that products are classified based on their sales 
pattern so that products in the same class would have a similar sales pattern. %FERGU: "_a_ similar sales pattern", mejor? ok
However, as the authors state, proposed system can not deal with qualitative 
date related to the products, and even more real-world cases, such as consumer 
electronics and fashion products, should be examined, as these industries 
introduce new products every season. 

  %[Pedro] otro caso a comentar es el paper de Chang2009 sobre: 
  % sales forecasting in printed circuit board industry.
In those cases where historical data is available, classical time series 
forecasting methods can be applied, which is what they do in \cite{Chang2009}, where a hybrid 
model integrating K-means and fuzzy neural networks to forecast the future sales 
of a printed circuit board is proposed.
A similar method is proposed by Chern et al \cite{Chern2015}, who analyze 
historical data along online reviews, reviewer characteristics and review 
influences to understand how electronic word-of-mouth affects product sales. 
The proposed method is suitable in those sales forecasting problems with abundant %FERGU: _The_ proposed method?    ok
online reviews and historical data.

Taking into account the kind of problem we address in this paper,
prediction of sales of 
a new product for which no historical data is available,
the classical forecasting methods based on time series are not
adequate, although they can obviously be used once the book has been
for sale. 
Thus, in this paper the relevant variables in the prediction of sales are determined
using data visualization tools and then, standard data mining software 
to predict sales is used to deduce print-run.
In addition, since publishers need models to explain obtained predictions, 
black box forecasting methods may not be suitable for this problem, so 
models based on rules are tested as an alternative.



%********************************************************************************
\section{The problem}
\label{sec:problem}


The main issue of this paper is to forecast sales for 
unpublished books for which no historical data is available, just some
descriptive variables relative to the new book. In order to do that,
training data includes historical sales data for all books published
in Spain for several years, including those descriptive variables. 

%%% Principio de párrafos a eliminar o mover a estado del arte.
%% Fin de párrafos a eliminar.
  % [pedro] eliminados

The initial data set, provided in the framework of a contract by
Trevenque Editorial, included 3159 books, and included descriptive information 
for each book, such as price, points of sale, subject, publisher, sales, returns, 
among other information.
Table \ref{tabla:paramsOrig50} shows the original set of parameters describing 
each book.

\begin{table}[!h]
\caption{Parameters describing a book, as provided by the company Trevenque.}
\label{tabla:paramsOrig50}
\begin{center}
\begin{tabular}{|c|c|}
\hline 

\begin{minipage}{2.45in} \begin{verbatim}
reference
author
retail pricer
subject1
subject2
subject3
editorial
collection
bookbinding
print-run
total sales
dept stores sales
sales through delegates
rest of sales
total sales 1st year
mall sales 1st year
delegates sales 1st year
rest of sales 1st year
total distributed
distributed through dept stores
distributed through delegates
distributed - rest
total distributed 1st year
distrib. through dept stores 1st year
distrib. through delegates 1st year
\end{verbatim} \end{minipage}     & 

\begin{minipage}{2.3in} \begin{verbatim}
rest of distributed 1st year
reprints
number of reprints
total returns
returns - dept stores
delegates returns
rest of returns
total returns firt year
returns -  dept stores 1st year
delegates returns 1st year
rest of returns 1st year
gifts
units distributed as novelty
total points of sale
points of sale - dept stores
delegates points of sale
rest of points of sale
total points of sale 1st year
dept stores points of sale 1st year
delegates points of sale 1st year
rest of points of sale 1st year
weeks for sale
positive sales environment
medium sales environment
negative sales environment
\end{verbatim} \end{minipage}    \\

\hline
\end{tabular}
\end{center}
\end{table}

In this prediction problem there are other external variables that also affect 
the sales of a new book, such as those related to seasonality, trends or 
promotions, that classical prediction methods can not deal with.
However, an expert can consider them to modify the forecast obtained by 
the automatic method. Even publishing companies tend to define rules or 
indexes to subjectively adjust the results of prediction based on
external data. %so what? 


%********************************************************************************
\section{Methodology}
\label{sec:methodology}

There are several issues inherent to the problem of predicting sales of new 
products: the limitation on the amount of data available, the application of 
different methods and choose the most suitable of them.
In addition, there is no a standard methodology to solve the general problem 
of sales forecasting. %FERGU: hacer más hincapié de esto en la intro entonces
Usually, a preliminary analysis of the data is carried out to eliminate those 
variables that describe the product but do not provide useful information during 
the prediction process.

\begin{figure}[!h] 
\begin{center}
  \epsfig{file=stages.eps,width=7cm}
\caption{\small{Three main stages in a typical sales prediction process: data analysis, (2) forecasting 
methods testing, and (3) subjectively adjusting the obtained results.}}
\label{fig:stages}
\end{center}
\end{figure}

Thus, in this study two main stages are proposed: 
\begin{itemize}

  \item  First, after the editorial company has provided the raw data, taken 
  directly from its database, a feature selection is carried out, as it is 
  an effective dimensionality reduction technique and an essential preprocessing 
  method to remove noise features \cite{Krishnapuram2004}. The idea of these 
  algorithms is searching the space of attribute subsets to find out which one 
  in the data works best for classification or prediction.
  In this stage, the information obtained using several feature selection 
  methods as well as analyzing the SOM cluster method results is used to 
  remove some unrelated variables and selects the best variables to be 
  considered in the forecasted model. % we will use pre-sales data.
  This task may help to identify leading indicators to be used by the 
  forecasting methods in order to improve the accuracy of future forecasts.

  \item  Then, several forecasting methods have to be tested. In this stage 
  different forecasting methods are used to obtain sales forecasts from the 
  data provided.

\end{itemize}

In some cases, depending on the company needs and after obtaining the forecasting 
results, it is necessary a third stage where experienced 
managers adjust subjectively the results obtained using the forecasting methods 
(see Figure \ref{fig:stages} for details). 
They might manually adjust the forecasting results taking into account their 
knowledge in industry experience and their perception about trends.

%Finally, although different authors have proposed metrics to determine 
%the accuracy of estimates, most companies use metrics based on the analysis 
%of errors, i.e. MAE or RMSE \cite{ChingChin2010,Madsen2008}.

Finally, in the literature different error measures can be found to report 
the results obtained using the prediction methods \cite{ChingChin2010,Madsen2008}. 
Those error measurements are also called error functions or cost functions. 
%One of the most commonly used is the mean absolute error (see equation \ref{eq:MAE}).
In any case, we recommend using widely accepted standard 
equations\footnote{http://www.gepsoft.com/gxpt4kb/Chapter10/Section3/Introduction.htm}
% FERGU: mejor tratar esto como una referencia, ya que es un texto
% Efectivamente- JJ
in order to compare obtained results with those presented in other research 
papers using other methods.
That is why different standard error measurements (Equations \ref{eq:MAE} to \ref{eq:RRSE}), 
implemented in tools like Weka\footnote{http://wiki.pentaho.com/display/DATAMINING} \cite{Witten2011} 
or R\footnote{https://www.otexts.org/fpp/2/5} \cite{Hyndman2013}, are
proposed.
% no se deben usar footnotes donde debiera haber referencias. - JJ
Specifically, in order to compute the forecasting errors of the different methods, 
%FERGU: quitar este salto de linea y unirlo al párrafo de arriba.    ok
Mean absolute error (MAE), Root mean squared error (RMSE), Relative absolute 
error (RAE) and Root relative squared error (RRSE) have been selected:

\begin{itemize}
  \item \emph{Mean Absolute Error} (MAE):
        \begin{equation}\label{eq:MAE}
            MAE = \frac{1}{n}\sum_{i=1}^n {\mid p_i - o_i\mid}
        \end{equation}

  \item \emph{Root Mean Squared Error} (RMSE):
        \begin{equation}\label{eq:RMSE}
            RMSE = \sqrt{ \frac{1}{n}\sum_{i=1}^n {(p_i - o_i)}^2 }
        \end{equation}

  \item \emph{Relative absolute error} (RAE):
        \begin{equation}\label{eq:RAE}
            RAE = \frac{ \sum_{i=1}^n {\mid p_i - o_i\mid} }{ \sum_{i=1}^n {\mid p_{i-1} - o_i\mid} }
        \end{equation}

  \item \emph{Root relative squared error} (RRSE):
        \begin{equation}\label{eq:RRSE}
            RRSE = \sqrt{ \frac{ \sum_{i=1}^n {(p_i - o_i)}^2  }{ \sum_{i=1}^n {(p_{i-1} - o_i)}^2 }  }
        \end{equation}
\end{itemize}
where $o_i$ is the individual data $i = {1,...,n}$ and $p_i$ is the obtained prediction.


%-------------------------------------------------
\subsection{Data collection and analysis}

In general, historical data is used to make time series, and from them extracting 
common patterns and obtaining accurate predictions.
However, in the case of new products, such as the issue of new books, past patterns 
is difficult to observe as there is no available historical sales data 
\cite{ChingChin2010} (only pre-sales data is available in this problem).

In addition, among the variables describing a new book, some may affect 
predictions (those variables provide information), while others may not be 
necessary and even redundant.


Moreover, the algorithm runing times grow with the number of features, and thus 
making the algorithm impractical for problems with a large number of features 
\cite{Selvakuberan2008}. Feature selection task is usually carried out by 
searching through all possible combinations of features, evaluating each one. 

During the evaluation, the set of all possible features is analyzed in order 
to find the best set of features, ranking them according to some metric.
In this process, the feature's predictive ability along with the degree 
of redundancy between them is considered.

Thus, in order to reduce the dimensionality of the input data and to choose an 
appropriate set of variables, both the SOM \cite{kohonen1998} method and several 
feature selection methods have been applied.


SOM is a feedforward neural network \cite{Haykin98_NNC} that uses an unsupervised training algorithm and which, through a process called self-organization, configures the output units into a topological representation of the original data. It tries to imitate the self-organization done in the sensory cortex of the human brain, where neighbouring neurons are activated by similar stimulus. SOM belongs to a general class of neural network methods, which are non-linear regression techniques that can be trained to learn or find relationships between inputs and outputs or to organize data so as to disclose so far unknown patterns or structures. It is usually used as a clustering/classification tool or used to find unknown relationships between a set of variables that describe a problem. The main property of the SOM is that it makes a nonlinear projection from a high-dimensional data space (one dimension per variable) on a regular, low-dimensional (usually 2D) grid of neurons, called units.
SOM is further processed using Ultsch method \cite{UmatUlts}, the Unified distance matrix (U-matrix), which uses SOM's codevectors (vectors of variables of the problem) as data source and generates a matrix where each component is a distance measure between two adjacent neurons. It allows us to visualize any multi-variated dataset in a two-dimensional display, so we can detect topological relations among neurons and infer about the input data structure. High values in the U-matrix represent a frontier region between clusters, and low values represent a high degree of similarities among neurons on that region, clusters.

SOMs (and U-Matrix) are usually applied for visualize natural structures in the data and their relations, as well as the natural groupings that could be among them. In addition, SOM makes easy the estimation of the variables that have more influence on these groupings. Other statistical and soft computing tools can also be used for this purpose, but since Kohonen's SOMs offers a visual way of doing it, it is much more intuitive, and takes advantage of the capabilities of the human brain as a pattern recognizer.
 

This method is usually applied to analyze data as similar items tend to be 
mapped close together, while those items dissimilar, are mapped apart.
Also, the SOM graph represents and highlight very clearly those regions (clusters) 
with high training sample concentration and fewer where the samples are scarce. 
%FERGU: mencionarlo en la intro


As far as the feature selection methods are concerned, both a correlation-based 
feature subset evaluation method (CfsSubsetEval) \cite{Hall1998} and an 
adaptation of relief for attribute estimation (ReliefFAttributeEval) evaluation 
method \cite{RobnikSikonja1997} are used with two search criteria: BestFirst, 
that searches the space of feature subsets by greedy hill-climbing augmented 
with a backtracking facility, and Ranker, that ranks features by their individual 
evaluations.


%-------------------------------------------------
\subsection{Selecting an appropriate forecasting method}

As previously stated, there are many prediction methods available in the 
literature, each one with a different parameter set that may affect the 
obtained results.

However, as in the case of new books only some descriptive data is available %FERGU: data es plural. is->are
(compared to those cases in which historical data on sales of published books 
is available), not all forecasting methods can be used in this specific problem.

Thus, in this research five forecasting methods, based on the literature 
review in the introduction 
\cite{Madsen2008,ChingChin2010,Thomassey2012,Xia2012,SThomassey2014}, have been used. 
Well known forecasting methods implemented in the Weka tool
\cite{Hall2009,Witten2011} have been chosen, as these methods are
widely known, and could be very helpful for practitioners to reproduce
experiments and even to solve similar sales prediction problems using
these methods. % eliminado el pie de página, es mejor usar referencias - JJ

Specifically, the following forecasting methods are proposed:
\begin{itemize}

 % weka.classifiers.trees.M5P
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/M5P.html
 \item \emph{M5 Model trees}: 
A decision tree consists of answer-nodes, that indicate a class, and decision-nodes, 
that contain an attribute name and branches to other sub-decision trees.
Building a decision tree can be done using many algorithms, i.e. ID3 and C4.5 \cite{Quinlan1986}.
However, in order to use this model in regression problems, some authors have 
extended the model using methods such as the M5 model tree \cite{Quinlan1986,Quinlan1992,Wang1997,WittenFrank2000}, 
by combining a conventional decision tree and generating linear regression 
functions at the nodes.

The construction of a model tree is similar to that of classical decision 
trees \cite{Solomatine2004}:
The process breaks the input space of the training data through decision points 
(nodes) to assign a linear model suitable for that sub-area of the input space. 
This process may lead to a complex model tree.

Model tree models can learn and tackle tasks with high dimensionality (up to 
hundreds of attributes) and generate reproducible and comprehensible representations, 
what makes them potentially more successful in the eyes of decision makers.

The final model consists of the collection of linear sub-models that brings the 
required non-linearity in dealing with the regression problem
and both the predicted values at the answer-nodes along with the path from the 
root to that node is given as a result.

 % weka.classifiers.functions.LinearRegression
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/LinearRegression.html
 \item \emph{Linear Regression}:
 Linear regression \cite{Cohen2003,Yan2009,Rencher2102} method models the 
relationship between a scalar dependent variable (outcome) and several independent 
variables (predictor) over the range of values in the data set. 
In this statistical technique, data is modeled using linear functions to estimate 
unknown model parameters, examining the linear correlations between independent 
and dependent variables.

This method of regression provides an adequate and interpretable description of 
how the input affects the output as it models the dependent variable as a linear 
function of the independent variables. Moreover, the linear relation can be 
solved using the least squares method, that minimizes the error between the 
actual data and the regression line \cite{McClendon2015}.

 % weka.classifiers.functions.SMOreg
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMOreg.html
 \item \emph{Support Vector Machine for Regression (SVM)}:
Support vector machine (SVM) \cite{Cortes1995,Shevade1999} method is based on  
statistical learning theory and has successfully used in classification and  
regression problems \cite{Cao2003,Jari2008}.

In classification problems the algorithm searches for an optimal hyperplane 
that separates two classes, maximizing the margin between two classes. 
In the case of regression the algorithm chooses a hyperplane close to as many 
of the data points as possible, minimizing the sum of the distances from 
the data points to the hyperplane. 
In both cases, the hyperplane is defined by a subset of training set samples 
(called support vectors).
%This method works well even if the space is highly dimensional and the problem is not linearly separable.

 % weka.classifiers.lazy.IBk 
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/lazy/IBk.html
 \item \emph{k-Nearest Neighbours (kNN)}: 
The kNN \cite{Aha1991,Mitchell1997} method is an instance-based classifier in 
which the unknown instances are classified by relating them to the known 
instances using a distance measurement/function (Euclidean, Minkowsky or minimax).

The main idea behind the algorithm is that two instances far enough in the space, 
taking into account the distance function, are less likely to belong to the 
same class than two closely situated instances.

The classification algorithm locates the nearest neighbour in instance space and 
assigns the class of that neighbour to the unknown instance.
In order to improve the robustness of the model, several neighbours can be 
located, assigning the resulting class to an unclassified vector using the 
closest k vectors found in the training set by majority vote.

 % weka.classifiers.functions.MultilayerPerceptron
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/MultilayerPerceptron.html
 \item \emph{Multilayer Perceptron}:
A Multilayer Perceptron (MLP) \cite{Rosenblatt1962,Widrow1990} is a feedforward 
artificial neural network model that maps the input data onto an appropriate output. 
It is an artificial neural network generally used for classification or 
approximation problems.

This model is a generalization of the standard linear perceptron that uses several 
layers of nodes (called neurons) and that is able to solve linearly inseparable 
problems \cite{SteinwenderBitzer2003}.
Each neuron consists of a linear combination of weighted inputs which is passed 
though a non-linear activation function to produce its output.

This kind of artificial neural network is trained using a supervised learning 
technique called back-propagation.
This training method was developed independently by Werbos \cite{Werbos1974}, 
Parker \cite{Parker1985} and Rumelhart et al. \cite{Rumelhart1985}, and consists
in updating the weights of the output layer neurons once the erroneous output 
has been obtained, and then, propagating the successive weight layers back to 
the input layer.

A key issue when designing an MLP is the number of hidden layers of neurons.
Lippmann proved in \cite{Lippmann1987} that two hidden layers are enough to 
create classifying regions of any kind. This result was then confirmed in later 
works by Bishop \cite{Bishop1996} and Reed et al. \cite{Reed1999}.

\end{itemize}

The objective is finding the most appropriate forecasting method that minimizes 
the error between the obtained forecast and the actual sales for each new book 
published.


%********************************************************************************
\section{Experiments and Results}
\label{sec:experiments}

Once the methods to be applied have been chosen, they must be applied to data sets 
to obtain the prediction models for each one. 
Later on, the models ability must be validated on non-previously seen data.
Specifically, instead of using the conventional validation strategy based on 
using a training set and later on a testing set (unknown data against which 
the model must be tested), a cross-validation technique \cite{Geisser1993,Kohavi1995,Devijver1982} 
has been used to estimate how accurately the predictive model will perform in 
practice. %, limiting at the same time the overfitting problem.

%
% TODO: explicar los resultados obtenidos en la selección de variables: 
%  (1) con el SOM, (2) con los métodos de selecc. vars.
%
% CONTAR COMO SE HAN PLANIFICADO LAS EJECUCIONES CON LOS DIFERENTES METODOS:
% - se ha usado la implementación de estos 5 métodos del paquete WEKA
% - se han realizado 30 ejecuciones con cada método
% - se ha usado una máquina Linux, Intel(R) Core(TM) i5-4430 CPU at 3.00GHz (4 cores) and 16 GB RAM
To do so, in a first stage, the feature selection process has been carried out 
using both the CfsSubsetEval \cite{Hall1998} (a correlation-based feature subset 
evaluation method) and the ReliefFAttributeEval \cite{RobnikSikonja1997} (an 
adaptation of relief for attribute estimation) evaluation methods, implemented 
in the Weka tool.

% con ReliefFAttributeEval salen seleccionadas (9):
% print_run num_reprints tot_points_sale tot_points_sale_1st_year reprints distrib_novelty weeks_sale + total_sales
Using the ReliefFAttributeEval feature selection method, and taking into account 
the best ranked atributes, the following variables have been selected: 
\emph{print-run, number of reprints, total points of sale, total points of sale 
1st year, reprints, units distributed as novelty, weeks for sale, total sales}.

% con CfsSubsetEval salen seleccionadas (4):
% num_reprints tot_points_sale print_run + total_sales
Using the CfsSubsetEval feature selection method, the following variables have 
been selected: \emph{number of reprints, total points of sale, print-run, total sales}.

% como tercer método, se ha usado el SOM; tras interpretar los mapas, se
% seleccionaron las variables más adecuadas.
Then, an exhaustive analisys of the SOM results was carried out in order to 
determine the best variables to be considered to buid the forecasting model.

%
% TODO: falta incluir aquí el análisis de los mapas SOM.
%*************************************** Determinar las variables a usar, según el SOM
\textbf{ TODO: add the SOM analysis }

After the SOM analysis, the following variables have been selected: 
\emph{retail pricer, reprints, number of reprints, gifts, units distributed as novelty, print-run, total sales }.

\textbf{ TODO: add the SOM analysis }
% una vez hecho, hay que hacer las ejecuciones para ese dataset.

Once the different reduced datasets have been obtained, proposed methods 
(M5P, LR, SVM, kNN, MLP) have been selected for this study from 
the Weka implementation, and they were run 30 times with the Weka default parameter 
settings and a different random initialization seed for each run and each dataset.

Experiments have been conducted on an Intel(R) Core(TM) i5-4430 CPU at 3.00GHz 
(4 cores) and 16 GB RAM, running Ubuntu Linux 14.04.1 LTS and Java JRE\_1.7.0\_72.

% datasets:
%  original (16 vars)
%  4 vars
%  9 vars
%  SOM selected vars

Table \ref{tabla:resultsT} shows obtained results (time to build the model and 
error) after applying the proposed forecasting methods on the original dataset,
using the 16 available variables per book.
In this dataset, every book is described using not only numerical variables, 
but using two additional categorical variables, \emph{editorial} and \emph{collection}, 
what makes harder to process the instances and build the different models.

% Mean absolute error MAE / Root mean squared error RMSE /  Relative absolute error RAE / Root relative squared error RRSE
  % ¿Qué sentido tiene usar los 4 métodos de error? ¿Es que vamos a seleccionar uno? - JJ
  %[pedro] es lo habitual en los trabajos en los que se realizan tareas de predicción
  % en algunos calculan muchos más, pero estos son los más usados. 
\begin{table*}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
 Method 	& Time (sec.) 	      & MAE 		& RMSE 		& RAE (\%) & RRSE (\%) 		\\
\hline
  M5P 		& 24.24 $\pm$ 0.96    & 197.48  & 577.28  & 35.21    & 41.85  \\
\hline
  LR 		  & 3533.84 $\pm$ 178.38& 247.22 & 561.67  & 44.08    & 40.71    \\
\hline
  SVM 		& 5586.96 $\pm$ 139.71& 215.69  & 574.76  & 38.46    & 41.66  \\
\hline
  kNN 		& 0.02 $\pm$ 0.00     & 302.07 	& 924.94 	& 53.86 	 & 67.05 	\\
\hline
  MLP 		& - 	  & - 	& - 	& - 	& - 	\\
\hline
\end{tabular}
\end{center}
\caption{\small{Results (errors and time) obtained using the different
    forecasting methods on the original dataset (16 variables per book). 
    In this case, due to the categorial variables, the MLP could not be run.}} 
\label{tabla:resultsT}
\end{table*}


Obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 7 variables per book (obtained 
using the SOM analysis) can be shown in Table \ref{tabla:resultsSOM}.
%In this dataset, every book is described using numerical variables, and thus, 
%processing the data to build the different models is a fast task.
%***************************************
% TODO: falta rellenar la tabla, una vez que hagamos el análisis con el SOM y decidamos qué variables incluir en este dataset
\begin{table*}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
 Method   & Time (sec.)         & MAE     & RMSE    & RAE (\%) & RRSE (\%)      \\
\hline
  M5P     & 1.74 $\pm$ 0.10     & 218.81  & 661.85  & 39.01    & 47.98  \\
\hline
  LR      & 0.27 $\pm$ 0.02     & 258.82  & 582.74  & 46.15    & 42.24  \\
\hline
  SVM     & 45.58 $\pm$ 1.89    & 240.18  & 614.30  & 42.82    & 44.53  \\
\hline
  kNN     & 0.04 $\pm$ 0.00     & 221.21  & 520.17  & 39.44    & 37.71  \\
\hline
  MLP     & 8.99 $\pm$ 0.10   & 353.70 $\pm$ 57.74 & 624.87 $\pm$ 86.77  & 63.06 $\pm$ 10.29  & 45.30 $\pm$ 6.29   \\
\hline
\end{tabular}
\end{center}
\caption{\small{Results (errors and time) obtained using the different 
forecasting methods on the dataset obtained after the SOM analysis (7 variables per book).}}
\label{tabla:resultsSOM}
\end{table*}


Obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 8 variables per book (obtained 
using the ReliefFAttributeEval feature selection method) can be shown 
in Table \ref{tabla:results9}.
In this dataset, every book is described using numerical variables, and thus, 
processing the data to build the different models is a fast task.

\begin{table*}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
 Method   & Time (sec.)         & MAE     & RMSE    & RAE (\%) & RRSE (\%)      \\
\hline
  M5P     & 2.27 $\pm$ 0.02     & 195.97  & 629.33  & 34.94    & 45.62  \\
\hline
  LR      & 0.38 $\pm$ 0.01     & 256.19  & 549.19  & 45.68    & 39.81   \\
\hline
  SVM     & 132.64 $\pm$ 2.86   & 230.66  & 613.27  & 41.12    & 44.45   \\
\hline
  kNN     & 0.06 $\pm$ 0.01     & 214.76  & 551.33  & 38.29    & 39.96   \\
\hline
  MLP     & 130.77 $\pm$ 1.51   & 326.60 $\pm$ 75.77 & 687.61 $\pm$ 55.70  & 58.23 $\pm$ 13.51  & 49.84 $\pm$ 4.04   \\
\hline
\end{tabular}
\end{center}
\caption{\small{Results (errors and time) obtained using the different 
forecasting methods on the dataset obtained using the ReliefFAttributeEval 
feature selection method (8 variables per book).}}
\label{tabla:results9}
\end{table*}


Finally, obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 4 variables per book (obtained 
using the CfsSubsetEval feature selection method) can be shown 
in Table \ref{tabla:results4}.
As in the previous case, in this dataset every book is described using numerical 
variables. Thus, due to the number of features per book, building the different 
models is faster than in the previous cases.

\begin{table*}[!h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
 Method   & Time (sec.)         & MAE     & RMSE    & RAE (\%)  & RRSE (\%)      \\
\hline
  M5P     & 2.02 $\pm$ 0.01     & 205.52  & 556.90  & 36.64     & 40.37  \\
\hline
  LR      & 0.32 $\pm$ 0.00     & 261.04  & 547.42  & 46.54     & 39.68   \\
\hline
  SVM     & 81.74 $\pm$ 1.66    & 242.26  & 606.07   & 43.19    & 43.93   \\
\hline
  kNN     & 0.05 $\pm$ 0.00     & 216.05  & 506.34  & 38.52    & 36.70   \\
\hline
  MLP     & 47.80 $\pm$ 0.51    & 318.29 $\pm$ 43.96 & 587.50 $\pm$ 51.50  & 56.75 $\pm$ 7.84  & 42.59 $\pm$ 3.73   \\
\hline
\end{tabular}
\end{center}
\caption{\small{Results (errors and time) obtained using the different 
forecasting methods on the dataset obtained using the CfsSubsetEval 
feature selection method (4 variables per book).}}
\label{tabla:results4}
\end{table*}

\begin{figure}[!h] 
\begin{center}
  \epsfig{file=fig_time.eps,width=11cm}
\caption{\small{Running time in seconds, obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). Plot shows that using categorical variables as part of the dataset makes the problem hard to solve. As expected, simpler methods are much faster to build the model than the complex ones.}}
\label{fig:time}
\end{center}
\end{figure}

\begin{figure}[!h] 
\begin{center}
  \epsfig{file=fig_rae.eps,width=11cm}
\caption{\small{Relative absolute error (RAE $\%$) obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). The M5P model obtains the lowest errors, whatever the dataset used.}}
\label{fig:rae}
\end{center}
\end{figure}

% - indicar que el uso de las dos variables categóricas hace más difícil de aprender el conjunto
Paying attention to the obtained results shown in tables \ref{tabla:resultsT}, 
\ref{tabla:resultsSOM}, \ref{tabla:results9} and \ref{tabla:results4}, 
as well as the figures \ref{fig:time} and \ref{fig:rae}, 
using those categorical variables as part of a big dataset makes the problem 
hard to solve, both in required time to build the forecasting models as well as 
in the obtained errors.
Moreover, some methods are unable to build the models in a reasonable time.
%However, as stated before, Trevenque expert sales manager, taking into account 
%his knowledge in industry experience and their perceptions, decided that 
%\emph{EDITORIAL} and \emph{SUBJECT} should be taken into account.


% - indicar pros y contras de los diferentes métodos
% - indicar qué método da mejores resultados
% - indicar qué método es tipo "caja-negra" y cuáles ofrecen modelos "explicables" 

As far as the different methods is concerned, M5P model obtained good results, 
both in time and error, and as stated before, this method can learn and tackle 
tasks with high dimensionality and it generates reproducible and comprehensible 
representations that provide information on the decision process and does not 
require human intervention either for the operation or for interpretation. 
Thus, it is very suitable in order to be incorporated in a publishing company 
processes.

The Linear Regression model outperforms some other complex methods, both in time 
(as it is a simple method to train) and in obtained errors.
However not in all cases, as more powerful models that obtain better results 
can be obtained in comparable time.

Taking into account the SVM model obtained results, this method obtains comparable 
errors to those of M5P. Even when used using the default parameter values it exhibit 
a good generalization performance.
However, it is much slower than the others to build the model, not only when 
using the dataset with categorical input variables, but also when using the 
datasets with numerical variables.

kNN is the faster method used in this study, as it is the simpler method.
However, the model is slow when classifying (as there are a large number of 
training examples), and it exhibit a poor generalization ability, as it does not 
learn anything from the training data.

As far as the Multilayer Perceptron, its main drawbacks are both the cost of model 
building (not only parameter and weights setting, but also time to train the 
network) and the fact that it is a back-box model, and thus, it is difficult 
to explain how the forecasts are obtained.
Moreover, in these experiments it has been seen the difficulty to process 
information related to categorial variables, such as \emph{editorial} and 
\emph{collection}.

%[Pedro] ¿aplicamos tests estadísticos para ver si hay diferencias entre métodos?

Finally, paying attention to the cost in time to obtain the forecasting models 
and their accuracy, the M5P model is the most adequate to solve the problem of 
how many books should be printed when a new book is published.
This forecasting method, as an additional advantage, can be used for products 
with historical sales data and for new products with little or no historical 
data available.


%********************************************************************************
\section{Conclusions and Future Work}
\label{sec:conclusionsAndFutureWork}

In this paper, the problem of how many books should be printed 
when a new book is published is faced using several data-mining and
forecasting methods.

This is a very challenging problem in this industry, because printing a higher 
number of volumnes than those finally sold will lead to losses, 
while printing an adequate number of copies will optimize sales and company earnings.
In addition, there are several difficulties inherent to the new book sales forecasting, 
such as dealing with limited data, finding adequate forecasting methods, and 
selecting the best method to use.

%La investigación descrita en este trabajo se ha llevado a cabo conjuntamente con una empresa editorial que ha facilitado un conjunto de datos con los que trabajar, así como con la ayuda de un experto en ventas de este campo para validar los resultados obtenidos.
A data set consisting of 3159 books provided by the Spanish %FERGU: mover este párrafo debajo del siguiente
publishing company Trevenque Editorial has been used.

In this paper, a data visualization method has been used to find out what are 
the relevant variables describing a book in the prediction of book sales.
Then, several standard data mining models for sales forecasting have been used 
to forecast new book sales, and from them deduce print-run, based on those 
variables.

The most relevant variables when predicting sales have been determined, 
while unrelated or superfluous ones have been removed, 
and sales predictions have been obtained using five different forecasting 
standard methods.
Not only the obtained forecasting error has been taking into account to evaluate
those methods, but also the the cost of model building as parameter setting and 
the required time to train the model.

In this sense, the decision trees proved to be a suitable model because of the 
ease to explain how the prediction is obtained from the variables that describe 
the new book. 
This, compared to the black-box models such as the artificial neural network, 
was decisive from the point of view of the needs of experts and Trevenque company.
Moreover, proposed method was validated by the company and incorporated in a 
business intelligence tool for publishing houses. %FERGU: no hay más info de esto? estaría guay describirlo

%FERGU: no mencionais el SOM en las conclusiones

Overall, this study contributes to the literature by proposing the use of different 
soft-computing standard techniques to solve a new challenging problem.
Moreover, proposed method could be implemented, not only in the editorial 
industry, but also in other domains where the specificity of products is similar, 
and might be of interest to other academic researches and industrial practitioners.


%---------- trabajos futuros

As future work, it would be interesting to compare the obtained forecasting results 
with other methods based on soft-computing, such as genetic programming or fuzzy logic.
Moreover, as default parameters have been used for proposed forecasting methods,
it would be of interest carrying out some parameter tuning procedure in order to
improve obtained results.

Furthermore, since there are some characteristics that may affect product sales 
significantly, another way to improve sales predictions could be making a thorough 
study of the effect of the application of discounts and promotions. 

Finally, there are some variables over which the published has some
control once the book has been published, such as the initial print
run or the books given out as gifts to reviewers or what proportion is
going to be sent to each possible point of sale. This can be
incorporated into an optimization model that optimizes those values to
obtain desired sales. This is left as future work.


%********************************************************************************
\section*{Acknowledgements}
This work has been supported in part by 
PreTEL (PRM Consultores - Trevenque),
TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitivity),
SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico), 
PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces en la
IX Convocatoria de Proyectos de Investigaci{\'o}n), 
and PYR-2014-17 GENIL project (CEI-BIOTIC Granada). 

%********************************************************************************
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
