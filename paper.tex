\documentclass{llncs}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[dvips]{epsfig}
\usepackage[latin1]{inputenc}
\usepackage{url}
\usepackage{appendix}

\def\CC{{C\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}~}
\addtolength{\textfloatsep}{-0.5cm}
\addtolength{\intextsep}{-0.5cm}


%%%%%%%%%%%%%%%% Titulo %%%%%%%%%%%%%%%
\title{Enhancing the prediction of sales for newly published books by
  means of computational intelligence methods} 

%%%%%%%%%%%%%%%% autores %%%%%%%%%%%%%%
\author {
P.A. Castillo$^1$, A.M. Mora$^1$, H. Faris$^2$, J.J. Merelo$^1$, \\ P. Garc\'{\i}a-S\'anchez$^1$, A.J. Fern\'andez-Ares$^1$, P. de las Cuevas$^1$, M.G. Arenas$^1$
}
\institute{
$^1$Department of Computer Architecture and Computer Technology. CITIC, ETSIIT. University of Granada (Spain)\\
$^2$Business Information Technology Department. King Abdullah II School for Information Technology. The University of Jordan. Amman (Jordan)\\
e-mail: {\tt pacv@ugr.es, amorag@geneura.ugr.es,
  jmerelo@geneura.ugr.es, pgarcia@geneura.ugr.es,
  antares.es@gmail.com, palomacd@ugr.es, mgarenas@ugr.es}
}
% Antonio - Hossam add your 'official' e-mail here. ;)

\date{}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
When a new book is launched the publisher faces the problem of how
many books should be printed for delivery to bookstores; printing too
many is the main issue, since it implies a loss of investment due to
inventory excess, but printing too few will have also a negative economical impact. In this paper, we are tackling the problem of predicting total sales in order to print the right amount of books and doing so even before the book has reached the stores. A real dataset including the complete sales data for books published in Spain across several years has been used.
We have conducted an analysis in three stages: an initial exploratory analysis, by means of data visualisation techniques; a feature selection process, using different techniques to find out what are the variables that have more impact on sales; and a regression or prediction stage, in which a set of machine learning methods have been applied to create forecasting models for book sales. 
The obtained models are able to predict sales from pre-publication data with
remarkable accuracy, and can be visualised as simple decision trees. Thus, these can be used as decision-aid tools for publishers, which can provide a reliable guidance on the decision process of publishing a book. 
This is also shown in the paper addressing example cases of four representative publishers, regarding their number of sales and the number of different books they sale.
\end{abstract}


%********************************************************************************

\section{Introduction}

Publishing a book, in the same way as releasing any other cultural product with a physical substrate, implies several types of risks and costs due to
the complexity, and derived expenses, of the production,
distribution and storage processes; most of these costs are associated
to the number of books actually printed. But the problem is further
complicated for completely new products, in the sense that there is an unknown
author or a new topic with no track record. Sales forecasts can
only be based on the knowledge obtained from other similar products,
but the definition of ``similar'' itself is fuzzy and, sometimes,
subjective. Even if the author or the topic is known, doing accurate
estimations of future sales of the new product is quite difficult, and extremely important, so that production runs heed predicted sales. 
Indeed,several studies have demonstrated that improving sales predictions 
(a reduction of forecasting errors) results in an enhancement in the 
production process \cite{Fildes2010,Saeed2008}.

This problem is specially acute for book publishers, who, among other
content providers, release new {\em products} more frequently than
other industries, so that they face the problem of predicting sales quite often. The issue, in this case, is to print an adequate number of copies but not too many, as the unsold volumes will lead to losses and sunk inventory cost, 
whereas if the number of printed copies was not enough, a new print
run can be made, resulting in temporary losses due to a lack of supply or in bad marketing for the customers, for instance.  

In this sense, errors in one or the other direction do not have the same impact, however, it is extremely important to develop an accurate predictive method
which could forecast the future sales a new book will achieve, in order to optimise production schedules, improve the publishing company profits and minimise losses \cite{Zhao2001}, being this the main objective of this paper. 

It is important to notice that the accuracy of the predictive method that we intend to design is affected negatively by several factors, related to decisions on purchases, to freight traffic, or with the distribution strategy \cite{Little1998}.
Thus, it is mandatory to know the product and the variables that
might affect the sales, and how an expert manager takes them into 
account \cite{Armstrong2001,SThomassey2014}. 
Moreover, their impact is difficult to estimate and it is not constant over time, hence the difficulty to adequately identify and measure their influence \cite{DeToni2000}.
In addition, some may have correlations between them. 
However, these correlations can be detected using data analysis methods, 
such as Self-Organizing Maps (SOM) \cite{kohonen1998} or stepwise regression analysis (SRA) \cite{Chang2009}. 

So, we will analyze in this work the data by means of different data mining methods and create predictive models that can be used mainly as decision-aid tools for book publishers. Then, they will be able to combine their expert-knowledge about the market with the created forecasting models in order to get a reliable estimation of book sales.

% Antonio - I think this is the main idea we have to sell in this paper. Due to the  difficulty of the book sales market (and its variables), we cannot give an accurate tool for predicting book sales, but a decision-aid tool for publishers. Do you agree?

In the research published in this paper, the problem of how many units should be printed  when a new book is published is faced using real data provided by the 
Spanish publishing company Trevenque Editorial S.L.\footnote{{\tt http://editorial.trevenque.es}}. It provides management systems and services for bookshops, publishers and distributors. 
Usually, the company's expert sales managers analysed historical data and applied their knowledge using external variables to compute correct estimations (predictions), for instance, they can decide increasing or decreasing the sales forecast based on season or sales period.
However, just carrying out the analysis and forecasting process as a human expert, may lead to imprecise predictions which may not take all the variables into account. 
In addition, the process might be tedious, especially when the amount
of data is quite large. Moreover, different experts can reach different
predictions from the same dataset \cite{Sanders1994}. 

%An initial version of this method was developed under a contract with that company and eventually integrated in their product. This paper represents an improved version of that method based on the same data. 
% We should also mention what data we actually mean, I don't think
% we've published it here. - JJ
% Antonio - I think this information is not relevant for the paper...

With this issue in mind, in this paper we firstly present an analysis on the aforementioned real dataset, by means of data mining and visualisation techniques. Then, a feature selection process is conducted using three different methods, in order to find out what are the relevant variables describing a book in the prediction (or regression) procedure. After this, a forecasting stage is performed applying a set of regression algorithms to the different datasets (considering a different number of variables).
The value of this methodology is contrasted considering four different publishing houses being representative as top sellers, medium sellers, the most varied and one which sells a medium amount of different books.

The rest of this paper is structured as follows: next, Section \ref{sec:soa} presents a comprehensive review of the approaches found in the bibliography related to the prediction of sales in similar scopes. 
Then, Section \ref{sec:problem} introduces the problem of the estimation of print-run for new books, along with a description of the considered dataset.
Section \ref{sec:methodology} details the methodology considered in the study, from the data collection and preprocessing stages, to the selection of the forecasting methods to be applied in the experiments.  These are reported in Section \ref{sec:experiments}, which is also devoted to analyse the obtained results in feature selection and book sales forecasting using different datasets (with different amounts of features) for all the publishing houses. The results obtained for the four considered special cases are commented in Section \ref{sec:example_cases}. Finally, conclusions and future lines of work are presented in Section \ref{sec:conclusionsAndFutureWork}. 

%********************************************************************************
\section{State of the art on sales forecasting} 
\label{sec:soa}

As far as we know, no previous attempt can be found in the literature in which 
prediction methods have been applied to estimate the print run. 
However, similar techniques to those proposed in this paper, e.g. 
regression models \cite{Papalexopoulos1990}, 
neural networks \cite{Yoo1999} 
or fuzzy systems \cite{Mastorocostas2001}, 
have been applied to solve sales prediction problems in other industrial sectors.


Specifically, time-series prediction methods
\cite{Chu2003,Brown1959,Winters1960,Box1969,Papalexopoulos1990}  
is perhaps the most used technique to tackle the sales forecasting
problems. The efficiency of these techniques strongly depends on the field of application 
and the correctness of the problem data. However, since they require a
large amount of data for predicting sales, these methods are not the
most suitable for this task \cite{ChingChin2010}. And they cannot be
applied before the book is published, as is the objective of this paper. 

In this case, just a few variables are known in advance
\cite{ChingChin2010,FaderHardie2005,Madsen2008}, and some of 
them are categorical, which makes them harder to process, even more so if the number
of categories is high. As stated in those works, given these issues, finding adequate
forecasting techniques and selecting the best method to use is also a problem.

For this reason, usually the methods used to predict book sales have
been generally based on experts' experience.% Reference - JJ
 They analyse data about
sales and, taking into account their knowledge in industry, their
experience, and their perception about trends, could make decisions on
the companies production, i.e. how many books should be printed when a new
book is published. 
This is a complex problem, since the predictions are influenced by
external variables that must be taken into account, such as
seasonality, promotions or fashions that expert managers might
subjectively apply \cite{Lapide1999,ChernWSF15}. % This
                                % has something to do with the
                                % previous paragraph? Put it there!!! 
%
  %[pedro] sobre la industria editorial no hay trabajos sobre predicción de ventas.
  % Lo habitual ha sido usar el conocimiento de expertos para realizar predicciones 
  %  o aconsejar acerca de la cantidad de libros a imprimir.
  % Lo más parecido que he encontrado son dos trabajos (uno en realidad) de Moon et al. 
  %  sobre "Use Blog Information As Book Sales Prediction":
  %     Moon2010ICSSSM
  %     Moon2010ICEC
Recently, some works have proposed analysing relationships between on-line information 
and book sales \cite{Moon2010ICSSSM,Moon2010ICEC}. Specifically, Moon
et al. propose using the number of blog references as an indicator of
the success of a book, and thus, its sales. Different sales patterns
are observed by analysing data obtained. 
% Antonio - describir esos 'sales patterns'. ;)
% FERGU: y también cuales son esos 'data obtained'
However, the authors used in these papers historical data from published books, 
so their proposed methodology can not be applied to the problem of unpublished-books sales prediction.
%FERGU: pero nosotros también usamos historical data, no? qué diferencia hay con lo nuestro? por qué lo nuestro es lo mejor?

%
% pasar a contar los casos de la predicción de ventas en la 
% industria de la moda (SThomassey2014) y de nuevos productos (ChingChin2010)
%
% Son problemas muy similares, ya que no hay datos históricos de los nuevos productos,
% al igual que no los hay sobre ventas, y en los que además influyen mucho las variables 
% externas (estacionalidad, modas, fama del author, etc).
% Hay que echar mano de productos similares o simplemente de los datos que describen el nuevo producto.
%

In order to deal with this issue, i.e. forecasting sales of new products, some authors have used different data mining methods \cite{Hammond1990,Chang2009,ChingChin2010,Thomassey2012,Xia2012}. 
% Antonio - Maybe we should describe them briefly...
%
In the same line, Thomassey et al. \cite{SThomassey2014} propose using a clustering method together with decision trees for sales forecasting in textile-apparel fashion industry, 
a very similar issue to the new book publishing one.
The same problem was previously faced by Xia et al. \cite{Xia2012} using a 
forecasting method based on extreme machine learning with adaptive metrics of inputs.
% Antonio - of -> as???
The complexity of this problem relies on the lack of historical data, on the short lifetime of the majority of items, 
% Antonio - ver si está bien el cambio 'large number' -> 'majority'
and on the influence of variables such as promotions, fashions, or economic environment \cite{Thomassey2012,Xia2012,SThomassey2014}, as mentioned above.
%FERGU: completar con: "In our case, these ideas ..."

  %[Pedro] otro caso muy similar es el paper de ChingChin2010 sobre:
  % New tea product
  % New cosmetic product
  % New soft drink product
In \cite{ChingChin2010}, a decision-support system for new product sales %FERGU: The work of Chinchin et al.?
forecasting is proposed to solve three real-world sales forecasting problems, namely: new tea, cosmetics, and soft drink products. This is addressed taking into account quantitative variables.
% Antonio - qué significa eso?
In that study, products are classified based on their sales patterns, so it is assumed that products in the same class would follow a similar sales pattern. 
However, as the authors state, the proposed system can not deal with qualitative 
data related to the products. Moreover, real-world cases, such as consumer 
electronics and fashion products, should be deeply analysed, as these industries 
present/launch new products every season. 

  %[Pedro] otro caso a comentar es el paper de Chang2009 sobre: 
  % sales forecasting in printed circuit board industry.
In those cases where historical data are available, classical time series 
forecasting methods can be applied, which is what the authors do in
\cite{Chang2009}, where a hybrid model integrating K-Means and fuzzy
neural networks to forecast the future sales of a printed circuit
board is proposed. % We have been talking above that above and SAID IT
                   % DIDN'T WORK? Why do we say it here again? - JJ
                   % No, above we said that the efficiency depends on the field 
                   % of application and the available problem data. - pedro
A similar method is described by Chern et al. \cite{ChernWSF15}, who analyse 
historical data along online reviews, reviewer characteristics and review 
influences to understand how electronic `word-of-mouth' influences product sales. 
The proposed method is suitable for those sales forecasting problems 
with a big amount of online reviews and historical data.

Thus, taking into account the kind of problem we address in this
paper, i.e. the prediction of sales of a new product for which no
historical data are available, the classical forecasting methods based
on time-series are not 
adequate, although they can obviously be used once the book has been launched for sale. 
In our work the relevant variables in the prediction of sales are identified
by means of data visualisation and feature selection methods. Then, machine learning techniques are applied to predict sales and to infer the ideal print-run of a book.
In addition, since publishers need models to explain obtained predictions, 
`black box' forecasting methods may not be suitable, so models based
on decision rules have been tested as an alternative. 



%********************************************************************************
\section{Problem description}
\label{sec:problem}


The main objective of this paper is to forecast sales for 
%FERGU: el objetivo es la herramienta o la predicción?
unpublished books for which no historical data are available. Instead, just some
%FERGU: pero usamos datos históricos de otros para la predicción, no? Como otros papers...
descriptive and variables known in advance related to the new book. In
order to do that, we have used a training dataset which includes
historical sales data for a big amount of books published in Spain for
several years, with a set of 50 features or variables per pattern.  

The initial dataset, provided in the framework of a contract by
Trevenque Editorial S.L., included 3159 books. Table
\ref{tabla:paramsOrig50} shows the initial set of variables describing
each book. 

\begin{table}[!ht]
\caption{Parameters describing a book, provided by the company Trevenque S.L.}
\label{tabla:paramsOrig50}
\begin{center}
\begin{tabular}{|c|c|}
\hline 

\begin{minipage}{2.45in} \begin{verbatim}
reference
author
retail price
subject1
subject2
subject3
publisher
collection
bookbinding
print-run
total sales
dept stores sales
sales through delegates
rest of sales
total sales 1st year
mall sales 1st year
delegates sales 1st year
rest of sales 1st year
total distributed
distributed through dept stores
distributed through delegates
distributed - rest
total distributed 1st year
distrib. through dept stores 1st year
distrib. through delegates 1st year
\end{verbatim} \end{minipage}     & 

\begin{minipage}{2.3in} \begin{verbatim}
rest of distributed 1st year
reprints
number of reprints
total returns
returns - dept stores
delegates returns
rest of returns
total returns 1st year
returns -  dept stores 1st year
delegates returns 1st year
rest of returns 1st year
gifts
units distributed as novelty
total points of sale
points of sale - dept stores
delegates points of sale
rest of points of sale
total points of sale 1st year
dept stores points of sale 1st year
delegates points of sale 1st year
rest of points of sale 1st year
weeks for sale
positive sales environment
medium sales environment
negative sales environment
\end{verbatim} \end{minipage}    \\

\hline
\end{tabular}
\end{center}
\end{table}

% Antonio - briefly describe the variables

In this prediction problem there are other external variables that also affect 
the sales of a new book, such as those related to seasonality, trends or 
promotions, that classical prediction methods can not deal with. %FERGU: por qué?
However, an expert can consider them to modify the forecast obtained by 
the automatic method. Even publishing companies tend to define rules or 
indexes to subjectively adjust the results of prediction based on
external data. 
Thus, the proposed method, will analyse the provided data and
variables and will yield a forecast of sales according to them. This
result could be used as a reference for a human expert, who would
consider it in his/her own prediction process. 

% This section is too short and really does not describe the problem
% to its full extent. First, it should say if some of these variables
% were discarded for some reason. Group the variables we have in those
% available to publisher and those that can be used afterwards. And
% then give an idea of the complexity of the problem and how difficult
% it should be to find a solution - JJ


%********************************************************************************
\section{Methodology}
\label{sec:methodology}

There is not a standard methodology to solve the general problem of sales forecasting. The steps we have followed in this paper are described below.
% Antonio - quito esto porque ya no es lo que se ha hecho. El objetivo del paper no es la metodología a seguir, eso es estándar. ;)
%FERGU: hacer más hincapié de esto en la intro entonces
%In this work we have followed these steps: %FERGU: por qué estos steps y no otros? Ponerlo aquí. Esta metodología es general o solo para este paper?
%\hspace{2cm}
%\begin{verbatim}
%  Step 1: Collect and analyse data
%       - Data preprocessing
%       - Feature selection
%  Step 2: Select the best forecasting method
%       - Evaluate forecasting models
%       - Perform forecast
%  Step 3: Subjectively adjust obtained results
%      - Apply additional expert knowledge, such as information 
%        about seasonality or trends.
%\end{verbatim}

%Usually, a preliminary analysis of the data is carried out to eliminate those 
%variables that describe the product but do not provide useful information during 
%the prediction process.

First, after the publishing house has provided the raw data
(extracted directly from its database), a preprocessing method has
% This should go to the previous section. 
been performed in order to remove incorrect or incoherent patterns,
such as those with null values in required variables, or with negative
sales, for instance. Moreover, the initial set of variables (presented
in Table \ref{tabla:paramsOrig50}) has been reduced by an expert to
use just those that can be known in advance when a new book is
launched, i.e. \textit{pre-sales data}. 

% You have to present the problem and not the solution. It's not "what
% you did last summer", it is SCIENCE. Why did you think FS was
% needed? What procedure did you use for it?
Then, a Feature Selection (FS) process \cite{kittler1986feature} has
been carried out, as it is an effective dimensionality reduction
technique and an essential preprocessing method to discard `noisy'
(non-useful) features \cite{Krishnapuram2004}. The aim of these
% Did you know there were noisy features? Why? 
% You should include in the previous section an exploratory data
% analysis that describes the range of variables, how many categories
% those variables have, and so an do forth - JJ 
algorithms is exploring the space of attributes to find out which
subset of them yields the best results in a classification or
prediction problem. 
  In this stage, both, numerical and visualisation-based approaches have been applied in this paper, in order to select the best variables for the forecasting 
methods.  
% Antonio - Decidir si es 'forecasting' o 'regression' (o si son indiferentes) y usarlo siempre en el texto
  This task may also help to identify leading indicators to be considered by a human expert to make predictions.

Moreover, the application of FS also aims to reduce the computational time for building the forecasting models, along with the complexity of the obtained models themselves. This has been a key factor in this work since one of the objectives is to obtain `easily' interpretable decision models, which can be useful for human experts (publishers).

After this process, several forecasting techniques have been tested, such as rule- or tree-based, together with numerical methods. 
% Antonio - is 'numerical' the best term for these kind of methods?
%FERGU: OJO que en esta sección a veces usáis el presente perfecto (have been) y otras el presente (is performed)
% Antonio - Corregido. ;)
%
The obtained results have been compared and analysed, computing, in addition to the obtained \textit{correlation coefficient}, some error measures from the literature \cite{ChingChin2010,Madsen2008}. We have considered the widely accepted standard formulae \cite{gepsoft} (Equations \ref{eq:MAE} to \ref{eq:RRSE}), which are implemented in tools like Weka \cite{pentaho,Witten2011} or R \cite{otexts,Hyndman2013}.
Specifically, in order to compute the forecasting errors of the different methods, Mean absolute error (MAE), Root mean squared error (RMSE), Relative absolute error (RAE) and Root relative squared error (RRSE) have been selected:
%FERGU: have been selected for this methodology, for this work... algo así
% Antonio - ya se ha dicho. ;)

\begin{itemize}
  \item \emph{Mean Absolute Error} (MAE):
        \begin{equation}\label{eq:MAE}
            MAE = \frac{1}{n}\sum_{i=1}^n {\mid p_i - o_i\mid}
        \end{equation}

  \item \emph{Root Mean Squared Error} (RMSE):
        \begin{equation}\label{eq:RMSE}
            RMSE = \sqrt{ \frac{1}{n}\sum_{i=1}^n {(p_i - o_i)}^2 }
        \end{equation}

  \item \emph{Relative absolute error} (RAE):
        \begin{equation}\label{eq:RAE}
            RAE = \frac{ \sum_{i=1}^n {\mid p_i - o_i\mid} }{ \sum_{i=1}^n {\mid p_{i-1} - o_i\mid} }
        \end{equation}

  \item \emph{Root relative squared error} (RRSE):
        \begin{equation}\label{eq:RRSE}
            RRSE = \sqrt{ \frac{ \sum_{i=1}^n {(p_i - o_i)}^2  }{ \sum_{i=1}^n {(p_{i-1} - o_i)}^2 }  }
        \end{equation}
\end{itemize}
where $o_i$ is the individual data $i = {1,...,n}$ and $p_i$ is the obtained prediction.

Once the models and results have been obtained for the whole dataset, the same methodology has been applied to some specific cases (publishing houses) in order to prove the value of the proposal.
Some specific interpretable models have been created as a final result of the work. The aim is that these will cold be used as a decision-aid tool for the experts who have to take decide about the best print-run quantity for new published books, in order to, logically maximise the benefit (or minimise the costs).

%The created forecasting models and results, along with the selected subset of features, are analysed and presented to the experts in order to aid his/her decision. They might manually adjust the forecasting results taking into account their knowledge in industry experience and their perception about trends.

%FERGU: poner aquí algo de un párrafo que explique como se valida la metodología
% Antonio - TODO: esto habrá que explicarlo mejor cuando esté hecho y veamos lo que sale. ;)

%-------------------------------------------------
\subsection{Data collection}
 

In general, historical data are used to make time series, and from them extracting common patterns and obtaining accurate predictions.
However, in the case of new products, such as the issue of new books, past patterns are difficult to observe as there is no available historical sales data 
\cite{ChingChin2010}. Thus, only pre-sales data are available in this problem.

Moreover, the initial set of features in the dataset must be revised in order to consider just those that can be known before the launch of the book, i.e. those whose value don't depend on the book sales. So, we have selected the subset of features described in Table \ref{tabla:params_pre_sales}.


\begin{table}
% Shouldn't this table go to "Problem description"? It is a
% preliminary selection - JJ
\caption{Considered pre-sales features and their types. It also shows
  the names of the variables used in the applied methods and if everyone is an input (independent) or an output (dependent) variable.} 
\label{tabla:params_pre_sales}
\begin{center}
\begin{tabular}{|c|l|l|c|c|}
\hline 
No. & Feature Name & Variable & Type & In/Out\\
\hline 
1 & retail price (when launched) & \texttt{ret\_price} & numerical & input\\
2 & main subject (code) & \texttt{subject1} & numerical & input\\
3 &  publisher + collection (joint code) & \texttt{collection} & categorical & input\\
4 & bookbinding (code) & \texttt{binding} & categorical & input\\
5 & gifts (promotional books) & \texttt{gifts} & numerical & input\\
6 & units distributed as novelty & \texttt{distrib\_novelty} & numerical & input\\
7 & total number of points of sale & \texttt{tot\_points\_sale} & numerical & input\\
8 & total number of points of sale (1st year) & \texttt{tot\_points\_sale\_1st\_year} & numerical & input\\
9 & weeks on sale & \texttt{weeks\_sale} & numerical & input\\
10 & print run & \texttt{print\_run} & numerical & input\\
\hline 
\hline
11 & total sales & \texttt{total\_sales} & numerical & output\\
\hline 
\end{tabular}
\end{center}
\end{table}

As it can be seen in the table, \texttt{total\_sales} is the output variable, so it is the objective of the prediction methods. \texttt{print\_run} has been considered as an input variable because it is something to be decided in advance by a publisher. In addition, this variable could be interesting to be used to test the prediction model, once it is built. So the publisher could try different values for it and see what the predictor yields, in order to take the best decision.

Initially, there were three different features describing the subjects and subsubjects of every book \textit{subject1,2,3} (see Table \ref{tabla:paramsOrig50}), but for most of the patterns (books) there is no value for subjects2 and 3, so we have discarded these features. 

Regarding the feature \textit{publisher}, we have removed it from the final dataset since it is indeed included in the code of \texttt{collection}. In addition, we think it is not really relevant for the accuracy of the models and, moreover, publishers will not consider the results in other publishing houses for predicting its own sales.
% Antonio - Think about this... should we mention this?

Therefore, in order to create simpler and more accurate prediction models, and with the aim to provide them as a useful decision-aid tool for the publisher, we have transformed the categorical variable \texttt{subject1} into a numerical one, considering it refers to a code of the Dewey Decimal Classification system for books \footnote{\url{https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes}}, and the existing similarity relationship between the books inside a class or subclass. So, for instance, all the books inside subdivisions of class 200, `Religion', are considered as similar regarding their topics, so they could be also close in terms of distance. The same reasoning can be applied to subclasses.



%-------------------------------------------------
\subsection{Data preprocessing and initial analysis}
\label{subsec:data_preproces}

Once a dataset has been composed it must be preprocessed, i.e. it
should be revised in order to fix the existing errors (such as wrong
or duplicated patterns), or to complete (or address) the absence of
information, for instance. The aim is looking for the correctness,
homogeneity and completeness of the data. % useless paragraph - JJ

%You need to lay out to the reader what you are going to do here. In
%order to reach objective x, we need to do y, that is why we use
%z. After using z, we prove that we have done y and thus reached
%objective x. For instance
% A exploratory data analysis that shows the distribution of variables
% as well as a priori clusters is the first phase in any data
% processing method
% (BTW, you should include data visualization in problem description,
% because it shows the distribution of problem variables and then
% suggests ways to deal with it) - JJ
% and so on...

After this, the features describing every pattern are frequently analysed.
In this case, among the variables describing a new book, some may affect 
predictions (those variables provide information), while others may not be 
necessary and even redundant.

% You have to justify its application to this particular problem. What
% was the runnning time with all variables? What reduction was your
% target? - JJ
Moreover, the algorithm running time grows with the number of
features, making it impractical for problems with a large number of features 
\cite{Selvakuberan2008}. Feature reduction is a task that is usually
applied in these kind of situations, having two ways of performance
\cite{kittler1986feature}: feature extraction and feature selection.  
% two ways of performance? WTF? Do you mean "being performed in two
% different ways?" - JJ
%Feature selection task is usually carried out by 
%searching through all possible combinations of features, evaluating each one. 

% During the evaluation, the set of all possible features is analyzed in order 
% to find the best set of features, ranking them according to some metric.
% In this process, the feature's predictive ability along with the degree 
% of redundancy between them is considered.

Feature extraction is a ``transformation-based approach'', therefore
it transforms the original meaning of the features. This approach
involves creating a subset of new features by combinations of the
existing features, thus it is employed when the semantics of the
original dataset will not be needed by any future process. On the
contrary, feature selection attempts to retain the meaning of the
original feature set. It is part of what is called semantic-preserving
techniques, known as ``selection-based approaches". By searching
through all possible combinations of features, the aim of these
techniques is to determine a minimal feature subset to reduce
processing time, while obtaining the same or higher accuracy as the
initial feature set \cite{liu1998feature}. In the problem this work
addresses, it is very important to maintain the semantics of the set of
initial variables, and therefore several feature selection methods
have been applied. 

In addition, in order to reduce the dimensionality of the input data
and to choose an appropriate set of variables, the Self-Organizing Map
(SOM) \cite{kohonen1998} method has been also applied. % Why? Is that
                                % one of the two forms of feature
                                % selection? What do we expect to
                                % learn from it? - JJ 

%--- new SOM description % -Sam

SOM is a type of unsupervised neural network inspired by the topographical organisation of the sensory cortex of human brain \cite{kohonen1998}. It maps highly dimensional data into a lower dimensional space where similar data patterns are located in more adjacent areas on the grid.
It is a very useful analysis tool for visualising complex data with high number of variables in a simple geometric plane. Therefore, it could be used for different tasks such as feature selection and data clustering and visualisation.

Basically, SOM consists of two layers; input layer and output layer (also known as Kohen layer). Input layer is fully connected with the Kohen layer which is usually a two dimensional grid of neurons (i.e. units). This means that every neuron is linked to every input in the input layer.

The learning process of SOM is an iterative process applied in two stages. First, the weights of the network are initialized randomly then the n-dimensional input vectors are presented to the network sequentially so the distance between the weights of the neurons and the inputs are computed using the simple Euclidean distance. In the second stage, the neuron which has the shortest distance with the presented input vector is updated along with its neighbor neurons. This neuron is called the best matching unit (BMU). The goal of the learning process is to minimize the neighborhood distance in the output clusters. 

When the learning process is done a visual representation method like the Unified Distance Matrix, or U-matrix for short, is used to visualize the distances between neurons in the output space. In the U-matrix, the distance between adjacent neurons is measured and assigned different colors. Dark colors between neurons indicate large distances (boundaries) while light colors represent similar areas (clusters).

%---




This method is usually applied to analyse data as similar items tend to be 
mapped close together, while those items dissimilar, are mapped apart.
Also, the SOM graph represents and highlight very clearly those
regions (clusters) with high training sample concentration and fewer
where the samples are scarce.  
%FERGU: mencionarlo en la intro
% Which is interesting because... JJ

As far as the feature selection methods are concerned, Weka software separates the process into two parts:
%FERGU: por qué mencionamos a Weka aquí y no antes?
% Antonio - se ha mencionado, pero sí, habría que destacarlo un poco más desde la intro quizá.
\begin{itemize}
  \item Attribute evaluator: This is the method by which the feature subsets are assessed. In this work, both a correlation-based feature subset evaluation method ({\tt CfsSubsetEval}) and an adaptation of relief for attribute estimation ({\tt ReliefFAttributeEval}) evaluation method are used. 
  
  The Correlation Feature Selection (CFS) method evaluates subsets of features/variables on the basis of the hypothesis made by Hall in \cite{Hall1998}, ``A  good  feature  subset  is  one  that  contains  features  highly  correlated  with
(predictive of) the class, yet uncorrelated with (not predictive of) each other''. This evaluator needs the numeric features to be transformed to nominal features first, by being discretised. The CFS evaluation function measures the ``merit'' of the subsets, which depend on the mean feature-class correlation, and the average feature-feature intercorrelation. In addition, the correlation is measured by three variations: the Minimum Description Length (MDL), the symmetrical uncertainty, and the ``relief''.

``Relief'' is the second evaluator used in this work. It is important to note that the relief method implemented in Weka is an updated version \cite{RobnikSikonja1997}, called RRELIEFF, of the original by Kira \& Rendell \cite{Kira1992}. The algorithm works by evaluating single attributes, and not whole subsets of attributes as CFS does. Also, it is applicable even working with both nominal and numeric features, without the need for transformations. For this algorithm, the weight of an attribute depends on the difference of an attribute value inside an instance with its neighbours, being these the nearest instance of the same class, and the nearest of the different class. Therefore, a good attribute is that which gives similar values for instances of the same class, and different values for instances of different class.
  \item Search method: This describes the structured way in which the set of possible feature subsets is studied, based on the results of the evaluator. With regard to the used search criteria, it is important to take into account that CFS evaluates attribute subsets, and Relief evaluates attributes separately.

For this reason, the search method used with CFS has been BestFirst, that searches the space of feature subsets by greedy hill-climbing augmented with a backtracking facility. On the other hand, Ranker is the searching criteria used with the Relief evaluator. This method ranks features by their individual evaluations.
\end{itemize}

Finally, Section \ref{sec:experiments} provides a comparison between the feature reduction through SOM, and the feature selection techniques that have been described.

%-------------------------------------------------
\subsection{Selecting an appropriate forecasting method}

As previously stated, there are many prediction methods available in the 
literature, each one with a different parameter set that may affect the 
obtained results.

However, as in the case of new books only some descriptive data are available 
(compared to those cases in which historical data on sales of published books 
is available), not all forecasting methods can be used in this specific problem.

Thus, in this research five forecasting methods, based on the literature 
review in the introduction 
\cite{Madsen2008,ChingChin2010,Thomassey2012,Xia2012,SThomassey2014}, have been used. 
Well known forecasting methods implemented in the Weka tool
\cite{Hall2009,Witten2011} have been chosen, as these methods are
widely known, and could be very helpful for practitioners to reproduce
experiments and even to solve similar sales prediction problems using
these methods. % Why these and not others? Exploratory data analysis?
               % - JJ

Specifically, the following forecasting methods are proposed: 
        % Proposed why? - JJ
        % because those methods were those that could lead with the problem data - pedro
\begin{itemize}

 % weka.classifiers.trees.M5P
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/M5P.html
 \item \emph{M5 Model trees}: 
A decision tree consists of answer-nodes, that indicate a class, and decision-nodes, 
that contain an attribute name and branches to other sub-decision trees.
Building a decision tree can be done using many algorithms, i.e. ID3 and C4.5 \cite{Quinlan1986}.
However, in order to use this model in regression problems, some authors have 
extended the model using methods such as the M5 model tree \cite{Quinlan1986,Quinlan1992,Wang1997,WittenFrank2000}, 
by combining a conventional decision tree and generating linear regression 
functions at the nodes.

The construction of a model tree is similar to that of classical decision 
trees \cite{Solomatine2004}:
The process breaks the input space of the training data through decision points 
(nodes) to assign a linear model suitable for that sub-area of the input space. 
This process may lead to a complex model tree.

Model tree models can learn and tackle tasks with high dimensionality (up to 
hundreds of attributes) and generate reproducible and comprehensible representations, 
what makes them potentially more successful in the eyes of decision makers.

The final model consists of the collection of linear sub-models that brings the 
required non-linearity in dealing with the regression problem
and both the predicted values at the answer-nodes along with the path from the 
root to that node is given as a result.


 % weka.classifiers.lazy.IBk 
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/lazy/IBk.html
 \item \emph{k-Nearest Neighbours (kNN)}: 
The kNN \cite{Aha1991,Mitchell1997} method is an instance-based classifier in 
which the unknown instances are classified by relating them to the known 
instances using a distance measurement/function (Euclidean, Minkowsky or minimax).

The main idea behind the algorithm is that two instances far enough in the space, 
taking into account the distance function, are less likely to belong to the 
same class than two closely situated instances.

The classification algorithm locates the nearest neighbour in instance space and 
assigns the class of that neighbour to the unknown instance.
In order to improve the robustness of the model, several neighbours can be 
located, assigning the resulting class to an unclassified vector using the 
closest $k$ vectors found in the training set by majority vote.


 \item \emph{Random Forest}: This method is based in the construction of a set of decision trees using a stochastic process over the basis of C4.5 algorithm. It was proposed by Breiman \cite{Breiman2001} and aims to create independent and uncorrelated trees based on different and random input vectors, following the same distribution.
The result is the average of the generated trees during the process.


 % weka.classifiers.functions.LinearRegression
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/LinearRegression.html
 \item \emph{Linear Regression}:
 Linear regression \cite{Cohen2003,Yan2009,Rencher2102} method models the 
relationship between a scalar dependent variable (outcome) and several independent 
variables (predictor) over the range of values in the dataset. 
In this statistical technique, data are modelled using linear functions to estimate unknown model parameters, examining the linear correlations between independent and dependent variables.

This method of regression provides an adequate and interpretable description of 
how the input affects the output as it models the dependent variable as a linear 
function of the independent variables. Moreover, the linear relation can be 
solved using the least squares method, that minimises the error between the 
actual data and the regression line \cite{McClendon2015}.


 % weka.classifiers.functions.SMOreg
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMOreg.html
 \item \emph{Support Vector Machine for Regression (SVM)}:
Support vector machine (SVM) \cite{Cortes1995,Shevade1999} method is based on  
statistical learning theory and has successfully used in classification and  
regression problems \cite{Cao2003,Jari2008}.

In classification problems the algorithm searches for an optimal hyperplane 
that separates two classes, maximising the margin between two classes. 
In the case of regression the algorithm chooses a hyperplane close to as many 
of the data points as possible, minimising the sum of the distances from 
the data points to the hyperplane. 
In both cases, the hyperplane is defined by a subset of training set samples 
(called support vectors).
%This method works well even if the space is highly dimensional and the problem is not linearly separable.




\end{itemize}

The objective is finding the most appropriate forecasting method that minimises 
the error between the obtained forecast and the actual sales for each new book 
published.


%********************************************************************************
\section{Experiments and Results}
\label{sec:experiments}

% An intro should go here - JJ
% Antonio - done

This section presents the experiments conducted over the dataset considering the pre-sales variables forecasting new book sales.
Feature selection techniques have been also applied in order to test their utility by comparing the obtained results.

We have analysed the whole dataset (all the publishing houses), but also performed some experiments considering specific publishing houses as example cases.
The obtained decision trees have been plotted to show their simplicity and their real value as a decision-aid tool for publishers.

%--------------------------------------------------------------------------

\subsection{Experimental setup}

In most data mining and machine learning algorithms, the values of
some parameters have a high influence on its performance. In our
experiments, the best value of $k$ in the $k$-nearest neighbour was
empirically found at $k=3$. % What???? - JJ
 For SVM, the widely used Radial Basis
Function (RBF) kernel is selected. As reported in several studies, RBF
kernel has a reliable performance and can analyse higher dimensional
data \cite{yu2004ec,chao2015construction,huang2006ga}. Two important
parameters should be tuned in SVM: the Cost $(C)$ and $(\gamma)$ of
the RBF kernel. We apply grid search which is the most common approach
to find $C$ and $\gamma$. The best performance of SVM was found at
$C=200$ and Gamma $\gamma = 0.01$. For MLP, we apply it with one
hidden layer since it was proven in the literature that an MLP with
one hidden layer can approximate any continuous or discontinuous
function \cite{hornik1989multilayer,mirjalili2014let}. However, there
is no rule of thumb on selecting the best number of neurons in the
hidden layer. One suggested method that is followed in this work is to
set the number of neurons in the hidden layer to $2 \times I + 1$
where $I$ is the number of inputs in the dataset
\cite{wdaa2008differential,mirjalili2014let,mirjalili2015effective}. 

All experiments are performed using 10-fold cross validation \cite{Geisser1993,Kohavi1995,Devijver1982}. The dataset is partitioned into 10 parts where the training process is carried out on 9 parts then the resulted model is tested on the left 10th part. This process is repeated 10 times, each time the model is trained using different 9 parts. At the end of the process the 10 evaluation results on the testing parts are averaged. 
This technique is used to estimate how accurately the predictive model will perform in practise, limiting, at the same time, the overfitting problem.

In addition, due to the stochastic component present in some of the methods and also in order to evaluate the running time, 30 repetitions of each experiment have been done per method. Then the average and standard deviation have been computed.

All experiments have been conducted on an  Intel(R) Core(TM) i5-2400 CPU at 3.10Ghz with a memory of 4 GB running Windows 7 professional 32bit operating system and Java JRE\_1.7.0\_72.


%--------------------------------------------------------------------------

\subsection{Results for all publishers}
\label{subsec:results_all_publishers}

In the first stage of the experiments, forecasting methods have been
applied to the whole dataset. All the pre-sales features have been
considered, including the categorical ones, which make it harder to
process the instances and build the different models. SVM for
instance, must previously transform these categories into binary
variables in order to deal with the data, which means the creation of
a different (and new) variable per each possible value. Thus, in the
case of \emph{publisher} or \emph{collection} this would imply the definition of
hundreds of new variables. 

Correlation coefficient, error measures and time to build the model have been computed for every method. Obtained results are presented in Table \ref{tab:all_publishers_no_fs}.

\begin{table}
\caption{Predicting Total sales for all publishers (all the features). 10 repetitions, 10-fold cross validation.
\label{tab:all_publishers_no_fs}}
\centering{}%
\scalebox{0.85}{

\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
 & $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
M5P  & 89.0 $\pm$ 0.09  & 214.37 $\pm$ 22.96  & 634.23 $\pm$ 307.76  & 37.46 $\pm$ 3.12  & 48.32 $\pm$ 25.16  & 18.85 $\pm$ 0.61\tabularnewline
\hline 
K-NN  & 78.7 $\pm$ 0.08  & 272.29 $\pm$ 32.38  & 840.59 $\pm$ 240.11  & 47.48 $\pm$ 3.39  & 63.07 $\pm$ 10.29  & 0.001 $\pm$ 0.01\tabularnewline
\hline 
RF & 84.0 $\pm$ 0.05  & 253.37 $\pm$ 29.43  & 788.94 $\pm$ 282.01  & 44.16 $\pm$ 2.76  & 57.77 $\pm$ 8.15  & 4.91 $\pm$ 0.12\tabularnewline
\hline 
LR & 87.2 $\pm$ 0.07  & 308.68 $\pm$ 25.44  & 680.91 $\pm$ 226.41  & 54.05 $\pm$ 4.38  & 52.13 $\pm$ 18.29  & 984.68 $\pm$ 128.21\tabularnewline
\hline 
SVM  & 87.6 $\pm$ 0.04  & 248.76 $\pm$ 27.49  & 732.64 $\pm$ 260.56  & 43.37 $\pm$ 2.35  & 53.47 $\pm$ 6.29  & 6169.24 $\pm$ 281.49\tabularnewline
\hline 
\end{tabular}
}
\end{table}



As far as the different methods are concerned, M5P model obtained the best results, 
both in time and error. The advantage of M5P is that it can learn and tackle 
classification tasks that have high dimensionality and categorical variables efficiently. Moreover, M5P generates reproducible and comprehensible representations that provide information on the decision process and does not require human intervention either for the operation or for interpretation. Thus, it is very suitable model be incorporated in a publishing company 
processes.

Taking into account the obtained results by SVM model, this method obtains comparable 
errors to those of M5P. Even when using roughly tuned parameter values, it shows 
a good generalisation performance. However, SVM is much slower than the others to build the model, not only when using the dataset with categorical input variables, but also when using the 
datasets with numerical variables, not to mention the extra time needed to optimize its hyperparameters.

According to Table \ref{tab:all_publishers_no_fs}, it can be noticed also that the Linear Regression model outperformed some other complex methods like the random forest, both in time and in obtained errors. Although, Linear regression is considered as a simple model to build, more powerful models can obtain better results in comparable time.


kNN is the fastest method used in this study in terms of training time, as it is the simplest method. However, the model is slow when classifying large number of 
training examples, and it exhibits a poor generalisation ability since it does not 
learn anything from the training data.

Moreover, in these experiments it has been seen the difficulty for some classifiers like the SVM to process information related to categorical variables, such as \emph{collection} and 
\emph{binding}.

Finally, paying attention to the cost in time to obtain the forecasting models 
and their accuracy, the M5P model is the most adequate to solve the problem of 
how many books should be printed when a new book is published.
This forecasting method, with these additional advantages, can be used for products 
with historical sales data and for new products with little or no historical 
data available.



%--------------------------------------------------------------------------

\subsection{Feature selection}
\label{subsec:feature_selection}

In the second stage of the experiments, different feature selection methods have been applied. The aim of these methods is to reduce the number of features considered in developing the forecasting models which consequently simplify the complexity of the models, improve their interpretability and decrease the training time. The simpler models generated after performing feature selection could give the decision maker a better insight on the underlying process by identifying the more influencing variables. 
  
We have used both the {\tt CfsSubsetEval} \cite{Hall1998} (a correlation-based feature subset evaluation method) and the ReliefFAttributeEval \cite{RobnikSikonja1997} (an adaptation of relief for attribute estimation) evaluation methods, implemented in the Weka tool and described in Section \ref{subsec:data_preproces}.

The dataset has been also projected using SOM, and a visual analysis of the obtained results has been carried out in order to determine the best variables to consider.
An important preprocessing step has been carried out before conducting the SOM analysis, so the two categorical features \emph{collection} and \emph{binding} have been
removed from the analysis since SOM cannot deal with them without a previous transformation into numerical ones. However, since they have a large number of unique different values this would be finally impracticable and useless. 



The SOM algorithm is applied with a large map size of $26 \times 16$  in order to represent the data. The best SOM training results are obtained with a final quantisation error of 0.691 and topographic error of 0.046. 


Figure \ref{fig:componentplanes} shows the U-matrix and component
variables resulted from SOM training. The term component is used in
SOM to refer to the features. The component planes in Figure
\ref{fig:componentplanes} show the trend of values of the prototype
vectors of the SOM map units. These values are represented as
colours. The colorbar on the right of each component plane shows the
indication of each colour. On the other side, the U-matrix on the top
left corner shows the distances between the adjacent units. It is
important to note that high values in the matrix indicates large
distance between the units of the map. It can be noticed also that
there are more hexagons in the U-matrix than component planes since
the distances between units is also represented as hexagons. 

Examining the component planes shown in Figure \ref{fig:componentplanes}, it can be noticed that some variables have very similar color maps which means that they are highly correlated. In this case, we have two examples; the first is the \emph{distrib\_novelty} and \emph{print\_run}variables while the second is the \emph{tot\_points\_sale} and \emph{tot\_points\_sale\_1st\_year}. In order to make the dataset having less number of uncorrelated features with each other, one of the redundant features is removed from each pair of features. Therefore, the \emph{distrib\_novelty} and \emph{tot\_points\_sale\_1st\_year} were omitted from the dataset.


\begin{figure*}[ht]
\begin{center}
\includegraphics[scale=0.28]{som_planes.eps}
\end{center}
\caption{U-matrix and component planes.}
\label{fig:componentplanes}
\end{figure*}


The selected features per method are shown in Table \ref{tab:features_selected}. Logically, just the independent or input variables are considered.

\begin{table}
\caption{Input features/variables selected by each method
\label{tab:features_selected}}
\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
No. & Variable & ReliefFAttributeEval & CfsSubsetEval & SOM\\
\hline 
1 & \texttt{ret\_price} & $\checkmark$ &  & $\checkmark$\\
\hline 
2 & \texttt{subject1} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\hline 
3 &  \texttt{collection} &  &  & \\
\hline 
4 & \texttt{binding} &  &  & \\
\hline 
5 & \texttt{gifts} & $\checkmark$ &  & $\checkmark$\\
\hline 
6 & \texttt{distrib\_novelty} & $\checkmark$ &  & \\
\hline 
7 & \texttt{tot\_points\_sale} & $\checkmark$ &  & $\checkmark$\\
\hline 
8 & \texttt{tot\_points\_sale\_1st\_year} & $\checkmark$ &  & \\
\hline 
9 & \texttt{weeks\_sale} & $\checkmark$ &  & $\checkmark$\\
\hline 
10 & \texttt{print\_run} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\hline 
\end{tabular}
\end{table}


After applying the feature selection methods, we have run the same forecasting models as in Section \ref{subsec:results_all_publishers} for the whole dataset (all the publishers), but considering only the selected features by each method. The results are shown in Table \ref{tab:results_all_publishers_fs}. Examining the obtained results for each feature selection method, it can be seen that RFE got the best results compared to SOM and CSE. It can be noticed also that the performance of M5P, kNN and RF has been improved with RFE compared to the case of using all features. In general, all feature selection methods have reduced the training time needed to build the forecasting methods, this is because they reduced the features and excluded the categorical features that have many possible values. On the other side, CSE and SOM did not perform as good as RAE which could be explained due to the extensive reduction made by CSE and due to the human intervention in the case of SOM. 

\begin{table}
\centering{}%
\caption{Predicting Total sales for all publishers based on features selected
by each method.
\label{tab:results_all_publishers_fs}}
% RELIEFATTRIBUTEEVAL
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
\multicolumn{7}{|c|}{\textit{ReliefAttributeEval}}\\
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 89.2 $\pm$0.10 & 211.15 $\pm$22.41 & 616.35 $\pm$321.25 & 36.93 $\pm$3.41 & 47.30 $\pm$26.83 & 0.49 $\pm$0.02\\
\hline 
K-NN & 86.9 $\pm$0.09 & 231.78 $\pm$24.25 & 668.03 $\pm$205.14 & 40.48 $\pm$3.06 & 51.28 $\pm$16.81 & 0.002 $\pm$0.004\\
\hline 
RF & 91.0 $\pm$0.05 & 198.16 $\pm$19.50 & 589.35 $\pm$195.80 & 34.61 $\pm$2.27 & 44.21 $\pm$11.63 & 3.63 $\pm$0.20\\
\hline 
LR & 86.4 $\pm$0.08 & 346.88 $\pm$25.40 & 695.90 $\pm$242.49 & 60.75 $\pm$4.55 & 53.16 $\pm$19.43 & 0.01 $\pm$0.01\\
\hline 
SVM & 84.2 $\pm$0.04 & 252.87 $\pm$33.38 & 858.26 $\pm$318.23 & 44.01 $\pm$3.04 & 62.22 $\pm$7.15 & 38.97 $\pm$5.75\\
\hline 
\end{tabular}

% CFSSUBEVAL
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
\multicolumn{7}{|c|}{\textit{CfsSubEval}}\\
\hline
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 87.0 $\pm$0.09 & 266.78 $\pm$23.12 & 662.32 $\pm$245.27 & 46.66 $\pm$3.25 & 50.9 $\pm$20.35 & 0.41 $\pm$0.04\\
\hline 
K-NN & 84.0 $\pm$0.10 & 289.09 $\pm$22.83 & 743.92 $\pm$260.45 & 50.59 $\pm$3.50 & 57.30 $\pm$22.90 & 0.001 $\pm$0.001\\
\hline 
RF & 85.0 $\pm$0.11 & 278.42 $\pm$23.07 & 714.13 $\pm$278.65 & 48.71 $\pm$3.43 & 55.26 $\pm$24.41 & 2.48 $\pm$0.13\\
\hline 
LR & 85.9 $\pm$0.06 & 356.35 $\pm$22.43 & 708.06 $\pm$195.15 & 62.42 $\pm$4.22 & 53.97 $\pm$14.05 & 0.01 $\pm$0.001\\
\hline 
SVM & 86.0 $\pm$0.06 & 288.47 $\pm$30.32 & 817.56 $\pm$253.24 & 50.30 $\pm$2.38 & 60.27 $\pm$5.68 & 32.73 $\pm$5.74\\
\hline
\end{tabular}\\

% SOM
\centering{}% SOM
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
\multicolumn{7}{|c|}{\textit{SOM}}\tabularnewline
\hline 
 &  $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
M5P  & 86.6 $\pm$0.10  & 234.09 $\pm$23.19  & 699.56 $\pm$317.20  & 40.92 $\pm$3.25  & 53.70 $\pm$26.87  & 0.475 $\pm$0.12\tabularnewline
\hline 
K-NN  & 86.8 $\pm$0.09  & 237.91 $\pm$25.34  & 665.64 $\pm$212.05  & 41.55 $\pm$3.14  & 50.96 $\pm$16.72  & 0.001 $\pm$0.004\tabularnewline
\hline 
RF & 89.8 $\pm$0.05  & 211.87 $\pm$20.07  & 617.78 $\pm$187.16  & 37.02 $\pm$2.46  & 46.58 $\pm$11.14  & 2.93 $\pm$0.11\tabularnewline
\hline 
LR  & 85.5 $\pm$0.08  & 351.99 $\pm$23.72  & 714.76 $\pm$214.21  & 61.65 $\pm$4.28  & 54.67 $\pm$16.88  & 0.01 $\pm$0.01\tabularnewline
\hline 
SVM  & 84.5 $\pm$0.04  & 267.71 $\pm$33.82  & 888.33 $\pm$308.93  & 46.60 $\pm$2.87  & 64.62 $\pm$5.45  & 32.27 $\pm$4.74\tabularnewline
\hline 
\end{tabular}
\end{table}



%********************************************************************************

\section{Experiments of special publishing houses cases}
\label{sec:example_cases}

This section presents further experiments conducted on data concerning four interesting special publishing house cases. These publishing houses were selected as representative cases which could be very interesting for decision makers and managers in the domain. The four cases are described as follows:

\begin{itemize}
    \item Best seller publishing house: this case represents the publishing house that sells the highest number of copies regardless the title of the books sold. Therefore, it is an example of a case on a higher business scale.
    \item Medium seller publishing house: this represents an publishing house that sells medium number of copies compared to the other publishing houses (Medium business scale). 
    \item Highest variation publishing house: this is an example of an publishing house that follows a strategy in publishing a very wide range of different titles.
    \item Medium varied publishing house: this case represents an publishing house that has a median rank between the other publishing house regarding the variety of titles sold.
\end{itemize}


Table \ref{Table:special_cases} shows number of sold copies and number of titles published by the selected publishing houses.

\begin{table}
\caption{Description of publishing houses considered as special cases.
\label{tab:desc_publishing houses}}
\centering{}%
\begin{tabular}{|c|l|l|l|}
\hline 
 Case & Different books sold & Total copies sold & Description\\
\hline 
\textit{All publishing houses} & 6083 & 3201645 & 209 publishing houses\\
\hline 
\textit{UDL929} & 216 & 460752 & Best selling publishing house\\
\hline 
\textit{UDLW12} & 97 & 2635 & Medium selling publishing house\\
\hline 
\textit{UDL704} & 293 & 58236 & Publishing house with most different\\ 
       &     &       & books sold\\
\hline 
\textit{UDLU11} & 180 & 249481 & Publishing house with medium number\\ 
       &     &       &  of different books sold\\
\hline 
\end{tabular}
\label{Table:special_cases}
\end{table}

The experiments applied on the selected special cases follow the same methodology used previously for the whole dataset. First, the forecasting models are applied on each dataset using all features, then, the three feature selection methods (RFE, CSE and SOM) are used to reduce the set of features in each case. Second, the classification models are applied again on the reduced datasets to evaluate their performance.

Evaluation results of the developed models (without performing any feature selection method) for the selected four cases are shown in Table \ref{Table:case1results}, \ref{Table:case2results}, \ref{Table:case3results} and \ref{Table:case4results}. The results in these tables show that the classifiers performed similarly to the case of the global models for all publishers which were previously presented in Table \ref{tab:all_publishers_no_fs}. In general, M5P, RF and SVM performed better than the simple classifiers kNN and LR. In the case of Best selling publisher houses (i.e. UDL929) and publishers who publish wide range of different books (i.e. UDL704), M5P could be recommended as it shows more accurate forecasting results than the other models. While in the case of medium sized publishers, in terms of sales and number of published books, the RF model is recommended.


\begin{table}
\caption{Predicting Total sales for Publishing house-UDL929 (The best selling publishing house) }
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
\hline 
M5P  & 90.0 $\pm$0.10 & 670.24 $\pm$181.05  & 1065.34 $\pm$413.84  & 40.83 $\pm$10.86  & 43.56 $\pm$19.87  & 0.03 $\pm$0.002\tabularnewline
\hline 
K-NN  & 73.3 $\pm$0.19  & 1057.21 $\pm$385.04  & 1875.85 $\pm$916.96  & 61.88 $\pm$12.37  & 68.75 $\pm$15.98  & 0 $\pm$0.0001\tabularnewline
\hline 
RF  & 87.6 $\pm$0.11  & 771.83 $\pm$326.34  & 1490.49 $\pm$930.58  & 45.40 $\pm$14.14  & 55.91 $\pm$31.70  & 0.08 $\pm$0.004\tabularnewline
\hline 
LR  & 86.3 $\pm$0.12  & 922.33 $\pm$289.01  & 1399.84 $\pm$619.62  & 56.11 $\pm$16.17  & 56.43 $\pm$25.09  & 0.005 $\pm$0.001\tabularnewline
\hline 
SVM  & 86.7 $\pm$0.1  & 873.22 $\pm$345.16 & 1621.48 $\pm$985.24  & 50.85 $\pm$11.83  & 57.23 $\pm$20.40  & 0.04 $\pm$0.016\tabularnewline
\hline 
\end{tabular}
\label{Table:case1results}
\end{table}


\begin{table}
\caption{Predicting Total sales for PublishingHouse-UDLW12 (The median selling publishing house) }
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
\hline 
M5P  & 83.2 $\pm$0.24  & 14.18 $\pm$6.62  & 23.82 $\pm$11.86  & 43.41 $\pm$20.39  & 55.79 $\pm$30.47  & 0.009 $\pm$0.002\tabularnewline
\hline 
K-NN  & 79.3 $\pm$0.24 & 15.34 $\pm$8.71  & 26.57 $\pm$16.64  & 46.20 $\pm$23.77  & 60.09 $\pm$35.21  & 0.0001 $\pm$0.0002\tabularnewline
\hline 
RF & 85.8 $\pm$0.17  & 14.19 $\pm$6.69  & 23.57 $\pm$11.83  & 42.53 $\pm$16.80  & 53.30 $\pm$23.41  & 0.03 $\pm$0.004\tabularnewline
\hline 
LR  & 82.1 $\pm$0.25 & 14.62 $\pm$7.92  & 24.79 $\pm$15.08  & 44.57 $\pm$24.27  & 56.85 $\pm$35.07  & 0.002 $\pm$0.0005\tabularnewline
\hline 
SVM  & 83.5 $\pm$0.21  & 14.01 $\pm$7.61  & 25.61 $\pm$15.30  & 40.66 $\pm$16.03  & 53.28 $\pm$18.45  & 0.02 $\pm$0.02\tabularnewline
\hline 
\end{tabular}
\label{Table:case2results}
\end{table}

\begin{table}
\caption{Predicting Total sales for PublishingHouse704 (highest number of different titles sold)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 &  $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
\hline 
M5P  & 88.9 $\pm$0.07 & 71.26 $\pm$21.29  & 125.19 $\pm$50.09  & 39.25 $\pm$11.77  & 47.50 $\pm$15.93  & 0.05 $\pm$0.03\tabularnewline
\hline 
K-NN  & 82.6 $\pm$0.10  & 87.60 $\pm$23.21  & 158.63 $\pm$54.16  & 47.62 $\pm$10.32  & 59.17 $\pm$14.43  & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 87.1 $\pm$0.08  & 74.63 $\pm$22.17  & 139.74 $\pm$54.36  & 40.62 $\pm$10.21  & 52.22 $\pm$16.27  & 0.11 $\pm$0.01\tabularnewline
\hline 
LR  & 83.8 $\pm$0.09  & 97.51 $\pm$18.87  & 155.27 $\pm$45.56  & 54.17 $\pm$13.60  & 60.31 $\pm$21.17  & 0.01 $\pm$0.001\tabularnewline
\hline 
SVM  & 87.4 $\pm$0.07  & 78.74 $\pm$22.25  & 147.30 $\pm$57.99  & 42.43 $\pm$7.83  & 53.29 $\pm$9.35  & 0.06 $\pm$0.016\tabularnewline
\hline 
\end{tabular}
\label{Table:case3results}
\end{table}




\begin{table}
\caption{Predicting Total sales for PublishingHouse-UDLU11 (medium number of different books sold))}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
\hline 
M5P  & 77.5 $\pm$0.15  & 573.41 $\pm$192.22  & 860.31 $\pm$456.22  & 58.26 $\pm$15.87  & 66.00 $\pm$29.45  & 0.024 $\pm$0.002\tabularnewline
\hline 
K-NN  & 73.2 $\pm$0.16  & 596.62 $\pm$205.37  & 933.06 $\pm$356.11  & 60.62 $\pm$17.85  & 71.36 $\pm$22.34  & 0 $\pm$0.0002\tabularnewline
\hline 
RF & 86.7 $\pm$0.07  & 449.16 $\pm$127.80  & 671.76 $\pm$212.74  & 45.58 $\pm$9.80  & 51.37 $\pm$11.16  & 0.06 $\pm$0.01\tabularnewline
\hline 
LR  & 73.3 $\pm$0.20  & 680.16 $\pm$263.79  & 1103.97 $\pm$850.69  & 69.78 $\pm$27.24  & 86.11 $\pm$71.32  & 0.002 $\pm$0.001\tabularnewline
\hline 
SVM  & 80.4 $\pm$0.12  & 529.35 $\pm$172.50  & 846.01 $\pm$341.12  & 53.26 $\pm$11.92  & 63.54 $\pm$18.25  & 0.03 $\pm$0.02\tabularnewline
\hline 
\end{tabular}
\label{Table:case4results}
\end{table}







%--------------------------------------------------------------------------


%\subsection{Sales forecasting with feature selection}
%\label{subsec:case_sales_feature_selection}

%***TO DO***: explain we have chosen the best Feature selection method (ReliefAttributeEval???). The rest of tables have been moved to an appendix.
%Sam will do it - JJ

In the second stage of the experiments for the special cases, the feature selection methods (RFE, CSE and SOM) are applied for each case independently. For clarity purpose, we present here only the results after applying RFE method for the four cases in Table \ref{Table:case1results-RAE}, \ref{Table:case2results-RAE}, \ref{Table:case3results-RAE} and \ref{Table:case4results-RAE}. The RFE  was chosen for representation since the selected features by this method showed higher predictive power in all classifiers than the other selection methods. According to the aforementioned tables, RFE has noticeably improved the performance of all classifiers specially in the case of the best selling publisher house (i.e. UDL929) and the publisher who publishes wide range of different books (i.e. UDL704). For the rest of the results which are obtained by CSE and SOM feature selection methods we refer the reader to the appendix of tables at the end of this paper.


\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDL929 (ReliefFAttributeEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 90.5 $\pm$0.1 & 661.81 $\pm$186.17 & 1070.24 $\pm$425.66 & 40.18 $\pm$10.50 & 43.56 $\pm$19.77 & 0.02 $\pm$0.002\tabularnewline
\hline 
K-NN & 87.8 $\pm$0.11 & 751.54 $\pm$270.91 & 1257.82 $\pm$615.66& 44.57 $\pm$10.89 & 48.62 $\pm$18.73 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 88.3 $\pm$0.11 & 735.04 $\pm$326.9 & 1420.43 $\pm$941.62 & 43.56 $\pm$15.88 & 54.36 $\pm$35.31 & 0.10 $\pm$0.009\tabularnewline
\hline 
LR & 88.4 $\pm$0.11 & 868.18 $\pm$208.65 & 1231.23 $\pm$454.86 & 53.38 $\pm$14.67 & 50.60 $\pm$22.04 & 0.0006 $\pm$0.0005\tabularnewline
\hline 
SVM & 87.6 $\pm$0.10 & 860.49 $\pm$334.67 & 1560.05 $\pm$962.64 & 49.98 $\pm$9.68 & 55.07 $\pm$17.76 & 0.023 $\pm$0.003\tabularnewline
\hline 
\end{tabular}
\label{Table:case1results-RAE}
\end{table}





\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLW12 (ReliefFAttributeEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 83.7 $\pm$0.24 & 13.85 $\pm$6.56 & 23.19 $\pm$11.80 & 42.35 $\pm$20.33 & 54.08 $\pm$30.06 & 0.0063 $\pm$0.0014\tabularnewline
\hline 
K-NN & 77.2 $\pm$0.30 & 14.18 $\pm$6.63 & 23.65 $\pm$11.03 & 43.37 $\pm$19.23 & 57.00 $\pm$30.59 & 0 $\pm$0.0001\tabularnewline
\hline 
RF & 86.7 $\pm$0.16 & 13.63 $\pm$6.67 & 22.80 $\pm$11.81 & 41.07 $\pm$17.71 & 52.43 $\pm$26.23 & 0.0304 $\pm$0.0025\tabularnewline
\hline 
LR & 84.4 $\pm$0.25 & 12.79 $\pm$6.65 & 21.52 $\pm$12.54 & 39.47 $\pm$23.35 & 50.14 $\pm$35.26 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 85.2 $\pm$0.19 & 13.22 $\pm$7.39 & 24.41 $\pm$15.09 & 38.24 $\pm$15.41 & 50.46 $\pm$17.97 & 0.014 $\pm$0.002\tabularnewline
\hline 
\end{tabular}
\label{Table:case2results-RAE}
\end{table}


\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse704 (ReliefFAttributeEval-FS) }
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE  & RMSE  & RAE (\%)  & RRSE (\%)  & Time (s)\tabularnewline
\hline 
\hline 
M5P  & 89.3 $\pm$0.08  & 69.70 $\pm$19.85  & 122.48 $\pm$47.28  & 38.49 $\pm$11.53  & 46.68 $\pm$16.02 & 0.028 $\pm$0.003\tabularnewline
\hline 
K-NN  & 85.6 $\pm$0.10  & 78.39 $\pm$24.12  & 147.07 $\pm$58.02  & 42.58 $\pm$10.99  & 54.42 $\pm$15.54  & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 87.7 $\pm$0.08  & 73.13 $\pm$22.95  & 137.60 $\pm$56.14  & 39.79 $\pm$10.69  & 51.35 $\pm$17.00  & 0.13 $\pm$0.01\tabularnewline
\hline 
LR  & 87.3 $\pm$0.06  & 89.72 $\pm$18.31  & 140.65 $\pm$47.12  & 49.45 $\pm$10.50  & 53.20 $\pm$14.16  & 0.0008 $\pm$0.0005\tabularnewline
\hline 
SVM  & 87.7 $\pm$0.07  & 77.30 $\pm$22.25  & 146.46 $\pm$57.68  & 41.67 $\pm$7.99  & 53.10 $\pm$9.51  & 0.05 $\pm$0.02\tabularnewline
\hline 
\end{tabular}
\label{Table:case3results-RAE}
\end{table}




\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLU11 (ReliefFAttributeEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 76.6 $\pm$0.17 & 592.49 $\pm$209.25 & 894.76 $\pm$508.14 & 60.21 $\pm$18.22 & 69.14 $\pm$37.35 & 0.02$\pm$0.001\tabularnewline
\hline 
K-NN & 82.7 $\pm$0.09 & 487.00 $\pm$140.27 & 741.81 $\pm$238.56 & 49.27 $\pm$10.50 & 56.62 $\pm$12.59 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 85.3 $\pm$0.09 & 455.7 $\pm$123.96 & 686.54 $\pm$198.16 & 46.32 $\pm$10.20 & 53.03 $\pm$13.05 & 0.07 $\pm$0.003\tabularnewline
\hline 
LR & 64.4 $\pm$0.20 & 817.21 $\pm$331.16 & 1324.05 $\pm$1090.85 & 83.22 $\pm$32.01 & 101.90 $\pm$87.83 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 71.4 $\pm$0.14 & 644.42 $\pm$203.43 & 1009.66 $\pm$371.83 & 64.47 $\pm$11.70 & 75.54 $\pm$15.99 & 0.018 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\label{Table:case4results-RAE}
\end{table}







%--------------------------------------------------------------------------

\subsection{Decision trees for best seller}
\label{subsec:decision_trees}

In this section we show the output of one of the models built during the regression, namely we present one of the decision trees generated by M5P method, to demonstrate the value of the proposal to generate an useful decision-aid tool for and expert (publisher) to see what sales can be expected for the different values of the input variables (including the print-run).

%***TO DO:*** better explain this
% Sam will do it 

We have considered the publisher that sells the most as a good example case to show.

Figure \ref{fig:decision_tree_best_seller} depicts the obtained decision tree for the best seller publishing house (UDL929). The leaves correspond to linear regression models that can be computed (easily) to get the final predicted value for book sales. This tree has been created considering the 10 pre-sales features.

\begin{figure}[!ht] 
\begin{center}
  \epsfig{file=decision_tree_no_fs.eps,width=12cm}
\caption{Decision Tree built by the M5P method for the best seller publishing house with all the pre-sales features.}
\label{fig:decision_tree_best_seller}
\end{center}
\end{figure}

%***TO DO***: comments on the figure\\

Figure \ref{fig:decision_tree_best_seller_fs} shows the decision tree obtained for the same publishing house (the same data, but just considering the selected features by ReliefAttributeEval method, since it is the one which yields the best results).

%***TO DO***: better justify this\\

\begin{figure}[!ht] 
\begin{center}
  \epsfig{file=decision_tree_fs.eps,width=12cm}
\caption{Decision Tree built by the M5P method for the best seller publishing house with the features selected by the ReliefAttributeEval method.}
\label{fig:decision_tree_best_seller_fs}
\end{center}
\end{figure}

%***TO DO***: comments on the figure\\

%********************************************************************************
\section{Conclusions and Future Work}
\label{sec:conclusionsAndFutureWork}

%***TO DO***: rewrite these conclusions with the new experiments, examples and results\\

In this paper, the problem of how many books should be printed 
when a new book is published is faced using several data-mining and
forecasting methods.

This is a very challenging problem in this industry, because printing a higher 
number of volumes than those finally sold will lead to losses, 
while printing an adequate number of copies will optimise sales and company earnings.
In addition, there are several difficulties inherent to the new book sales forecasting, 
such as dealing with limited data, finding adequate forecasting methods, and 
selecting the best method to use.

%La investigación descrita en este trabajo se ha llevado a cabo conjuntamente con una empresa editorial que ha facilitado un conjunto de datos con los que trabajar, así como con la ayuda de un experto en ventas de este campo para validar los resultados obtenidos.
A dataset consisting of 3159 books provided by the Spanish %FERGU: mover este párrafo debajo del siguiente
publishing company Trevenque Editorial S.L. has been used.

In this paper, a data visualisation method has been used to find out what are 
the relevant variables describing a book in the prediction of book sales.
Then, several standard data mining models for sales forecasting have been used 
to forecast new book sales, and from them deduce print-run, based on those 
variables.

The most relevant variables when predicting sales have been determined, 
while unrelated or superfluous ones have been removed, 
and sales predictions have been obtained using five different forecasting 
standard methods.
Not only the obtained forecasting error has been taking into account to evaluate
those methods, but also the cost of model building as parameter setting and 
the required time to train the model.

In this sense, the decision trees proved to be a suitable model because of the 
ease to explain how the prediction is obtained from the variables that describe 
the new book. 
This, compared to the black-box models such as the artificial neural network, 
was decisive from the point of view of the needs of experts and Trevenque company.
Moreover, proposed method was validated by the company and incorporated in a 
business intelligence tool for publishing houses. %FERGU: no hay más info de esto? estaría guay describirlo

%FERGU: no mencionais el SOM en las conclusiones

Overall, this study contributes to the literature by proposing the use of different 
soft-computing standard techniques to solve a new challenging problem.
Moreover, proposed method could be implemented, not only in the publishing 
industry, but also in other domains where the specificity of products is similar, 
and might be of interest to other academic researches and industrial practitioners.


%---------- trabajo futuro

As future work, it would be interesting to compare the obtained forecasting results 
with other methods based on soft-computing, such as genetic programming or fuzzy logic.
Moreover, as default parameters have been used for proposed forecasting methods,
it would be of interest carrying out some parameter tuning procedure in order to
improve obtained results.

Furthermore, since there are some characteristics that may affect product sales 
significantly, another way to improve sales predictions could be making a thorough 
study of the effect of the application of discounts and promotions. 

Finally, there are some variables over which the published has some
control once the book has been published, such as the initial print
run or the books given out as gifts to reviewers or what proportion is
going to be sent to each possible point of sale. This can be
incorporated into an optimisation model that optimises those values to
obtain desired sales. This is left as future work.

% Antonio - TO BE INCLUDED IN THIS SECTION
% The method presented here provides a reliable data processing
% technique for editorial data and also yields a reasonable result
% that allows publishers to forecast sales and then create the
% appropriate print run without incurring in losses. 
%
% Besides, if this is true:
% It also provides a method that is able to provide insight on the
% decisions that have an influence on the print run for publishers. 



%********************************************************************************
\section*{Acknowledgements}

***TO DO***: revise the projects\\

This work has been supported in part by projects 
PreTEL (PRM Consultores - Trevenque S.L.),
TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness), 
SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico), 
PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigaci{\'o}n), 
PROY-PP2015-06 (Plan Propio 2015 UGR), 
and project CEI2015-MP-V17 of the Microprojects program 2015 from CEI BioTIC Granada.

%********************************************************************************
\bibliographystyle{plain}
\bibliography{refs}

%----------------------------------------------------------------------

\appendix
\section{APENDIX TABLES}

Case 1-A: Developing prediction models to predict Total sales for
PublishingHouse704 with CfsSubsetEval feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse704 (CfsSubsetEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 80.1 $\pm$0.09 & 104.87 $\pm$21.38 & 174.57 $\pm$47.33 & 58.11 $\pm$14.73 & 67.58 $\pm$22.61 & 0.018 $\pm$0.003\tabularnewline
\hline 
K-NN & 78.2 $\pm$0.12 & 99.45 $\pm$24.31 & 170.83 $\pm$46.34 & 54.53 $\pm$12.28 & 65.86 $\pm$18.34 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 77.6 $\pm$0.13 & 101.93 $\pm$23.66 & 180.56 $\pm$51.20 & 56.10 $\pm$13.62 & 69.86 $\pm$23.27 & 0.0738 $\pm$0.003\tabularnewline
\hline 
LR & 80.1 $\pm$0.10 & 106.62 $\pm$21.52 & 173.18 $\pm$46.58 & 59.11 $\pm$14.96 & 67.53 $\pm$24.53 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 81.8 $\pm$0.08 & 94.29 $\pm$25.52 & 180.54 $\pm$62.67 & 50.79 $\pm$8.48 & 65.62 $\pm$6.25 & 0.040 $\pm$0.02\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-A: Developing prediction models to predict Total sales for
PublishingHouse704 with SOM feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse704 (SOM-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 89.2 $\pm$0.07 & 71.536 $\pm$18.95 & 123.06 $\pm$42.13 & 39.43 $\pm$10.58 & 47.20 $\pm$15.16 & 0.031 $\pm$0.007\tabularnewline
\hline 
K-NN & 85.8 $\pm$0.09 & 79.42 $\pm$21.47 & 140.95 $\pm$44.93 & 43.34 $\pm$10.07 & 53.31 $\pm$14.37 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
RF & 87.7 $\pm$0.08 & 73.98 $\pm$20.91 & 134.11 $\pm$51.70 & 40.48 $\pm$10.51 & 50.41 $\pm$16.38 & 0.10 $\pm$0.0076\tabularnewline
\hline 
LR & 87.5 $\pm$0.06 & 88.70 $\pm$16.34 & 135.03 $\pm$35.18 & 49.08 $\pm$10.47 & 51.97 $\pm$13.65 & 0.0006 $\pm$0.0005\tabularnewline
\hline 
SVM & 87.8 $\pm$0.06 & 81.09 $\pm$22.48 & 154.82 $\pm$59.93 & 43.65 $\pm$7.36 & 55.72 $\pm$7.15 & 0.031 $\pm$0.003\tabularnewline
\hline 
\end{tabular}
\end{table}


%-----------------------------------


Case 1-B: Developing prediction models to predict Total sales for
PublishingHouse-UDLU11 with CfsSubsetEval feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLU11 (CfsSubsetEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 64.0 $\pm$0.23 & 777.94 $\pm$342.04 & 1305.28 $\pm$1075.29 & 78.37 $\pm$30.4 & 99.86 $\pm$85.43 & 0.013 $\pm$0.001\tabularnewline
\hline 
K-NN & 64.8 $\pm$0.18 & 702.70 $\pm$181.41 & 1008.39 $\pm$305.87 & 71.41 $\pm$13.95 & 78.47 $\pm$22.70 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
RF & 65.3 $\pm$0.19 & 685.56 $\pm$195.28 & 1021.37 $\pm$352.88 & 69.74 $\pm$16.96 & 79.37 $\pm$26.98 & 0.0466 $\pm$0.0023\tabularnewline
\hline 
LR & 67.0 $\pm$0.23\ & 857.22 $\pm$308.69 & 1352.26 $\pm$945.31 & 86.10 $\pm$24.25 & 102.89 $\pm$74.43 & 0.0003 $\pm$0.0005\tabularnewline
\hline 
SVM & 64.8 $\pm$0.23 & 879.91 $\pm$280.33 & 1402.82 $\pm$593.44 & 87.74 $\pm$15.27 & 105.40 $\pm$37.73 & 0.017 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-B: Developing prediction models to predict Total sales for
PublishingHouse-UDLU11 with SOM feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLU11 (SOM-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 79.0 $\pm$0.14 & 573.25 $\pm$167.88 & 827.55 $\pm$388.16 & 58.42 $\pm$14.14 & 63.98 $\pm$25.63 & 0.0153 $\pm$0.0015\tabularnewline
\hline 
K-NN & 83.6 $\pm$0.10 & 492.18 $\pm$126.86 & 727.33 $\pm$209.88 & 50.12 $\pm$10.69 & 56.14 $\pm$12.89 & 0 $\pm$0.0002\tabularnewline
\hline 
RF & 85.6 $\pm$0.08 & 446.25 $\pm$119.79 & 673.46 $\pm$189.06 & 45.38 $\pm$9.78 & 52.24 $\pm$13.09 & 0.0594 $\pm$0.0049\tabularnewline
\hline 
LR & 71.0 $\pm$0.19 & 723.74 $\pm$247.96 & 1108.74 $\pm$736.17 & 74.14 $\pm$24.91 & 85.96 $\pm$59.99 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 71.6 $\pm$0.15 & 640.14 $\pm$206.46 & 1017.02 $\pm$376.13 & 63.96 $\pm$11.87 & 76.10 $\pm$16.80 & 0.019 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\end{table}


%-----------------------------------


Case 2-A: Developing prediction models to predict Total sales for
PublishingHouse-UDL929 with CfsSubsetEval feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDL929 (CfsSubsetEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 88.0 $\pm$0.13 & 738.28 $\pm$188.53 & 1141.79 $\pm$472.025 & 45.45 $\pm$13.62 & 47.58 $\pm$24.84 & 0.017 $\pm$0.002\tabularnewline
\hline 
K-NN & 85.3 $\pm$0.15& 825.19 $\pm$235.34 & 1280.33 $\pm$523.06 & 50.23 $\pm$14.07 & 51.42 $\pm$21.24 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
RF & 86.4 $\pm$0.14 & 790.09 $\pm$225.86 & 1245.23 $\pm$584.23 & 48.22 $\pm$13.79 & 50.01 $\pm$21.97 & 0.063 $\pm$0.0039\tabularnewline
\hline 
LR & 86.6 $\pm$0.12 & 936.56 $\pm$204.46 & 1329.24 $\pm$480.79 & 58.04 $\pm$16.92 & 55.41 $\pm$25.62 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 86.8 $\pm$0.12 & 940.39 $\pm$344.16 & 1648.50 $\pm$1026.04 & 54.50 $\pm$7.12 & 56.17 $\pm$7.8 & 0.020 $\pm$0.002\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-A: Developing prediction models to predict Total sales for
PublishingHouse-UDL929 with SOM feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDL929 (SOM-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 89.7 $\pm$0.10 & 685.67 $\pm$185.99 & 1116.42 $\pm$478.06 & 42.09 $\pm$12.8 & 46.31 $\pm$24.61 & 0.02 $\pm$0.004\tabularnewline
\hline 
K-NN & 89.0 $\pm$0.10 & 746.45 $\pm$250.76 & 1203.41 $\pm$564.84 & 44.36 $\pm$9.73 & 46.25 $\pm$14.41 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 89.5 $\pm$0.09 & 708.24 $\pm$264.52 & 1270.26 $\pm$708.82 & 42.14 $\pm$12.45 & 49.07 $\pm$26.59 & 0.080 $\pm$0.0058\tabularnewline
\hline 
LR & 86.6 $\pm$0.13 & 894.08 $\pm$206.60 & 1282.83 $\pm$451.87 & 55.29 $\pm$15.92 & 53.74 $\pm$24.70 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 87.1 $\pm$0.10 & 908.56 $\pm$348.20 & 1608.77 $\pm$972.55 & 52.63 $\pm$8.75 & 55.95 $\pm$12.49 & 0.022 $\pm$0.002\tabularnewline
\hline 
\end{tabular}
\end{table}


%-----------------------------------

Case 2-B: Developing prediction models to predict Total sales for
PublishingHouse-UDLW12 with CfsSubsetEval feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLW12 (CfsSubsetEval-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 81.6 $\pm$0.22 & 15.67 $\pm$6.58 & 25.63 $\pm$11.84 & 47.93 $\pm$18.81 & 60.72 $\pm$31.75 & 0.006 $\pm$0.002\tabularnewline
\hline 
K-NN & 75.2 $\pm$0.27 & 15.73 $\pm$7.73 & 26.68 $\pm$14.82 & 48.58 $\pm$24.77 & 66.23 $\pm$48.59 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
RF & 75.1 $\pm$0.26 & 15.61 $\pm$7.19 & 26.83 $\pm$13.36 & 47.38 $\pm$20.10 & 64.65 $\pm$38.66 & 0.0224 $\pm$0.0027\tabularnewline
\hline 
LR & 81.6 $\pm$0.22 & 15.67 $\pm$6.55 & 25.49 $\pm$11.63 & 47.97 $\pm$18.85 & 60.28 $\pm$30.62 & 0.0002 $\pm$0.0004\tabularnewline
\hline 
SVM & 81.8 $\pm$0.21 & 17.71 $\pm$9.70 & 31.95 $\pm$19.26 & 50.07 $\pm$14.93 & 65.12 $\pm$16.36 & 0.013 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-B: Developing prediction models to predict Total sales for
PublishingHouse-UDLW12 with SOM feature selection.

\begin{table}[!ht]
\caption{Predicting Total sales for PublishingHouse-UDLW12 (SOM-FS)}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & $R^{2}$  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 86.0 $\pm$0.19 & 13.84 $\pm$6.75 & 23.94 $\pm$11.64 & 41.65 $\pm$17.56 & 55.44 $\pm$27.13 & 0.006 $\pm$0.001\tabularnewline
\hline 
K-NN & 74.0 $\pm$0.29 & 15.93 $\pm$8.52 & 28.28 $\pm$16.28 & 48.83 $\pm$25.89 & 68.75 $\pm$48.85 & 0 $\pm$0.0002\tabularnewline
\hline 
RF & 85.5 $\pm$0.19 & 13.66 $\pm$6.47 & 22.51 $\pm$11.03 & 41.38 $\pm$17.87 & 52.20 $\pm$27.09 & 0.026 $\pm$0.0019\tabularnewline
\hline 
LR & 85.4 $\pm$0.21 & 14.59 $\pm$7.13 & 24.40 $\pm$12.87 & 44.81 $\pm$23.65 & 57.73 $\pm$38.63 & 0.0003 $\pm$0.0005\tabularnewline
\hline 
SVM & 84.4 $\pm$0.18 & 14.18 $\pm$7.96 & 26.49 $\pm$16.54 & 40.93 $\pm$16.65 & 54.37 $\pm$20.00 & 0.01 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\end{table}


\end{document}
%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
