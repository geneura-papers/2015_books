\documentclass{llncs}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[dvips]{epsfig}
\usepackage[latin1]{inputenc}

\def\CC{{C\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}~}
\addtolength{\textfloatsep}{-0.5cm}
\addtolength{\intextsep}{-0.5cm}


%%%%%%%%%%%%%%%% Titulo %%%%%%%%%%%%%%%
\title{Enhancing the sales prediction of newly published books by means of computational intelligence methods} 

% Antonio - El título habría que cambiarlo a otro más descriptivo, creo yo. A ver si os gusta este

%%%%%%%%%%%%%%%% autores %%%%%%%%%%%%%%%
\author {
P.A. Castillo$^1$, A.M. Mora$^1$, J.J. Merelo$^1$, H. Fariss$^2$, \\ P. Garc\'{\i}a-S\'anchez$^1$, A.J. Fern\'andez-Ares$^1$, M.G. Arenas$^1$, P. de las Cuevas$^1$
}
\institute{
$^1$Department of Computer Architecture and Computer Technology. CITIC. University of Granada (Spain)\\
$^2$Business Information Technology Department. King Abdullah II School for Information Technology. The University of Jordan. Amman (Jordan)\\
           e-mail: {\tt pacv@ugr.es}
}

\date{} 

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
When a new book is launched the publisher faces the problem of
how many books should be printed for delivery to bookstores. Printing too many is the main issue,
since it implies a loss of investment due to inventory excess. In this
paper we are tackling the problem of predicting total sales in order to 
print the right amount of books, doing so even before the book has
reached shelves. 
Several years 
% Antonio - Decir cuántos años son, que no me acuerdo.
of data on published books in Spain have been processed by means of data visualization and other feature selection techniques. The aim is to find out what are the variables that have more impact on sales, and then use machine learning and forecasting methods to predict sales, and, from them, estimate the print-run.
The described methodology improves the forecasting running time and is eventually able to predict sales with remarkable accuracy. Moreover, the proposed method has been validated by the company that provided the data and incorporated in a business intelligence tool for publishers,
since the obtained models provide a reliable guidance on the decision process of
publishing a book, without human intervention 
either for the operation or for interpretation.
% Antonio - esto es cierto??? :D
\end{abstract}


%********************************************************************************

\section{Introduction}

Publishing a book, as releasing any other product, implies several
types of risks and costs due to
the complexity and derived expenses of the production and distribution processes. 
% tienes que enmarcar el problema. Se trata de productos culturales
% con soporte físico y con ventas relativamente pequeñas. Libros,
% discos, no sé si alguna cosa más. - JJ
A newly launched product could be, additionally, completely new to the company (not a new version or edition of a previous product), and sales forecasts can
only be based on the experience of other similar products. 
In this scope, having accurate estimations of future sales of the new
product is extremely important \cite{ChingChin2010}, so that production
runs do not exceed predicted sales. 
Indeed, several studies have demonstrated that improving sale predictions 
(a reduction of forecasting errors) results in an enhancing in the 
production process \cite{Bayraktar2008,Fildes2010,Saeed2008,Zhao2002}.

% In this sense, the publishing industry is based on very particular and
% established production process, and publishing a book requires several
% steps, between which the distribution is important and plays an
% important role. The publishing process and individual decisions at each step are very
% important. One of those decisions is the number of copies to print. 
  % ¿Esto que tiene que ver con la frase anterior? - JJ
% Thus, a suitable prediction system for new books should take into
% account many details about how the editorial process works. % y? - JJ

% Implementing a good forecasting tool requires not only a good study of
% the state of the art in this field, but also an accurate knowledge of
% the operations and challenges facing the publishing industry and its
% supply chain. % ¿eso es lo que vamos a hacer? - JJ

Book publishers, as well as other content producers, release new {\em
  products} more frequently than other industries, so that they face the
problem of predicting sales quite often. The issue, in this case, is to print an
adequate number of copies but not too many, as the unsold volumes will
lead to losses and sunk inventory cost, 
% Antonio - 'sunk inventory cost' es corecto?
whereas if the number of printed copies was not enough, you can always print more. 
% Antonio - Esto habría que matizarlo más, de hecho, decirlo va un poco en contra de nuestro artículo, ya que simplifica el problema. Yo indicaría que errores en predicciones por exceso o por defecto tienen MUCHAS pegas (ambos casos), si bien el exceso es peor, claro, pero decir que lo otro es también muy negativo y por eso hay que ajustar muy bien la predicción. ;)
% Se podría señalar, por ejemplo:
% - repetición de costes de distribución
% - mala impresión al cliente/lector, que pueden repercutir negativamente en las ventas
% - algunas más... :D
In this sense, errors in one or the other direction do not have the same impact, although a temporal scarcity of a book that is selling well is also a problem.
% Antonio - esto habría que rescribirlo de acuerdo a lo anterior que comento. ;)

Thus, it is extremely important to develop an accurate predictive tool which could accurately forecast the future sales a new book will achieve, in order to optimize production and to improve the publishing company profits or minimize losses \cite{Zhao2001}. This is the main objective of this paper.
An example is the optimization of sales \cite{Mattila2002} by
improving profit margins by selling the whole print-run. 
% Antonio - podrías extender un poco el ejemplo? Es decir, describir un poco más lo que se hace en ese trabajo. ;)

However, designing such an accurate predictive tool is a difficult task in this scope, as book sales are influenced by several factors that make them fluctuate and, moreover, usually they might be not controllable and even unknown. 
For instance, some are related with decisions on purchases, others with freight 
traffic, or others with the distribution strategy \cite{Little1998}. 

Thus, it is very important to know the product and the variables that
might affect the sales, and how an expert manager takes them into account \cite{Armstrong2001,SThomassey2014}. 
                                % ¿Esto también pasa en la industria de la
                                % moda. ¿Qué vamos a usar de esto? - JJ
                                % [Pedro] esto iba referido a la Industria Editorial.
Their impact is difficult to estimate and it is not constant over time, 
hence the difficulty to adequately identify and measure their influence \cite{DeToni2000}.
In addition, some may have correlations between them. 
However, these correlations can be detected using data analysis methods, 
such as Self-Organizing Maps (SOM) \cite{kohonen1998} 
or stepwise regression analysis (SRA) \cite{Chang2009}. % What do they do? What are the results?

In this research, the problem of how many units should be printed 
when a new book is published is faced using data provided by the 
Spanish publishing company Trevenque Editorial \footnote{{\tt http://editorial.trevenque.es}},
a company that provides management systems and services for book shops,
% Antonio - 'libraries' o 'book shops'? Supongo que lo segundo. ;D
publishers and distributors. 
Usually, the company's expert sales managers analyzed historical data 
and applied their knowledge using external variables to compute correct estimations (predictions), for instance, she can decide increasing or decreasing the sales prediction based on season or sales period.
                        % varias citas - JJ
                        % [Pedro] lo modifico para comentar quienes nos facilitaron los datos
                        %  y que así era como funcionaban hasta ese momento.
%In any case, in general companies usually use a forecasting model and then, 
%an expert applies his knowledge, as the company might require an 
%interpretable and understandable model. 
However, carrying out the analysis and forecasting process as a human expert,
may lead to a 'fuzzy-like' analysis, which may not take all the variables into account. 
In addition, the process might be tedious, especially when the amount
of data is quite large, and even, different experts can reach different
predictions from the same dataset \cite{ChingChin2010,Sanders1994}.

For these reasons, the industry requires the best suited forecasting
techniques to be adopted so that, estimating sales task is carried out
reliably, accurately, and in an automatic way. 

With this issue in mind, in this paper a data visualization and preprocessing strategy to find out what are the relevant variables describing a book in the prediction of book sales is proposed; then several standard data mining models 
for sales forecasting are evaluated so a publishing company 
can deduce and optimize print-run.
% Antonio - cambiar este párrafo para describirlo como un proceso en dos pasos:
% - análisis y extracción
% - predicción

% Antonio - Además habría que justificar bien (con citas) el por qué de la extracción/selección de características, por ejemplo:
% - menor tiempo de procesamiento
% - mejores resultados (si algunas variables interfieren entre sí)

The rest of this paper is structured as follows: next, 
Section \ref{sec:soa} presents a comprehensive review of the
approaches found in the bibliography related to the prediction of
sales from product characteristics. 
Then, Section \ref{sec:problem} describes the 
problem of how many books should be printed when a new book is published, 
        % no es el problema de publicación, es el problema de venta
        % de libros sin publicar. Habrá que buscar un nombre - JJ
        %[Pedro] cierto, lo cambio, aunque me ha quedado un nombre muy largo  :)
 followed by Section \ref{sec:methodology} where the
experimental setup and the methodology considered in the study is
presented.  %FERGU: mejor described para no repetir
Section \ref{sec:experiments} reports and analyzes the obtained results with a 
real dataset. Finally, conclusions and future work are presented in Section
\ref{sec:conclusionsAndFutureWork}. 
% Antonio - TODO: rescribir con los contenidos finales del artículo. ;)

%********************************************************************************
\section{State of the art on sales forecasting} %FERGU: State of the art on sales forecasting?  ok  ;)
\label{sec:soa}

% [Pedro] no se encuentra gran cosa sobre predicción de ventas de libros
%  por eso quiero comenzar hablando de la aplicación de métodos
%  de predicción en la industria en general, que sí hay bastante bibliografía.
% Antonio - me parece bien

% Antonio - Cosas generales a mejorar en el SotA:
% - habría que comparar más veces este trabajo con lo que hacen otros, destacando nuestras diferencias. Sólo se menciona al final de la sección
% - habría que destacar claramente ls aportes de este trabajo al SotA, cómo hace que avance ;)
% - habría que incluir citas a artículos de la revista que elijamos y comentarlos aquí

Sales forecasting is a problem which has been addressed several times in the literature. Most of the studies have been conducted using different analysis and forecasting methods, such as regression models \cite{Papalexopoulos1990}, neural networks \cite{Yoo1999} or fuzzy systems \cite{Mastorocostas2001}, applied to different industrial sectors.
% ¿Y qué? Esta frase
% no aporta nada. La primera frase
% tiene que enmarcar el estado del
% arte - JJ
% Antonio - la he intentado mejorar.

Specifically, Time-Series prediction methods \cite{Chu2003,Brown1959,Winters1960,Box1969,Papalexopoulos1990} 
is perhaps the most used technique to tackle the sales forecasting problems. 
%FERGU: terminar esta frase con punto en vez de coma para que no sea larga.   OK
The efficiency of these techniques strongly depends on the field of application 
and the correctness of the problem data. However, since they require a large amount of data for predicting sales, these methods are not the most suitable for this task \cite{ChingChin2010}.

This is the case of new book sales forecasting, a problem with issues such as 
dealing with quite limited data (just a few variables are known in advance) or the existence of several categorical values (which are harder to be processed). Given these issues, finding adequate forecasting techniques and selecting 
the best method to use is also a problem \cite{ChingChin2010}.
Moreover, in the case of new unpublished-books, there is no historical data to define patterns, and only some descriptive data about the new book are available 
\cite{ChingChin2010,FaderHardie2005,Madsen2008}. Thus, not any time-series forecasting method can be used \cite{Madsen2008,ChernWSF15}. 

%This is the new book sales forecasting case, for which no historical data to make predictions is available \cite{ChingChin2010,FaderHardie2005,Madsen2008} and only some descriptive variables relative to the new unpublished-book are available.

%Forecasting sales of new books is a problem with issues such as dealing with limited data, finding adequate forecasting methods and selecting the best method to use \cite{ChingChin2010}.

%In the case of already published books, available historical data might help to generate predictive models.
%However, in the case of new books there is no historical data to define patterns, and only some descriptive data about the new book is available. 
%In these cases not any timeseries-based forecasting method can be used \cite{Madsen2008,ChernWSF15}. 

%An additional issue is the difficulty to compare our model with classical forecasting methods, such as time series prediction methods, since books do not have specific sales time series.

For this reason, usually the methods used to predict book sales have been generally based on experts' experience. They analyze data about sales and, taking into account their knowledge in industry, their experience, and their perception about trends, could make decisions on the companies production (how many books should be printed when a new book is published).
This is a complex problem, since the predictions are influenced by external variables that must be taken into account, such as seasonality, promotions or fashions that expert managers might subjectively apply \cite{Lapide1999,ChingChin2010,ChernWSF15}.

  %[pedro] sobre la industria editorial no hay trabajos sobre predicción de ventas.
  % Lo habitual ha sido usar el conocimiento de expertos para realizar predicciones 
  %  o aconsejar acerca de la cantidad de libros a imprimir.
  % Lo más parecido que he encontrado son dos trabajos (uno en realidad) de Moon et al. 
  %  sobre "Use Blog Information As Book Sales Prediction":
  %     Moon2010ICSSSM
  %     Moon2010ICEC
Recently, some works propose analyzing relationships between on-line information 
and book sales \cite{Moon2010ICSSSM,Moon2010ICEC}. Specifically, Moon et al. propose using the number of blog references as an indicator of the success of a book, and thus its sales. Different sales patterns are observed by analyzing data obtained.
% Antonio - describir esos 'sales patterns'. ;)
%
However, the authors used in these papers historical data from published books, 
%FERGU: a qué research se refiere? A la nuestra o a la de la referencia? Dejarlo más claro, por si acaso.   ok
so their proposed methodology can not be applied in the problem of unpublished-books sales prediction.

%
% pasar a contar los casos de la predicción de ventas en la 
% industria de la moda (SThomassey2014) y de nuevos productos (ChingChin2010)
%
% Son problemas muy similares, ya que no hay datos históricos de los nuevos productos,
% al igual que no los hay sobre ventas, y en los que además influyen mucho las variables 
% externas (estacionalidad, modas, fama del author, etc).
% Hay que echar mano de productos similares o simplemente de los datos que describen el nuevo producto.
%

In order to deal with this issue, i.e. forecasting sales of new products, some authors have used different data mining methods \cite{Hammond1990,Chang2009,ChingChin2010,Thomassey2012,Xia2012}. 
% Antonio - quizá habría que comentar alguno o todos bervemente...
%
In the same line, Thomassey et al. \cite{SThomassey2014} propose using a clustering method together with decision trees for sales forecasting in textile-apparel fashion industry, %FERGU: appared? apparel, creo.  ¡sí!
a very similar issue to the new book publishing one.
The same problem was previously faced by Xia et al. \cite{Xia2012} using a 
forecasting method based on extreme machine learning with adaptive metrics of inputs.
% Antonio - of -> as???
The complexity of this problem relies on the lack of historical data, on the short lifetime of the majority of items, 
% Antonio - ver si está bien el cambio 'large number' -> 'majority'
and on the influence of variables such as promotions, fashions, or economic environment \cite{Thomassey2012,Xia2012,SThomassey2014}, as mentioned above.

  %[Pedro] otro caso muy similar es el paper de ChingChin2010 sobre:
  % New tea product
  % New cosmetic product
  % New soft drink product
In \cite{ChingChin2010}, a decision-support system for new product sales %FERGU: The work of Chinchin et al.?
forecasting is proposed to solve three real-world sales forecasting problems, namely: new tea, cosmetics, and soft drink products. This is addressed taking into account quantitative variables.
% Antonio - qué significa eso?
In that study, products are classified based on their sales patterns, so it is assumed that products in the same class would follow a similar sales pattern. %FERGU: "_a_ similar sales pattern", mejor? ok
However, as the authors state, the proposed system can not deal with qualitative 
data related to the products. Moreover, real-world cases, such as consumer 
electronics and fashion products, should be deeply analyzed, as these industries 
present/launch new products every season. 

  %[Pedro] otro caso a comentar es el paper de Chang2009 sobre: 
  % sales forecasting in printed circuit board industry.
In those cases where historical data is available, classical time series 
forecasting methods can be applied, which is what the authors do in \cite{Chang2009}, where a hybrid model integrating K-means and fuzzy neural networks to forecast the future sales of a printed circuit board is proposed.
A similar method is described by Chern et al. \cite{ChernWSF15}, who analyze 
historical data along online reviews, reviewer characteristics and review 
influences to understand how electronic `word-of-mouth' influences product sales. The proposed method is suitable for those sales forecasting problems with a big amount of online reviews and historical data.

Thus, taking into account the kind of problem we address in this paper, i.e. the prediction of sales of a new product for which no historical data is available, the classical forecasting methods based on time-series are not
adequate, although they can obviously be used once the book has been launched for sale. 
In our work the relevant variables in the prediction of sales are identified
by means of data visualization and feature selection methods. Then, machine learning techniques are applied to predict sales and to infer the ideal print-run of a book.
In addition, since publishers need models to explain obtained predictions, 
`black box' forecasting methods may not be suitable, so models based on decision rules have been tested as an alternative.



%********************************************************************************
\section{The problem}
\label{sec:problem}


The main issue of this paper is to forecast sales for 
unpublished books for which no historical data is available, just some
descriptive variables relative to the new book. In order to do that,
training data includes historical sales data for all books published
in Spain for several years, including those descriptive variables. 

%%% Principio de párrafos a eliminar o mover a estado del arte.
%% Fin de párrafos a eliminar.
  % [pedro] eliminados

The initial dataset, provided in the framework of a contract by
Trevenque Editorial, included 3159 books, and included descriptive information 
for each book, such as price, points of sale, subject, publisher, sales, returns, 
among other information.
Table \ref{tabla:paramsOrig50} shows the original set of parameters describing 
each book.

\begin{table}[!ht]
\caption{Parameters describing a book, as provided by the company Trevenque.}
\label{tabla:paramsOrig50}
\begin{center}
\begin{tabular}{|c|c|}
\hline 

\begin{minipage}{2.45in} \begin{verbatim}
reference
author
retail pricer
subject1
subject2
subject3
editorial
collection
bookbinding
print-run
total sales
dept stores sales
sales through delegates
rest of sales
total sales 1st year
mall sales 1st year
delegates sales 1st year
rest of sales 1st year
total distributed
distributed through dept stores
distributed through delegates
distributed - rest
total distributed 1st year
distrib. through dept stores 1st year
distrib. through delegates 1st year
\end{verbatim} \end{minipage}     & 

\begin{minipage}{2.3in} \begin{verbatim}
rest of distributed 1st year
reprints
number of reprints
total returns
returns - dept stores
delegates returns
rest of returns
total returns firt year
returns -  dept stores 1st year
delegates returns 1st year
rest of returns 1st year
gifts
units distributed as novelty
total points of sale
points of sale - dept stores
delegates points of sale
rest of points of sale
total points of sale 1st year
dept stores points of sale 1st year
delegates points of sale 1st year
rest of points of sale 1st year
weeks for sale
positive sales environment
medium sales environment
negative sales environment
\end{verbatim} \end{minipage}    \\

\hline
\end{tabular}
\end{center}
\end{table}

In this prediction problem there are other external variables that also affect 
the sales of a new book, such as those related to seasonality, trends or 
promotions, that classical prediction methods can not deal with.
However, an expert can consider them to modify the forecast obtained by 
the automatic method. Even publishing companies tend to define rules or 
indexes to subjectively adjust the results of prediction based on
external data. %so what? 


%********************************************************************************
\section{Methodology}
\label{sec:methodology}

There are several issues inherent to the problem of predicting sales of new 
products: the limitation on the amount of data available, the application of 
different methods and choose the most suitable of them.
In addition, there is no a standard methodology to solve the general problem 
of sales forecasting. %FERGU: hacer más hincapié de esto en la intro entonces
Usually, a preliminary analysis of the data is carried out to eliminate those 
variables that describe the product but do not provide useful information during 
the prediction process.

\hspace{2cm}
\begin{verbatim}
  Stage 1: Collecting and analyze data
		- Variables selection
 		- Data preprocesing
  Stage 2: Select the best forecasting method
 		- Obtaining and evaluating forecasting models
 		- Calculate forecasting values
  Stage 3: Subjectively adjust obtained results
	 	- Apply additional expert knowledge, such as information 
 	 	  about seasonality, trends, etc.
\end{verbatim}

% Maribel: quito la figura por el issue asignado por antonio mora.
%\begin{figure}[!ht] 
%\begin{center}
%  \epsfig{file=stages.eps,width=7cm}
%\caption{\small{Three main stages in a typical sales prediction process: data analysis, (2) forecasting 
%methods testing, and (3) subjectively adjusting the obtained results.}}
%\label{fig:stages}
%\end{center}
%\end{figure}

Thus, in this study two main stages are proposed: 
\begin{itemize}

  \item  First, after the editorial company has provided the raw data, taken 
  directly from its database, a feature selection is carried out, as it is 
  an effective dimensionality reduction technique and an essential preprocessing 
  method to remove noise features \cite{Krishnapuram2004}. The idea of these 
  algorithms is searching the space of attribute subsets to find out which one 
  in the data works best for classification or prediction.
  In this stage, the information obtained using several feature selection 
  methods as well as analyzing the SOM cluster method results is used to 
  remove some unrelated variables and selects the best variables to be 
  considered in the forecasted model. % we will use pre-sales data.
  This task may help to identify leading indicators to be used by the 
  forecasting methods in order to improve the accuracy of future forecasts.

  \item  Then, several forecasting methods have to be tested. In this stage 
  different forecasting methods are used to obtain sales forecasts from the 
  data provided.

\end{itemize}

%In some cases, depending on the company needs and after obtaining the forecasting  results, it is necessary a third stage where experienced managers adjust subject1ively the results obtained using the forecasting methods (see Figure \ref{fig:stages} for details). Esta frase no aporta nada que no esté dicho ya antes en los comentarios de las tres etapas que antes estaban en una figura, así que quito la frase entera: Maribel

They might manually adjust the forecasting results taking into account their 
knowledge in industry experience and their perception about trends.

%Finally, although different authors have proposed metrics to determine 
%the accuracy of estimates, most companies use metrics based on the analysis 
%of errors, i.e. MAE or RMSE \cite{ChingChin2010,Madsen2008}.

Finally, in the literature different error measures can be found to report 
the results obtained using the prediction methods \cite{ChingChin2010,Madsen2008}. 
Those error measurements are also called error functions or cost functions. 
%One of the most commonly used is the mean absolute error (see equation \ref{eq:MAE}).
In any case, we recommend using widely accepted standard 
equations \cite{gepsoft} in order to compare obtained results with those presented in other research 
papers using other methods.
That is why different standard error measurements (Equations \ref{eq:MAE} to \ref{eq:RRSE}), 
implemented in tools like Weka \cite{pentaho,Witten2011} 
or R \cite{otexts,Hyndman2013}, are
proposed.
Specifically, in order to compute the forecasting errors of the different methods, 
Mean absolute error (MAE), Root mean squared error (RMSE), Relative absolute 
error (RAE) and Root relative squared error (RRSE) have been selected:

\begin{itemize}
  \item \emph{Mean Absolute Error} (MAE):
        \begin{equation}\label{eq:MAE}
            MAE = \frac{1}{n}\sum_{i=1}^n {\mid p_i - o_i\mid}
        \end{equation}

  \item \emph{Root Mean Squared Error} (RMSE):
        \begin{equation}\label{eq:RMSE}
            RMSE = \sqrt{ \frac{1}{n}\sum_{i=1}^n {(p_i - o_i)}^2 }
        \end{equation}

  \item \emph{Relative absolute error} (RAE):
        \begin{equation}\label{eq:RAE}
            RAE = \frac{ \sum_{i=1}^n {\mid p_i - o_i\mid} }{ \sum_{i=1}^n {\mid p_{i-1} - o_i\mid} }
        \end{equation}

  \item \emph{Root relative squared error} (RRSE):
        \begin{equation}\label{eq:RRSE}
            RRSE = \sqrt{ \frac{ \sum_{i=1}^n {(p_i - o_i)}^2  }{ \sum_{i=1}^n {(p_{i-1} - o_i)}^2 }  }
        \end{equation}
\end{itemize}
where $o_i$ is the individual data $i = {1,...,n}$ and $p_i$ is the obtained prediction.


%-------------------------------------------------
\subsection{Data collection and analysis}

In general, historical data is used to make time series, and from them extracting 
common patterns and obtaining accurate predictions.
However, in the case of new products, such as the issue of new books, past patterns 
are difficult to observe as there is no available historical sales data 
\cite{ChingChin2010} (only pre-sales data is available in this problem).
% (Paloma) - Se deberían evitar paréntetis.

In addition, among the variables describing a new book, some may affect 
predictions (those variables provide information), while others may not be 
necessary and even redundant.

Moreover, the algorithm running times grow with the number of features, and thus 
making the algorithm impractical for problems with a large number of features 
\cite{Selvakuberan2008}. Feature reduction is a task that is usually applied in these kind of situations, having two ways of performance \cite{kittler1986feature}: feature extraction and feature selection. 
%Feature selection task is usually carried out by 
%searching through all possible combinations of features, evaluating each one. 

% During the evaluation, the set of all possible features is analyzed in order 
% to find the best set of features, ranking them according to some metric.
% In this process, the feature's predictive ability along with the degree 
% of redundancy between them is considered.

Feature extraction is a ``transformation-based approach", therefore it transforms the original meaning of the features. This approach involves creating a subset of new features by combinations of the existing features, thus is employed when the semantics of the original dataset will not be needed by any future process. On the contrary, feature selection attempts to retain the meaning of the original feature set. It is part of what is called semantic-preserving techniques, known as ``selection-based approaches". By searching through all possible combinations of features, the aim of these techniques is to determine a minimal feature subset to reduce processing time, while obtaining the same or higher accuracy as the initial feature set \cite{liu1998feature}. In the problem this work assesses, it is very important to maintain the semantics of the set of initial variables, and therefore several feature selection methods have been applied.

In addition, in order to reduce the dimensionality of the input data and to choose an 
appropriate set of variables, the SOM \cite{kohonen1998} method has been also applied.


SOM is a feedforward neural network \cite{Haykin98_NNC} that uses an unsupervised training algorithm and which, through a process called self-organization, configures the output units into a topological representation of the original data. It tries to imitate the self-organization done in the sensory cortex of the human brain, where neighbouring neurons are activated by similar stimulus. SOM belongs to a general class of neural network methods, which are non-linear regression techniques that can be trained to learn or find relationships between inputs and outputs or to organize data so as to disclose so far unknown patterns or structures. It is usually used as a clustering/classification tool or used to find unknown relationships between a set of variables that describe a problem. The main property of the SOM is that it makes a nonlinear projection from a high-dimensional data space (one dimension per variable) on a regular, low-dimensional (usually 2D) grid of neurons, called units.
SOM is further processed using Ultsch method \cite{UmatUlts}, the Unified distance matrix (U-matrix), which uses SOM's codevectors (vectors of variables of the problem) as data source and generates a matrix where each component is a distance measure between two adjacent neurons. It allows us to visualize any multi-variated dataset in a two-dimensional display, so we can detect topological relations among neurons and infer about the input data structure. High values in the U-matrix represent a frontier region between clusters, and low values represent a high degree of similarities among neurons on that region, clusters.

SOMs (and U-Matrix) are usually applied for visualize natural structures in the data and their relations, as well as the natural groupings that could be among them. In addition, SOM makes easy the estimation of the variables that have more influence on these groupings. Other statistical and soft computing tools can also be used for this purpose, but since Kohonen's SOMs offers a visual way of doing it, it is much more intuitive, and takes advantage of the capabilities of the human brain as a pattern recognizer.
 

This method is usually applied to analyze data as similar items tend to be 
mapped close together, while those items dissimilar, are mapped apart.
Also, the SOM graph represents and highlight very clearly those regions (clusters) 
with high training sample concentration and fewer where the samples are scarce. 
%FERGU: mencionarlo en la intro


As far as the feature selection methods are concerned, Weka separates the process in two parts:
\begin{itemize}
  \item Attribute evaluator: This is the method by which the feature subsets are assessed. In this work, both a correlation-based feature subset evaluation method (CfsSubsetEval) and an adaptation of relief for attribute estimation (ReliefFAttributeEval) evaluation method are used. 
  
  The Correlation Feature Selection (CFS) method evaluates subsets of features/variables on the basis of the hypothesis made by Hall in \cite{Hall1998}, ``A  good  feature  subset  is  one  that  contains  features  highly  correlated  with
(predictive of) the class, yet uncorrelated with (not predictive of) each other''. This evaluator needs the numeric features to be transformed to nominal features first, by being discretized. The CFS evaluation function measures the ``merit'' of the subsets, which depend on the mean feature-class correlation, and the average feature-feature intercorrelation. In addition, the correlation is measured by three variations: the Minimum Description Length (MDL), the symmetrical uncertainty, and the ``relief''.

``Relief'' is the second evaluator used in this work. It is important to note that the relief method implemented in Weka is an updated version \cite{RobnikSikonja1997}, called RRELIEFF, of the original by Kira \& Rendell \cite{Kira1992}. The algorithm works by evaluating single attributes, and not whole subsets of attributes as CFS does. Also, it is applicable even working with both nominal and numeric features, without the need for transformations. For this algorithm, the weight of an attribute depends on the difference of an attribute value inside an instance with its neighbors, being these the nearest instance of the same class, and the nearest of the different class. Therefore, a good attribute is that which gives similar values fos instances of the same class, and different values for instances of different class.
  \item Search method: This describes the structured way in which the set of possible feature subsets is studied, based on the results of the evaluator. With regard to the used search criteria, it is important to take into account that CFS evaluates attribute subsets, and Relief evaluates attributes separately.

For this reason, the search method used with CFS has been BestFirst, that searches the space of feature subsets by greedy hill-climbing augmented with a backtracking facility. On the other hand, Ranker is the searching criteria used with the Relief evaluator. This method ranks features by their individual evaluations.
\end{itemize}

Finally, Section \ref{sec:experiments} provides a comparison between the feature reduction through SOM, and the feature selection techniques that have been described.

%-------------------------------------------------
\subsection{Selecting an appropriate forecasting method}

As previously stated, there are many prediction methods available in the 
literature, each one with a different parameter set that may affect the 
obtained results.

However, as in the case of new books only some descriptive data is available %FERGU: data es plural. is->are
(compared to those cases in which historical data on sales of published books 
is available), not all forecasting methods can be used in this specific problem.

Thus, in this research five forecasting methods, based on the literature 
review in the introduction 
\cite{Madsen2008,ChingChin2010,Thomassey2012,Xia2012,SThomassey2014}, have been used. 
Well known forecasting methods implemented in the Weka tool
\cite{Hall2009,Witten2011} have been chosen, as these methods are
widely known, and could be very helpful for practitioners to reproduce
experiments and even to solve similar sales prediction problems using
these methods. % eliminado el pie de página, es mejor usar referencias - JJ

Specifically, the following forecasting methods are proposed:
\begin{itemize}

 % weka.classifiers.trees.M5P
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/M5P.html
 \item \emph{M5 Model trees}: 
A decision tree consists of answer-nodes, that indicate a class, and decision-nodes, 
that contain an attribute name and branches to other sub-decision trees.
Building a decision tree can be done using many algorithms, i.e. ID3 and C4.5 \cite{Quinlan1986}.
However, in order to use this model in regression problems, some authors have 
extended the model using methods such as the M5 model tree \cite{Quinlan1986,Quinlan1992,Wang1997,WittenFrank2000}, 
by combining a conventional decision tree and generating linear regression 
functions at the nodes.

The construction of a model tree is similar to that of classical decision 
trees \cite{Solomatine2004}:
The process breaks the input space of the training data through decision points 
(nodes) to assign a linear model suitable for that sub-area of the input space. 
This process may lead to a complex model tree.

Model tree models can learn and tackle tasks with high dimensionality (up to 
hundreds of attributes) and generate reproducible and comprehensible representations, 
what makes them potentially more successful in the eyes of decision makers.

The final model consists of the collection of linear sub-models that brings the 
required non-linearity in dealing with the regression problem
and both the predicted values at the answer-nodes along with the path from the 
root to that node is given as a result.

 % weka.classifiers.functions.LinearRegression
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/LinearRegression.html
 \item \emph{Linear Regression}:
 Linear regression \cite{Cohen2003,Yan2009,Rencher2102} method models the 
relationship between a scalar dependent variable (outcome) and several independent 
variables (predictor) over the range of values in the dataset. 
In this statistical technique, data is modeled using linear functions to estimate 
unknown model parameters, examining the linear correlations between independent 
and dependent variables.

This method of regression provides an adequate and interpretable description of 
how the input affects the output as it models the dependent variable as a linear 
function of the independent variables. Moreover, the linear relation can be 
solved using the least squares method, that minimizes the error between the 
actual data and the regression line \cite{McClendon2015}.

 % weka.classifiers.functions.SMOreg
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMOreg.html
 \item \emph{Support Vector Machine for Regression (SVM)}:
Support vector machine (SVM) \cite{Cortes1995,Shevade1999} method is based on  
statistical learning theory and has successfully used in classification and  
regression problems \cite{Cao2003,Jari2008}.

In classification problems the algorithm searches for an optimal hyperplane 
that separates two classes, maximizing the margin between two classes. 
In the case of regression the algorithm chooses a hyperplane close to as many 
of the data points as possible, minimizing the sum of the distances from 
the data points to the hyperplane. 
In both cases, the hyperplane is defined by a subset of training set samples 
(called support vectors).
%This method works well even if the space is highly dimensional and the problem is not linearly separable.

 % weka.classifiers.lazy.IBk 
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/lazy/IBk.html
 \item \emph{k-Nearest Neighbours (kNN)}: 
The kNN \cite{Aha1991,Mitchell1997} method is an instance-based classifier in 
which the unknown instances are classified by relating them to the known 
instances using a distance measurement/function (Euclidean, Minkowsky or minimax).

The main idea behind the algorithm is that two instances far enough in the space, 
taking into account the distance function, are less likely to belong to the 
same class than two closely situated instances.

The classification algorithm locates the nearest neighbour in instance space and 
assigns the class of that neighbour to the unknown instance.
In order to improve the robustness of the model, several neighbours can be 
located, assigning the resulting class to an unclassified vector using the 
closest k vectors found in the training set by majority vote.

 % weka.classifiers.functions.MultilayerPerceptron
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/MultilayerPerceptron.html
 \item \emph{Multilayer Perceptron}:
A Multilayer Perceptron (MLP) \cite{Rosenblatt1962,Widrow1990} is a feedforward 
artificial neural network model that maps the input data onto an appropriate output. 
It is an artificial neural network generally used for classification or 
approximation problems.

This model is a generalization of the standard linear perceptron that uses several 
layers of nodes (called neurons) and that is able to solve linearly inseparable 
problems \cite{SteinwenderBitzer2003}.
Each neuron consists of a linear combination of weighted inputs which is passed 
though a non-linear activation function to produce its output.

This kind of artificial neural network is trained using a supervised learning 
technique called back-propagation.
This training method was developed independently by Werbos \cite{Werbos1974}, 
Parker \cite{Parker1985} and Rumelhart et al. \cite{Rumelhart1985}, and consists
in updating the weights of the output layer neurons once the erroneous output 
has been obtained, and then, propagating the successive weight layers back to 
the input layer.

A key issue when designing an MLP is the number of hidden layers of neurons.
Lippmann proved in \cite{Lippmann1987} that two hidden layers are enough to 
create classifying regions of any kind. This result was then confirmed in later 
works by Bishop \cite{Bishop1996} and Reed et al. \cite{Reed1999}.

\end{itemize}

The objective is finding the most appropriate forecasting method that minimizes 
the error between the obtained forecast and the actual sales for each new book 
published.


%********************************************************************************
\section{Experiments and Results}
\label{sec:experiments}

\subsubsection{Experimental setup}

    \textbf{ TODO: details about the method parameters }


\subsubsection{Obtained results}

Once the methods to be applied have been chosen, they must be applied to datasets 
to obtain the prediction models for each one. 
Later on, the models ability must be validated on non-previously seen data.
Specifically, instead of using the conventional validation strategy based on 
using a training set and later on a testing set (unknown data against which 
the model must be tested), a cross-validation technique \cite{Geisser1993,Kohavi1995,Devijver1982} 
has been used to estimate how accurately the predictive model will perform in 
practice. %, limiting at the same time the overfitting problem.

%
% TODO: explicar los resultados obtenidos en la selección de variables: 
%  (1) con el SOM, (2) con los métodos de selecc. vars.
%
% CONTAR COMO SE HAN PLANIFICADO LAS EJECUCIONES CON LOS DIFERENTES METODOS:
% - se ha usado la implementación de estos 5 métodos del paquete WEKA
% - se han realizado 30 ejecuciones con cada método
% - se ha usado una máquina Linux, Intel(R) Core(TM) i5-4430 CPU at 3.00GHz (4 cores) and 16 GB RAM
To do so, in a first stage, the feature selection process has been carried out 
using both the CfsSubsetEval \cite{Hall1998} (a correlation-based feature subset 
evaluation method) and the ReliefFAttributeEval \cite{RobnikSikonja1997} (an 
adaptation of relief for attribute estimation) evaluation methods, implemented 
in the Weka tool.

% con ReliefFAttributeEval salen seleccionadas (9):
% print_run num_reprints tot_points_sale tot_points_sale_1st_year reprints distrib_novelty weeks_sale + total_sales
Using the ReliefFAttributeEval feature selection method, and taking into account 
the best ranked atributes, the following variables have been selected: 
\emph{print-run, number of reprints, total points of sale, total points of sale 
1st year, reprints, units distributed as novelty, weeks for sale, total sales}.

% con CfsSubsetEval salen seleccionadas (4):
% num_reprints tot_points_sale print_run + total_sales
Using the CfsSubsetEval feature selection method, the following variables have 
been selected: \emph{number of reprints, total points of sale, print-run, total sales}.
%FERGU: podeis quitar estos parrafos de variables porque ya está la tabla :)

%*************************************** SOM ANALISYS
% como tercer método, se ha usado el SOM; tras interpretar los mapas, se
% seleccionaron las variables más adecuadas.
Then, an exhaustive analisys of the SOM results was carried out in order to 
determine the best variables to be considered to buid the forecasting model.

Further important prepocessing steps are carried out before applying the SOM analysis.
First, in order to label the U-matrix with total-sales values, this variable is converted from numerical to categorical. Five categories have been created to represent the quantities of sales of each book: Very Low (VL), Low (L), Medium (M), High (H) and Very High (VH). Table \ref{table:freq} shows the ranges of total-sales that correspond to each category along with the number of books that fall in. Table \ref{table:freq} and Figure \ref{fig:count} reveal more information about the nature of the dataset. We can notice that most of 77\% of the book are categorized as VL and L with limited number of sales. On the other side, the books that achieved more than 2000 sold copies are just around 4\%. This makes the problem of identifying those two categories much harder. The new categorical feature is used to replace the old numerical Total Sales feature in the SOM analysis.

\begin{table*}[ht]
\caption{Categories corresponding to total-sales variable. }
\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
Category & Range of total sales & Count & Ratio\tabularnewline
\hline 
\hline 
Very Low & 1-100 & 2233 & 36.71\%\tabularnewline
\hline 
Low & 100-500 & 2323 & 38.19\%\tabularnewline
\hline 
Medium & 500-2000 & 1239 & 20.37\%\tabularnewline
\hline 
High & 2000-5000 & 224 & 3.68\%\tabularnewline
\hline 
Very High & $>$5000 & 64 & 1.05\%\tabularnewline
\hline 
\end{tabular}
\label{table:freq}
\end{table*}


\begin{figure*}[ht]
\begin{center}
\includegraphics[scale=0.60]{freq-sales}
\end{center}
\caption{Number of books in each category.}
\label{fig:count}
\end{figure*}

Second, two categorical features (i.e; Editorial and Collection) are removed from the analysis since they have large number of unique different values and therefore they are less informative. Moreover, four features (i.e; Subject1, Subject2, Subject3 and Book-bining) have more than 10\% missing values so they are removed also.

Finally, the resulted dataset consisted of ten features including the new Total Sales categorical feature. The SOM algorithm is trained based on the nine numerical variables while the new total-sales variable is used to label this data. The SOM algorithm is applied with a large map size of $26 \times 16$  in order to represent the data. The best SOM training results are obtained with a final quantization error of 0.691 and topographic error of 0.046.

Figure \ref{fig:componentplanes} shows the U-matrix and component variables resulted from SOM training. The term component is used in SOM to refer to the features. The component planes in Figure \ref{fig:componentplanes} show the trend of values of the the prototype vectors of the SOM map units. These values are represented as colors. The colorbar on the right of each component plane shows the indication of each color. On the other side, the U-matrix on the top left corner shows the distances between the adjacent units. It is important to note that high values in the matrix indicates large distance between the units of the map. It can be noticed also that there are more hexagons in the U-matrix than component planes since the distances between units is also represented as hexagons.

In Figure \ref{fig:Umatrix} we label the U-matrix with the Total Sales categories. It can be clearly shown that the SOM algorithm has successfully clustered the labels of the Total Sales into clear identified regions. In the top of the U-matrix the VL category is located. The L category appears in the middle of the matrix. Most of the M category appears in bottom-right corner while the H and VH categories are concentrated in the bottom-left corner. 
Inspecting the component \texttt{distrib\_novelity}, \texttt{tot\_points\_sale}, \texttt{tot\_points\_sale\_1st\_year} and \texttt{print\_run} components have the highest similarity to the clusters shown in the U-matrix. Based on this analysis, those features will be selected for the next stage. 

\begin{figure*}[ht]
\begin{center}
\includegraphics[scale=0.80]{fig1}
\end{center}
\caption{U-matrix and component planes.}
\label{fig:componentplanes}
\end{figure*}

\begin{figure*}[http]
\centering
    \begin{subfigure}[b]{0.32\textwidth}            
            \includegraphics[scale=0.50]{Umatrix.eps}
            \caption{U-matrix}
            \label{fig:Um}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}  
            \includegraphics[scale=0.50]{Labels.eps}
            \caption{Labels}
            \label{L}
    \end{subfigure}
 
    \caption{Labeling the U-matrix using the Total Sales variable.}
\label{fig:Umatrix}
\end{figure*}

%*************************************** SOM ANALISYS


\begin{table}
\begin{tabular}[htb]{|c|c|c|c|c|}
\hline
Variable  & PreSales & ReliefFAttributeEval  & SOM & CfsSubsetEval \\ \hline \hline
subject1    & $\checkmark$  &   &   &   \\ \hline
subject2    & $\checkmark$  &   &   &   \\ \hline
subject3    & $\checkmark$  &   &   &   \\ \hline
editorial   & $\checkmark$  &   &   &   \\ \hline
collection  & $\checkmark$  &   &   &   \\ \hline
binding   & $\checkmark$  &   &   &   \\ \hline
ret\_pricer    & $\checkmark$  &   & $\checkmark$  &   \\ \hline
reprints    & $\checkmark$  & $\checkmark$  & $\checkmark$  &   \\ \hline
num\_reprints    & $\checkmark$  & $\checkmark$  & $\checkmark$  & $\checkmark$  \\ \hline
gifts   & $\checkmark$  &   & $\checkmark$  &   \\ \hline
distrib\_novelty   & $\checkmark$  & $\checkmark$  & $\checkmark$  &   \\ \hline
tot\_points\_sale   & $\checkmark$  & $\checkmark$  &   & $\checkmark$  \\ \hline
tot\_points\_sale\_1st\_year    & $\checkmark$  & $\checkmark$  &   &   \\ \hline
weeks\_sale    & $\checkmark$  & $\checkmark$  &   &   \\ \hline
print\_run   & $\checkmark$  & $\checkmark$  & $\checkmark$  & $\checkmark$  \\ \hline
total\_sales   & $\checkmark$  & $\checkmark$  & $\checkmark$  & $\checkmark$  \\ \hline

\end{tabular}
\caption{Variables used in each method.} %PABLO: Poned este caption bien
\label{tab:varmethods}
\end{table}

Table \ref{tab:varmethods} shows the used variables used in each step. %Pablo: poner también esta frase mejor 
Once the different reduced datasets have been obtained, proposed methods 
(M5P, LR, SVM, kNN, MLP) have been selected for this study from 
the Weka implementation, and they were run 30 times with the Weka default parameter 
settings and a different random initialization seed for each run and each dataset.

Experiments have been conducted on an Intel(R) Core(TM) i5-4430 CPU at 3.00GHz 
(4 cores) and 16 GB RAM, running Ubuntu Linux 14.04.1 LTS and Java JRE\_1.7.0\_72.

% datasets:
%  original (16 vars)
%  4 vars
%  9 vars
%  SOM selected vars

Table \ref{tabla:resultsT} shows obtained results (time to build the model and 
error) after applying the proposed forecasting methods on the original dataset,
using the 16 available variables per book.
In this dataset, every book is described using not only numerical variables, 
but using two additional categorical variables, \emph{editorial} and \emph{collection}, 
what makes harder to process the instances and build the different models.

% Mean absolute error MAE / Root mean squared error RMSE /  Relative absolute error RAE / Root relative squared error RRSE
  % ¿Qué sentido tiene usar los 4 métodos de error? ¿Es que vamos a seleccionar uno? - JJ
  %[pedro] es lo habitual en los trabajos en los que se realizan tareas de predicción
  % en algunos calculan muchos más, pero estos son los más usados. 
\begin{table*}[h]
\caption{Results (errors and time) obtained using the different forecasting
methods on the original dataset (16 variables per book). In this case,
due to the categorial variables, the MLP could not be run.}
\label{tabla:resultsT}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
Base & Correlation coefficient (\%) & MAE & RMSE & RAE(\%) & RRSE (\%) & Time(sec)\tabularnewline
\hline 
\hline 
M5P & 0.9076 & \textbf{202.1547} & 591.0257 & \textbf{35.31}\% & 42.10\% & 39.73\tabularnewline
\hline 
LR & 0.9204 & 255.0526 & 548.0975 & 44.5527 & 39.10\% & 0.03\tabularnewline
\hline 
SVM & \textbf{0.9402} & 203.4416 & \textbf{481.7528} & 35.54\% & \textbf{34.37}\% & 308.31\tabularnewline
\hline 
kNN & 0.7273 & 310.4078 & 993.2663 & 54.22\% & 70.85\% & 0\tabularnewline
\hline 
MLP & 0.9197 & 289.7393 & 585.6445 & 50.61\% & 41.78\% & 82.08\tabularnewline
\hline 
\end{tabular}
\end{table*}


Obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 7 variables per book (obtained 
using the SOM analysis) can be shown in Table \ref{tabla:resultsSOM}.
\begin{table*}[h]
\caption{Results (errors and time) obtained using the different forecasting methods
on the dataset obtained using the SOM feature selection method (6 variables
per book).}
\label{tabla:resultsSOM}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
Base  & Correlation coefficient (\%)  & MAE  & RMSE  & RAE(\%)  & RRSE (\%)  & Time(sec)\tabularnewline
\hline 
\hline 
M5P  & 0.9161 & \textbf{198.8864} & 562.2113 & \textbf{34.74}\% & 40.11\% & 0.76\tabularnewline
\hline 
LR  & 0.9179 & 263.1702 & 556.3372 & 45.97\% & 39.69\% & 0.07\tabularnewline
\hline 
SVM  & \textbf{0.9388} & 207.7159 & \textbf{486.8913} & 36.28\% & \textbf{34.73}\% & 157.87\tabularnewline
\hline 
kNN  & 0.9304 & 202.0939 & 525.4642 & 35.30\% & 37.48\% & 0\tabularnewline
\hline 
MLP  & 0.9193 & 305.78 & 590.8412 & 53.41\% & 42.15\% & 25.83\tabularnewline
\hline 
\end{tabular}
\end{table*}


Obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 8 variables per book (obtained 
using the ReliefFAttributeEval feature selection method) can be shown 
in Table \ref{tabla:results9}.
In this dataset, every book is described using numerical variables, and thus, 
processing the data to build the different models is a fast task.

\begin{table*}[h]
\caption{Results (errors and time) obtained using the different forecasting methods
on the dataset obtained using the ReliefFAttributeEval feature selection method (8
variables per book).}
\label{tabla:results9}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
RAE & Correlation coefficient (\%) & MAE & RMSE & RAE(\%) & RRSE (\%) & Time(sec)\tabularnewline
\hline 
\hline 
M5P & 0.9131 & \textbf{197.2886} & 580.328 & \textbf{34.46}\% & 41.40\% & 0.74\tabularnewline
\hline 
LR & 0.9209 & 254.3979 & 546.3899 & 44.44\% & 38.98\% & 0.03\tabularnewline
\hline 
SVM & \textbf{0.9394} & 204.2075 & \textbf{484.2617} & 35.67\% & \textbf{34.54}\% & 232.99\tabularnewline
\hline 
kNN & 0.9103 & 222.0827 & 597.8885 & 38.79\% & 42.65\% & 0\tabularnewline
\hline 
MLP & 0.9199 & 296.3581 & 575.9623 & 51.77\% & 41.09\% & 32.52\tabularnewline
\hline 
\end{tabular}
\end{table*}


Finally, obtained results (time to build the model and error) using the different 
forecasting methods on the dataset with 4 variables per book (obtained 
using the CfsSubsetEval feature selection method) can be shown 
in Table \ref{tabla:results4}.
As in the previous case, in this dataset every book is described using numerical 
variables. Thus, due to the number of features per book, building the different 
models is faster than in the previous cases.

\begin{table*}[h]
\caption{Results (errors and time) obtained using the different forecasting methods
on the dataset obtained using the CfsSubsetEval feature selection method (4 variables
per book).}
\label{tabla:results4}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
CFS & Correlation coefficient (\%) & MAE & RMSE & RAE(\%) & RRSE (\%) & Time(sec)\tabularnewline
\hline 
\hline 
M5P & 0.9255 & \textbf{206.6638} & 531.306 & \textbf{36.10}\% & 37.90\% & 0.56\tabularnewline
\hline 
LR & 0.9194 & 259.3791 & 551.2005 & 45.31\% & 39.32\% & 0.05\tabularnewline
\hline 
SVM & \textbf{0.9342} & 213.0254 &\textbf{ 502.4048} & 37.21\% & \textbf{35.84}\% & 70.12\tabularnewline
\hline 
kNN & 0.9285 & 220.6548 & 527.7374 & 38.54\% & 37.65\% & 0\tabularnewline
\hline 
MLP & 0.9047 & 369.3486 & 646.1241 & 64.52\% & 46.09\% & 10.93\tabularnewline
\hline 
\end{tabular}
\end{table*}


    \textbf{ TODO: comment the following figures: conv2 , conv2ReliefAttributeEval , conv2CfsSubsetEval , conv2SOM }


\begin{figure*}[http]
\centering
    \begin{subfigure}[b]{0.32\textwidth}            
            \includegraphics[scale=0.32]{m5p.eps}
            \caption{M5P}
            \label{fig:M5P}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}  
            \includegraphics[scale=0.32]{lr.eps}
            \caption{LR}
            \label{fig:LR}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{svm.eps}
            \caption{SVM}
            \label{SVM}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{knn.eps}
            \caption{k-NN}
            \label{fig:k-NN}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{mlp.eps}
            \caption{MLP}
            \label{MLP}
    \end{subfigure}
    \caption{Actual VS predicted sales for M5P, LR, SVM, k-NN, MLP models.}
\label{conv2}
\end{figure*}


\begin{figure*}[http]
\centering
    \begin{subfigure}[b]{0.32\textwidth}            
            \includegraphics[scale=0.32]{m5p_RAE.eps}
            \caption{M5P}
            \label{fig:M5P1}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}  
            \includegraphics[scale=0.32]{lr_RAE.eps}
            \caption{LR}
            \label{fig:LR1}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{svm_RAE.eps}
            \caption{SVM}
            \label{SVM1}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{knn_RAE.eps}
            \caption{k-NN}
            \label{fig:k-NN1}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{mlp_RAE.eps}
            \caption{MLP}
            \label{MLP1}
    \end{subfigure}
    \caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using ReliefAttributeEval.}
\label{conv2ReliefAttributeEval}
\end{figure*}


\begin{figure*}[http]
\centering
    \begin{subfigure}[b]{0.32\textwidth}            
            \includegraphics[scale=0.32]{m5p_CFS.eps}
            \caption{M5P}
            \label{fig:M5P2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}  
            \includegraphics[scale=0.32]{lr_CFS.eps}
            \caption{LR}
            \label{fig:LR2}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{svm_CFS.eps}
            \caption{SVM}
            \label{SVM2}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{knn_CFS.eps}
            \caption{k-NN}
            \label{fig:k-NN2}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{mlp_CFS.eps}
            \caption{MLP}
            \label{MLP2}
    \end{subfigure}
    \caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using CfsSubsetEval.}
\label{conv2CfsSubsetEval}
\end{figure*}


\begin{figure*}[http]
\centering
    \begin{subfigure}[b]{0.32\textwidth}            
            \includegraphics[scale=0.32]{m5p_SOM.eps}
            \caption{M5P}
            \label{fig:M5P2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.32\textwidth}  
            \includegraphics[scale=0.32]{lr_SOM.eps}
            \caption{LR}
            \label{fig:LR2}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{svm_SOM.eps}
            \caption{SVM}
            \label{SVM2}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{knn_SOM.eps}
            \caption{k-NN}
            \label{fig:k-NN2}
    \end{subfigure}
      \begin{subfigure}[b]{0.32\textwidth}
            \includegraphics[scale=0.32]{mlp_SOM.eps}
            \caption{MLP}
            \label{MLP2}
    \end{subfigure}
    \caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using SOM analysis.}
\label{conv2SOM}
\end{figure*}


%\begin{figure}[!ht] 
%\begin{center}
%  \epsfig{file=fig_time.eps,width=11cm}
%\caption{\small{Running time in seconds, obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). Plot shows that using categorical variables as part of the dataset makes the problem hard to solve. As expected, simpler methods are much faster to build the model than the complex ones.}}
%\label{fig:time}
%\end{center}
%\end{figure}

%\begin{figure}[!ht] 
%\begin{center}
%  \epsfig{file=fig_rae.eps,width=11cm}
%\caption{\small{Relative absolute error (RAE $\%$) obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). The M5P model obtains the lowest errors, whatever the dataset used.}}
%\label{fig:rae}
%\end{center}
%\end{figure}


% - indicar que el uso de las dos variables categóricas hace más difícil de aprender el conjunto
Paying attention to the obtained results shown in tables \ref{tabla:resultsT}, 
\ref{tabla:resultsSOM}, \ref{tabla:results9} and \ref{tabla:results4},
using those categorical variables as part of a big dataset makes the problem 
hard to solve, both in required time to build the forecasting models as well as 
in the obtained errors.
Moreover, some methods are unable to build the models in a reasonable time.
%However, as stated before, Trevenque expert sales manager, taking into account 
%his knowledge in industry experience and their perceptions, decided that 
%\emph{EDITORIAL} and \emph{SUBJECT} should be taken into account.


% - indicar pros y contras de los diferentes métodos
% - indicar qué método da mejores resultados
% - indicar qué método es tipo "caja-negra" y cuáles ofrecen modelos "explicables" 

As far as the different methods is concerned, M5P model obtained good results, 
both in time and error, and as stated before, this method can learn and tackle 
tasks with high dimensionality and it generates reproducible and comprehensible 
representations that provide information on the decision process and does not 
require human intervention either for the operation or for interpretation. 
Thus, it is very suitable in order to be incorporated in a publishing company 
processes.

The Linear Regression model outperforms some other complex methods, both in time 
(as it is a simple method to train) and in obtained errors.
However not in all cases, as more powerful models that obtain better results 
can be obtained in comparable time.

Taking into account the SVM model obtained results, this method obtains comparable 
errors to those of M5P. Even when used using the default parameter values it exhibit 
a good generalization performance.
However, it is much slower than the others to build the model, not only when 
using the dataset with categorical input variables, but also when using the 
datasets with numerical variables.

kNN is the faster method used in this study, as it is the simpler method.
However, the model is slow when classifying (as there are a large number of 
training examples), and it exhibit a poor generalization ability, as it does not 
learn anything from the training data.

As far as the Multilayer Perceptron, its main drawbacks are both the cost of model 
building (not only parameter and weights setting, but also time to train the 
network) and the fact that it is a back-box model, and thus, it is difficult 
to explain how the forecasts are obtained.
Moreover, in these experiments it has been seen the difficulty to process 
information related to categorial variables, such as \emph{editorial} and 
\emph{collection}.

%[Pedro] ¿aplicamos tests estadísticos para ver si hay diferencias entre métodos?

Finally, paying attention to the cost in time to obtain the forecasting models 
and their accuracy, the M5P model is the most adequate to solve the problem of 
how many books should be printed when a new book is published.
This forecasting method, as an additional advantage, can be used for products 
with historical sales data and for new products with little or no historical 
data available.


%********************************************************************************
\section{Conclusions and Future Work}
\label{sec:conclusionsAndFutureWork}

In this paper, the problem of how many books should be printed 
when a new book is published is faced using several data-mining and
forecasting methods.

This is a very challenging problem in this industry, because printing a higher 
number of volumnes than those finally sold will lead to losses, 
while printing an adequate number of copies will optimize sales and company earnings.
In addition, there are several difficulties inherent to the new book sales forecasting, 
such as dealing with limited data, finding adequate forecasting methods, and 
selecting the best method to use.

%La investigación descrita en este trabajo se ha llevado a cabo conjuntamente con una empresa editorial que ha facilitado un conjunto de datos con los que trabajar, así como con la ayuda de un experto en ventas de este campo para validar los resultados obtenidos.
A dataset consisting of 3159 books provided by the Spanish %FERGU: mover este párrafo debajo del siguiente
publishing company Trevenque Editorial has been used.

In this paper, a data visualization method has been used to find out what are 
the relevant variables describing a book in the prediction of book sales.
Then, several standard data mining models for sales forecasting have been used 
to forecast new book sales, and from them deduce print-run, based on those 
variables.

The most relevant variables when predicting sales have been determined, 
while unrelated or superfluous ones have been removed, 
and sales predictions have been obtained using five different forecasting 
standard methods.
Not only the obtained forecasting error has been taking into account to evaluate
those methods, but also the the cost of model building as parameter setting and 
the required time to train the model.

In this sense, the decision trees proved to be a suitable model because of the 
ease to explain how the prediction is obtained from the variables that describe 
the new book. 
This, compared to the black-box models such as the artificial neural network, 
was decisive from the point of view of the needs of experts and Trevenque company.
Moreover, proposed method was validated by the company and incorporated in a 
business intelligence tool for publishing houses. %FERGU: no hay más info de esto? estaría guay describirlo

%FERGU: no mencionais el SOM en las conclusiones

Overall, this study contributes to the literature by proposing the use of different 
soft-computing standard techniques to solve a new challenging problem.
Moreover, proposed method could be implemented, not only in the editorial 
industry, but also in other domains where the specificity of products is similar, 
and might be of interest to other academic researches and industrial practitioners.


%---------- trabajos futuros

As future work, it would be interesting to compare the obtained forecasting results 
with other methods based on soft-computing, such as genetic programming or fuzzy logic.
Moreover, as default parameters have been used for proposed forecasting methods,
it would be of interest carrying out some parameter tuning procedure in order to
improve obtained results.

Furthermore, since there are some characteristics that may affect product sales 
significantly, another way to improve sales predictions could be making a thorough 
study of the effect of the application of discounts and promotions. 

Finally, there are some variables over which the published has some
control once the book has been published, such as the initial print
run or the books given out as gifts to reviewers or what proportion is
going to be sent to each possible point of sale. This can be
incorporated into an optimization model that optimizes those values to
obtain desired sales. This is left as future work.


%********************************************************************************
\section*{Acknowledgements}

This work has been supported in part by projects 
PreTEL (PRM Consultores - Trevenque),
TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness), 
SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico), 
PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigaci{\'o}n), 
PROY-PP2015-06 (Plan Propio 2015 UGR), 
and project CEI2015-MP-V17 of the Microprojects program 2015 from CEI BioTIC Granada.

%********************************************************************************
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
