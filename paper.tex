\documentclass{llncs}
\usepackage{graphics}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[dvips]{epsfig}
\usepackage[latin1]{inputenc}
\usepackage{url}

\def\CC{{C\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}~}
\addtolength{\textfloatsep}{-0.5cm}
\addtolength{\intextsep}{-0.5cm}


%%%%%%%%%%%%%%%% Titulo %%%%%%%%%%%%%%%
\title{Enhancing the sales prediction of newly published books by means of computational intelligence methods} 


%%%%%%%%%%%%%%%% autores %%%%%%%%%%%%%%
\author {
P.A. Castillo$^1$, A.M. Mora$^1$, H. Faris$^2$, J.J. Merelo$^1$, \\ P. Garc\'{\i}a-S\'anchez$^1$, A.J. Fern\'andez-Ares$^1$, M.G. Arenas$^1$, P. de las Cuevas$^1$
}
\institute{
$^1$Department of Computer Architecture and Computer Technology. CITIC, ETSIIT. University of Granada (Spain)\\
$^2$Business Information Technology Department. King Abdullah II School for Information Technology. The University of Jordan. Amman (Jordan)\\
           e-mail: {\tt pacv@ugr.es}
}

\date{}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
% ***TO DO***: rewrite this considering the new contents and results\\

When a new book is launched the publisher faces the problem of
how many books should be printed for delivery to bookstores. Printing too many is the main issue,
since it implies a loss of investment due to inventory excess. In this
paper, we are tackling the problem of predicting total sales in order to 
print the right amount of books, doing so even before the book has
reached the shelves. 
Several years 
% % Antonio - Decir cuántos años son, que no me acuerdo.
% of data on published books in Spain have been processed by means of
% data visualisation %visualization does not "process". I propose an alternative
%  and other feature selection techniques. 
We have been working on a data set that includes the complete sales
data for books published in Spain across several years. After an
initial exploratory phase using visualisation techniques, we have
applied several feature selection techniques in order
to find out what are the variables that have more impact on sales, and
then used machine learning to create a model and forecast sales. 
The described methodology improves the forecasting running time  %with
                                %respect to what?- JJ
and is
eventually able to predict sales with remarkable accuracy. Moreover,
the proposed method was validated by the company that provided
the data and incorporated in a business intelligence tool for
publishers, % We shouldn't say this, it's outside the bounds of the
            % paper. Besides, it was a different technique - JJ
since the obtained models provide a reliable guidance on the decision process of
publishing a book, without human intervention 
either for the operation or for interpretation.
% Antonio - esto es cierto??? :D
% We should simply say: 
% The method presented here provides a reliable data processing
% technique for editorial data and also yields a reasonable result
% that allows publishers to forecast sales and then create the
% appropriate print run without incurring in losses. 
%
% Besides, if this is true:
% It also provides a method that is able to provide insight on the
% decisions that have an influence on the print run for publishers. 
\end{abstract}


%********************************************************************************

\section{Introduction}

%***TO DO***: complete this considering the new contents and results\\

Publishing a book, as releasing any other cultural product with a
physical substrate, implies several
types of risks and costs due to
the complexity, and derived expenses, of the production and
distribution processes.  When a company is faced with the problem or
releasing a completely new product, in the sense that it is an unknown
author or a new topic with no track record, sales forecasts can
only be based on the experience obtained from other similar products,
but the definition of ``similar'' itself is obscure and, sometimes,
subjective. Even if it is a known author or topic, having accurate
estimations of future sales of the new 
product is extremely important \cite{ChingChin2010}, so that production
runs do not exceed predicted sales by too much. 
Indeed, several studies have demonstrated that improving sales predictions 
(a reduction of forecasting errors) results in an enhancement in the 
%FERGU: an enhancing está bien dicho? Mejor "an enhacement", no?
production process \cite{Bayraktar2008,Fildes2010,Saeed2008,Zhao2002}.
%which should be obvious from the get go, don't know why you need 4
%papers to affirm that - JJ

% In this sense, the publishing industry is based on very particular and
% established production process, and publishing a book requires several
% steps, between which the distribution is important and plays an
% important role. The publishing process and individual decisions at each step are very
% important. One of those decisions is the number of copies to print. 
  % ¿Esto que tiene que ver con la frase anterior? - JJ
% Thus, a suitable prediction system for new books should take into
% account many details about how the editorial process works. % y? - JJ

% Implementing a good forecasting tool requires not only a good study of
% the state of the art in this field, but also an accurate knowledge of
% the operations and challenges facing the publishing industry and its
% supply chain. % ¿eso es lo que vamos a hacer? - JJ

The problem is specially acute for book publishers, who, among other
content providers, release new {\em products} more frequently than
other industries, so that they face the 
problem of predicting sales quite often. The issue, in this case, is to print an
adequate number of copies but not too many, as the unsold volumes will
lead to losses and sunk inventory cost, 
% Antonio - 'sunk inventory cost' es corecto?
% Sí, pero corecto no lo es. - JJ
whereas if the number of printed copies was not enough, a new print
run can be made, resulting in temporary losses due to lack of supply;
a real problem, to be sure, but less dire than having too many unsold
books.  
%FERGU: el you ese es muy coloquial, reescribir
% Antonio - Esto habría que matizarlo más, de hecho, decirlo va un poco en contra de nuestro artículo, ya que simplifica el problema. Yo indicaría que errores en predicciones por exceso o por defecto tienen MUCHAS pegas (ambos casos), si bien el exceso es peor, claro, pero decir que lo otro es también muy negativo y por eso hay que ajustar muy bien la predicción. ;)
% Se podría señalar, por ejemplo:
% - repetición de costes de distribución
% - mala impresión al cliente/lector, que pueden repercutir negativamente en las ventas
% - algunas más... :D
% Corregido - JJ
In this sense, errors in one or the other direction do not have the same impact.
% Antonio - esto habría que rescribirlo de acuerdo a lo anterior que comento. ;)
%FERGU: lo de one or the other direction creo que confunde
% Lo que habría que decir es hablar de cómo hemos tratado una u otra
% cosa. ¿Estamos sesgando hacia predecir menos ventas de las que
% habría? ¿O simplemente es por tener un poco de background? Igual
% deberíamos haberlas tratado de forma diferente. - JJ

Thus, it is extremely important to develop an accurate predictive method
which could accurately forecast the future sales a new book will
achieve, in order to optimise production, improve the publishing
company profits or minimise losses \cite{Zhao2001}. This is the main
objective of this paper. 
%FERGU: entonces el objetivo es desarrollar una herramienta? La metodología? Dejarlo más claro en todos los sitios
An example is the optimisation of sales \cite{Mattila2002} by
improving profit margins by selling the whole print-run. 
% Antonio - podrías extender un poco el ejemplo? Es decir, describir
% un poco más lo que se hace en ese trabajo. ;)
% An example of what? Does that go there? - JJ

However, designing such an accurate predictive tool is a difficult
task in this scope, as book sales are influenced by several factors
that make them fluctuate and, moreover, usually they might be not
controllable and even unknown. 
%FERGU: "moreover, usually" son dos adverbios juntos y queda un poco raro 
For instance, some are related to decisions on purchases, others with freight 
%FERGU: some factors para dejarlo claro
traffic, or others with the distribution strategy \cite{Little1998}. % This paragraph is rather useless - JJ

Thus, it is very important to know the product and the variables that
might affect the sales, and how an expert manager takes them into account \cite{Armstrong2001,SThomassey2014}. 
                                % ¿Esto también pasa en la industria de la
                                % moda. ¿Qué vamos a usar de esto? - JJ
                                % [Pedro] esto iba referido a la Industria Editorial.
Their impact is difficult to estimate and it is not constant over time, 
hence the difficulty to adequately identify and measure their influence \cite{DeToni2000}.
In addition, some may have correlations between them. 
%FERGU: some ... variables, no?
However, these correlations can be detected using data analysis methods, 
such as Self-Organizing Maps (SOM) \cite{kohonen1998} 
or stepwise regression analysis (SRA) \cite{Chang2009}. % What do they
                                % do? What are the results?
% This is a state of the art? Are we already in the state of the art?
% - JJ

In the research published in this paper, the problem of how many units should be printed 
when a new book is published is faced using data provided by the 
Spanish publishing company Trevenque Editorial S.L.\footnote{{\tt http://editorial.trevenque.es}},
a company that provides management systems and services for bookshops,
% Antonio - 'libraries' o 'book shops'? Supongo que lo segundo. ;D
publishers and distributors. 
Usually, the company's expert sales managers analysed historical data 
and applied their knowledge using external variables to compute
correct estimations (predictions), for instance, she can decide
increasing or decreasing the sales prediction based on season or sales
period.                         % varias citas - JJ
                        % [Pedro] lo modifico para comentar quienes nos facilitaron los datos
                        %  y que así era como funcionaban hasta ese momento.
%In any case, in general companies usually use a forecasting model and then, 
%an expert applies his knowledge, as the company might require an 
%interpretable and understandable model. 
However, carrying out the analysis and forecasting process as a human expert,
may lead to a `fuzzy-like' analysis, which may not take all the variables into account. 
%FERGU: fuzzy-like existe? suena raruno :P
In addition, the process might be tedious, especially when the amount
of data is quite large, and even, different experts can reach different
predictions from the same dataset \cite{ChingChin2010,Sanders1994}.

For these reasons, the industry requires the best suited forecasting
techniques to be adopted so that, estimating sales task is carried out
reliably, accurately, and in an automatic way. % Iterative. Another
                                % useless paragraph - JJ

With this issue in mind, in this paper a data visualisation and
preprocessing strategy to find out what are the relevant variables
describing a book in the prediction of book sales is proposed; then
several standard data mining models  
for sales forecasting are evaluated so a publishing company 
can deduce and optimise print-run.
% Antonio - cambiar este párrafo para describirlo como un proceso en dos pasos:
% - análisis y extracción
% - predicción
After this initial processing phase, we apply machine learning
techniques to create a sales model and use it to predict them. 

% Antonio - Además habría que justificar bien (con citas) el por qué de la extracción/selección de características, por ejemplo:
% - menor tiempo de procesamiento
% - mejores resultados (si algunas variables interfieren entre sí)
% - Permiten saber qué variables son las que influyen realmente en el
% resultado. Esto es lo único importante - JJ

% ***TO DO***: rewrite this considering the new structure of the paper\\

The rest of this paper is structured as follows: next, 
Section \ref{sec:soa} presents a comprehensive review of the
approaches found in the bibliography related to the prediction of
sales from product characteristics. 
Then, Section \ref{sec:problem} describes the 
problem of how many books should be printed when a new book is published, 
        % no es el problema de publicación, es el problema de venta
        % de libros sin publicar. Habrá que buscar un nombre - JJ
        %[Pedro] cierto, lo cambio, aunque me ha quedado un nombre muy largo  :)
 followed by Section \ref{sec:methodology} where the
experimental setup and the methodology considered in the study is
presented.  %FERGU: mejor described para no repetir
Section \ref{sec:experiments} reports and analyses the obtained results with a 
real dataset. Finally, conclusions and future work are presented in Section
\ref{sec:conclusionsAndFutureWork}. 
% Antonio - TODO: rescribir con los contenidos finales del artículo. ;)

%********************************************************************************
\section{State of the art on sales forecasting} 
\label{sec:soa}

% [Pedro] no se encuentra gran cosa sobre predicción de ventas de libros
%  por eso quiero comenzar hablando de la aplicación de métodos
%  de predicción en la industria en general, que sí hay bastante bibliografía.
% Antonio - me parece bien

% Antonio - Cosas generales a mejorar en el SotA:
% - habría que comparar más veces este trabajo con lo que hacen otros, destacando nuestras diferencias. Sólo se menciona al final de la sección
% - habría que destacar claramente ls aportes de este trabajo al SotA, cómo hace que avance ;)
% - habría que incluir citas a artículos de la revista que elijamos y comentarlos aquí

Sales forecasting is a problem which has been addressed several times
in the literature. Most of the studies have been conducted using
different analysis and forecasting methods, such as regression models
\cite{Papalexopoulos1990}, neural networks \cite{Yoo1999} or fuzzy
systems \cite{Mastorocostas2001}, applied to different industrial
sectors. 
% ¿Y qué? Esta frase
% no aporta nada. La primera frase
% tiene que enmarcar el estado del
% arte - JJ
% Antonio - la he intentado mejorar.
% Muchas cosas de la intro tendrían que venir aquí, al estado del
% arte. Habría que reescribir la intro. - JJ

Specifically, Time-Series prediction methods \cite{Chu2003,Brown1959,Winters1960,Box1969,Papalexopoulos1990} 
is perhaps the most used technique to tackle the sales forecasting
problems. The efficiency of these techniques strongly depends on the field of application 
and the correctness of the problem data. However, since they require a
large amount of data for predicting sales, these methods are not the
most suitable for this task \cite{ChingChin2010}. And they cannot be
applied before the book is published, as is the objective of this
paper. 

In this case, just a few variables are known in advance
\cite{ChingChin2010,FaderHardie2005,Madsen2008}, and some of 
them are categorical, harder to process, even more so if the number
of categories is high. Given these issues, finding adequate
forecasting techniques and selecting  
the best method to use is also a problem \cite{ChingChin2010}. % Qualify this citation - jj

% Thus, not any time-series forecasting method can be used
% \cite{Madsen2008,ChernWSF15}. 
% Qualify those references!!! What do they say? 

For this reason, usually the methods used to predict book sales have
been generally based on experts' experience.% Reference - JJ
 They analyse data about
sales and, taking into account their knowledge in industry, their
experience, and their perception about trends, could make decisions on
the companies production (how many books should be printed when a new
book is published). 
%FERGU: quitad los paréntesis :)
% Fergu: quítalos tú, tío, aquí puede editar todo el mundo - JJ
This is a complex problem, since the predictions are influenced by
external variables that must be taken into account, such as
seasonality, promotions or fashions that expert managers might
subjectively apply \cite{Lapide1999,ChingChin2010,ChernWSF15}. % This
                                % has something to do with the
                                % previous paragraph? Put it there!!! 
%
  %[pedro] sobre la industria editorial no hay trabajos sobre predicción de ventas.
  % Lo habitual ha sido usar el conocimiento de expertos para realizar predicciones 
  %  o aconsejar acerca de la cantidad de libros a imprimir.
  % Lo más parecido que he encontrado son dos trabajos (uno en realidad) de Moon et al. 
  %  sobre "Use Blog Information As Book Sales Prediction":
  %     Moon2010ICSSSM
  %     Moon2010ICEC
Recently, some works have proposed analysing relationships between on-line information 
and book sales \cite{Moon2010ICSSSM,Moon2010ICEC}. Specifically, Moon
et al. propose using the number of blog references as an indicator of
the success of a book, and thus, its sales. Different sales patterns
are observed by analysing data obtained. 
% Antonio - describir esos 'sales patterns'. ;)
% FERGU: y también cuales son esos 'data obtained'
However, the authors used in these papers historical data from published books, 
so their proposed methodology can not be applied to the problem of unpublished-books sales prediction.
%FERGU: pero nosotros también usamos historical data, no? qué diferencia hay con lo nuestro? por qué lo nuestro es lo mejor?

%
% pasar a contar los casos de la predicción de ventas en la 
% industria de la moda (SThomassey2014) y de nuevos productos (ChingChin2010)
%
% Son problemas muy similares, ya que no hay datos históricos de los nuevos productos,
% al igual que no los hay sobre ventas, y en los que además influyen mucho las variables 
% externas (estacionalidad, modas, fama del author, etc).
% Hay que echar mano de productos similares o simplemente de los datos que describen el nuevo producto.
%

In order to deal with this issue, i.e. forecasting sales of new products, some authors have used different data mining methods \cite{Hammond1990,Chang2009,ChingChin2010,Thomassey2012,Xia2012}. 
% Antonio - quizá habría que comentar alguno o todos bervemente...
%
In the same line, Thomassey et al. \cite{SThomassey2014} propose using a clustering method together with decision trees for sales forecasting in textile-apparel fashion industry, 
a very similar issue to the new book publishing one.
The same problem was previously faced by Xia et al. \cite{Xia2012} using a 
forecasting method based on extreme machine learning with adaptive metrics of inputs.
% Antonio - of -> as???
The complexity of this problem relies on the lack of historical data, on the short lifetime of the majority of items, 
% Antonio - ver si está bien el cambio 'large number' -> 'majority'
and on the influence of variables such as promotions, fashions, or economic environment \cite{Thomassey2012,Xia2012,SThomassey2014}, as mentioned above.
%FERGU: completar con: "In our case, these ideas ..."

  %[Pedro] otro caso muy similar es el paper de ChingChin2010 sobre:
  % New tea product
  % New cosmetic product
  % New soft drink product
In \cite{ChingChin2010}, a decision-support system for new product sales %FERGU: The work of Chinchin et al.?
forecasting is proposed to solve three real-world sales forecasting problems, namely: new tea, cosmetics, and soft drink products. This is addressed taking into account quantitative variables.
% Antonio - qué significa eso?
In that study, products are classified based on their sales patterns, so it is assumed that products in the same class would follow a similar sales pattern. 
However, as the authors state, the proposed system can not deal with qualitative 
data related to the products. Moreover, real-world cases, such as consumer 
electronics and fashion products, should be deeply analysed, as these industries 
present/launch new products every season. 

  %[Pedro] otro caso a comentar es el paper de Chang2009 sobre: 
  % sales forecasting in printed circuit board industry.
In those cases where historical data are available, classical time series 
forecasting methods can be applied, which is what the authors do in
\cite{Chang2009}, where a hybrid model integrating K-Means and fuzzy
neural networks to forecast the future sales of a printed circuit
board is proposed. % We have been talking above that above and SAID IT
                   % DIDN'T WORK? Why do we say it here again? - JJ
                   % No, above we said that the efficiency depends on the field 
                   % of application and the available problem data. - pedro
A similar method is described by Chern et al. \cite{ChernWSF15}, who analyse 
historical data along online reviews, reviewer characteristics and review 
influences to understand how electronic `word-of-mouth' influences product sales. 
The proposed method is suitable for those sales forecasting problems 
with a big amount of online reviews and historical data.

Thus, taking into account the kind of problem we address in this paper, i.e. the prediction of sales of a new product for which no historical data are available, the classical forecasting methods based on time-series are not
adequate, although they can obviously be used once the book has been launched for sale. 
In our work the relevant variables in the prediction of sales are identified
by means of data visualisation and feature selection methods. Then, machine learning techniques are applied to predict sales and to infer the ideal print-run of a book.
In addition, since publishers need models to explain obtained predictions, 
`black box' forecasting methods may not be suitable, so models based
on decision rules have been tested as an alternative. 



%********************************************************************************
\section{Problem description}
\label{sec:problem}


The main objective of this paper is to forecast sales for 
%FERGU: el objetivo es la herramienta o la predicción?
unpublished books for which no historical data are available. Instead, just some
%FERGU: pero usamos datos históricos de otros para la predicción, no? Como otros papers...
descriptive and variables known in advance related to the new book. In
order to do that, we have used a training dataset which includes
historical sales data for a big amount of books published in Spain for
several years, with a set of 50 features or variables per pattern.  

The initial dataset, provided in the framework of a contract by
Trevenque Editorial S.L., included 3159 books. Table
\ref{tabla:paramsOrig50} shows the initial set of variables describing
each book. 

\begin{table}[!ht]
\caption{Parameters describing a book, provided by the company Trevenque S.L.}
\label{tabla:paramsOrig50}
\begin{center}
\begin{tabular}{|c|c|}
\hline 

\begin{minipage}{2.45in} \begin{verbatim}
reference
author
retail price
subject1
subject2
subject3
editorial
collection
bookbinding
print-run
total sales
dept stores sales
sales through delegates
rest of sales
total sales 1st year
mall sales 1st year
delegates sales 1st year
rest of sales 1st year
total distributed
distributed through dept stores
distributed through delegates
distributed - rest
total distributed 1st year
distrib. through dept stores 1st year
distrib. through delegates 1st year
\end{verbatim} \end{minipage}     & 

\begin{minipage}{2.3in} \begin{verbatim}
rest of distributed 1st year
reprints
number of reprints
total returns
returns - dept stores
delegates returns
rest of returns
total returns 1st year
returns -  dept stores 1st year
delegates returns 1st year
rest of returns 1st year
gifts
units distributed as novelty
total points of sale
points of sale - dept stores
delegates points of sale
rest of points of sale
total points of sale 1st year
dept stores points of sale 1st year
delegates points of sale 1st year
rest of points of sale 1st year
weeks for sale
positive sales environment
medium sales environment
negative sales environment
\end{verbatim} \end{minipage}    \\

\hline
\end{tabular}
\end{center}
\end{table}

% Antonio - briefly describe the variables

In this prediction problem there are other external variables that also affect 
the sales of a new book, such as those related to seasonality, trends or 
promotions, that classical prediction methods can not deal with. %FERGU: por qué?
However, an expert can consider them to modify the forecast obtained by 
the automatic method. Even publishing companies tend to define rules or 
indexes to subjectively adjust the results of prediction based on
external data. 
Thus, the proposed method, will analyse the provided data and
variables and will yield a forecast of sales according to them. This
result could be used as a reference for a human expert, who would
consider it in his/her own prediction process. 

% This section is too short and really does not describe the problem
% to its full extent. First, it should say if some of these variables
% were discarded for some reason. Group the variables we have in those
% available to publisher and those that can be used afterwards. And
% then give an idea of the complexity of the problem and how difficult
% it should be to find a solution - JJ


%********************************************************************************
\section{Methodology}
\label{sec:methodology}

There is not a standard methodology to solve the general problem of sales forecasting. The steps we have followed in this paper are described below.
% Antonio - quito esto porque ya no es lo que se ha hecho. El objetivo del paper no es la metodología a seguir, eso es estándar. ;)
%FERGU: hacer más hincapié de esto en la intro entonces
%In this work we have followed these steps: %FERGU: por qué estos steps y no otros? Ponerlo aquí. Esta metodología es general o solo para este paper?
%\hspace{2cm}
%\begin{verbatim}
%  Step 1: Collect and analyse data
%       - Data preprocessing
%       - Feature selection
%  Step 2: Select the best forecasting method
%       - Evaluate forecasting models
%       - Perform forecast
%  Step 3: Subjectively adjust obtained results
%      - Apply additional expert knowledge, such as information 
%        about seasonality or trends.
%\end{verbatim}

%Usually, a preliminary analysis of the data is carried out to eliminate those 
%variables that describe the product but do not provide useful information during 
%the prediction process.

First, after the editorial company has provided the raw data
(extracted directly from its database), a preprocessing method has
% This should go to the previous section. 
been performed in order to remove incorrect or incoherent patterns,
such as those with null values in required variables, or with negative
sales, for instance. Moreover, the initial set of variables (presented
in Table \ref{tabla:paramsOrig50}) has been reduced by an expert to
use just those that can be known in advance when a new book is
launched, i.e. \textit{pre-sales data}. 

% You have to present the problem and not the solution. It's not "what
% you did last summer", it is SCIENCE. Why did you think FS was
% needed? What procedure did you use for it?
Then, a Feature Selection (FS) process \cite{kittler1986feature} has
been carried out, as it is an effective dimensionality reduction
technique and an essential preprocessing method to discard `noisy'
(non-useful) features \cite{Krishnapuram2004}. The aim of these
% Did you know there were noisy features? Why? 
% You should include in the previous section an exploratory data
% analysis that describes the range of variables, how many categories
% those variables have, and so an do forth - JJ 
algorithms is exploring the space of attributes to find out which
subset of them yields the best results in a classification or
prediction problem. 
  In this stage, both, numerical and visualisation-based approaches have been applied in this paper, in order to select the best variables for the forecasting 
methods.  
% Antonio - Decidir si es 'forecasting' o 'regression' (o si son indiferentes) y usarlo siempre en el texto
  This task may also help to identify leading indicators to be considered by a human expert to make predictions.

Moreover, the application of FS also aims to reduce the computational time for building the forecasting models, along with the complexity of the obtained models themselves. This has been a key factor in this work since one of the objectives is to obtain `easily' interpretable decision models, which can be useful for human experts (editors).

After this process, several forecasting techniques have been tested, such as rule- or tree-based, together with numerical methods. 
% Antonio - is 'numerical' the best term for these kind of methods?
%FERGU: OJO que en esta sección a veces usáis el presente perfecto (have been) y otras el presente (is performed)
% Antonio - Corregido. ;)
%
The obtained results have been compared and analysed, computing, in addition to the obtained \textit{correlation coefficient}, some error measures from the literature \cite{ChingChin2010,Madsen2008}. We have considered the widely accepted standard formulae \cite{gepsoft} (Equations \ref{eq:MAE} to \ref{eq:RRSE}), which are implemented in tools like Weka \cite{pentaho,Witten2011} or R \cite{otexts,Hyndman2013}.
Specifically, in order to compute the forecasting errors of the different methods, Mean absolute error (MAE), Root mean squared error (RMSE), Relative absolute error (RAE) and Root relative squared error (RRSE) have been selected:
%FERGU: have been selected for this methodology, for this work... algo así
% Antonio - ya se ha dicho. ;)

\begin{itemize}
  \item \emph{Mean Absolute Error} (MAE):
        \begin{equation}\label{eq:MAE}
            MAE = \frac{1}{n}\sum_{i=1}^n {\mid p_i - o_i\mid}
        \end{equation}

  \item \emph{Root Mean Squared Error} (RMSE):
        \begin{equation}\label{eq:RMSE}
            RMSE = \sqrt{ \frac{1}{n}\sum_{i=1}^n {(p_i - o_i)}^2 }
        \end{equation}

  \item \emph{Relative absolute error} (RAE):
        \begin{equation}\label{eq:RAE}
            RAE = \frac{ \sum_{i=1}^n {\mid p_i - o_i\mid} }{ \sum_{i=1}^n {\mid p_{i-1} - o_i\mid} }
        \end{equation}

  \item \emph{Root relative squared error} (RRSE):
        \begin{equation}\label{eq:RRSE}
            RRSE = \sqrt{ \frac{ \sum_{i=1}^n {(p_i - o_i)}^2  }{ \sum_{i=1}^n {(p_{i-1} - o_i)}^2 }  }
        \end{equation}
\end{itemize}
where $o_i$ is the individual data $i = {1,...,n}$ and $p_i$ is the obtained prediction.

Once the models and results have been obtained for the whole dataset, the same methodology has been applied to some specific cases (editorials) in order to prove the value of the proposal.
Some specific interpretable models have been created as a final result of the work. The aim is that these will cold be used as a decision-aid tool for the experts who have to take decide about the best print-run quantity for new published books, in order to, logically maximise the benefit (or minimise the costs).

%The created forecasting models and results, along with the selected subset of features, are analysed and presented to the experts in order to aid his/her decision. They might manually adjust the forecasting results taking into account their knowledge in industry experience and their perception about trends.

%FERGU: poner aquí algo de un párrafo que explique como se valida la metodología
% Antonio - TODO: esto habrá que explicarlo mejor cuando esté hecho y veamos lo que sale. ;)

%-------------------------------------------------
\subsection{Data collection}
 

In general, historical data are used to make time series, and from them extracting common patterns and obtaining accurate predictions.
However, in the case of new products, such as the issue of new books, past patterns are difficult to observe as there is no available historical sales data 
\cite{ChingChin2010}. Thus, only pre-sales data are available in this problem.

Moreover, the initial set of features in the dataset must be revised in order to consider just those that can be known before the launch of the book, i.e. those whose value don't depend on the book sales. So, we have selected the subset of features described in Table \ref{tabla:params_pre_sales}.


\begin{table}
% Shouldn't this table go to "Problem description"? It is a
% preliminary selection - JJ
\caption{Considered pre-sales features and their types. It also shows
  the names of the variables used in the applied methods and if everyone is an input (independent) or an output (dependent) variable.} 
\label{tabla:params_pre_sales}
\begin{center}
\begin{tabular}{|c|l|l|c|c|}
\hline 
No. & Feature Name & Variable & Type & In/Out\\
\hline 
1 & retail price (when launched) & \texttt{ret\_price} & numerical & input\\
2 & main subject (code) & \texttt{subject1} & numerical & input\\
3 &  editorial + collection (joint code) & \texttt{collection} & categorical & input\\
4 & bookbinding (code) & \texttt{binding} & categorical & input\\
5 & gifts (promotional books) & \texttt{gifts} & numerical & input\\
6 & units distributed as novelty & \texttt{distrib\_novelty} & numerical & input\\
7 & total number of points of sale & \texttt{tot\_points\_sale} & numerical & input\\
8 & total number of points of sale (1st year) & \texttt{tot\_points\_sale\_1st\_year} & numerical & input\\
9 & weeks on sale & \texttt{weeks\_sale} & numerical & input\\
10 & print run & \texttt{print\_run} & numerical & input\\
\hline 
\hline
11 & total sales & \texttt{total\_sales} & numerical & output\\
\hline 
\end{tabular}
\end{center}
\end{table}

As it can be seen in the table, \texttt{total\_sales} is the output variable, so it is the objective of the prediction methods. \texttt{print\_run} has been considered as an input variable because it is something to be decided in advance by an editor. In addition, this variable could be interesting to be used to test the prediction model, once it is built. So the editor could try different values for it and see what the predictor yields, in order to take the best decision.

Initially, there were three different features describing the subjects and subsubjects of every book \textit{subject1,2,3} (see Table \ref{tabla:paramsOrig50}), but for most of the patterns (books) there is no value for subjects2 and 3, so we have discarded these features. 

Regarding the feature \textit{editorial}, we have removed it from the final dataset since it is indeed included in the code of \texttt{collection}. In addition, we think it is not really relevant for the accuracy of the models and, moreover, editors will not consider the results in other editorials for predicting its own sales.
% Antonio - Think about this... should we mention this?

Therefore, in order to create simpler and more accurate prediction models, and with the aim to provide them as a useful decision-aid tool for the editor, we have transformed the categorical variable \texttt{subject1} into a numerical one, considering it refers to a code of the Dewey Decimal Classification system for books \footnote{\url{https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes}}, and the existing similarity relationship between the books inside a class or subclass. So, for instance, all the books inside subdivisions of class 200, `Religion', are considered as similar regarding their topics, so they could be also close in terms of distance. The same reasoning can be applied to subclasses.



%-------------------------------------------------
\subsection{Data preprocessing and initial analysis}
\label{subsec:data_preproces}

Once a dataset has been composed it must be preprocessed, i.e. it
should be revised in order to fix the existing errors (such as wrong
or duplicated patterns), or to complete (or address) the absence of
information, for instance. The aim is looking for the correctness,
homogeneity and completeness of the data. % useless paragraph - JJ

%You need to lay out to the reader what you are going to do here. In
%order to reach objective x, we need to do y, that is why we use
%z. After using z, we prove that we have done y and thus reached
%objective x. For instance
% A exploratory data analysis that shows the distribution of variables
% as well as a priori clusters is the first phase in any data
% processing method
% (BTW, you should include data visualization in problem description,
% because it shows the distribution of problem variables and then
% suggests ways to deal with it) - JJ
% and so on...

After this, the features describing every pattern are frequently analysed.
In this case, among the variables describing a new book, some may affect 
predictions (those variables provide information), while others may not be 
necessary and even redundant.

% You have to justify its application to this particular problem. What
% was the runnning time with all variables? What reduction was your
% target? - JJ
Moreover, the algorithm running time grows with the number of
features, making it impractical for problems with a large number of features 
\cite{Selvakuberan2008}. Feature reduction is a task that is usually
applied in these kind of situations, having two ways of performance
\cite{kittler1986feature}: feature extraction and feature selection.  
% two ways of performance? WTF? Do you mean "being performed in two
% different ways?" - JJ
%Feature selection task is usually carried out by 
%searching through all possible combinations of features, evaluating each one. 

% During the evaluation, the set of all possible features is analyzed in order 
% to find the best set of features, ranking them according to some metric.
% In this process, the feature's predictive ability along with the degree 
% of redundancy between them is considered.

Feature extraction is a ``transformation-based approach'', therefore
it transforms the original meaning of the features. This approach
involves creating a subset of new features by combinations of the
existing features, thus it is employed when the semantics of the
original dataset will not be needed by any future process. On the
contrary, feature selection attempts to retain the meaning of the
original feature set. It is part of what is called semantic-preserving
techniques, known as ``selection-based approaches". By searching
through all possible combinations of features, the aim of these
techniques is to determine a minimal feature subset to reduce
processing time, while obtaining the same or higher accuracy as the
initial feature set \cite{liu1998feature}. In the problem this work
addresses, it is very important to maintain the semantics of the set of
initial variables, and therefore several feature selection methods
have been applied. 

In addition, in order to reduce the dimensionality of the input data
and to choose an appropriate set of variables, the Self-Organizing Map
(SOM) \cite{kohonen1998} method has been also applied. % Why? Is that
                                % one of the two forms of feature
                                % selection? What do we expect to
                                % learn from it? - JJ 

Kohonen's SOM is a feedforward neural network \cite{Haykin98_NNC} that
uses an unsupervised training algorithm and which, through a process
called self-organisation, configures the output units into a
topological representation of the original data. It tries to imitate
the self-organisation done in the sensory cortex of the human brain,
where neighbouring neurons are activated by similar stimulus. SOM
belongs to a general class of neural network methods, which are
non-linear regression techniques that can be trained to learn or find
relationships between inputs and outputs or to organise data so as to
disclose so far unknown patterns or structures. It is usually used as
a clustering/classification tool or used to find unknown relationships
between a set of variables that describe a problem. The main property
of the SOM is that it makes a nonlinear projection from a
high-dimensional data space (one dimension per variable) on a regular,
low-dimensional (usually 2D) grid of neurons, called units. 
SOM is further processed using Ultsch method \cite{UmatUlts}, the
Unified distance matrix (U-matrix), which uses SOM's codevectors
(vectors of variables of the problem) as data source and generates a
matrix where each component is a distance measure between two adjacent
neurons. It allows us to visualise any multi-variated dataset in a
two-dimensional display, so we can detect topological relations among
neurons and infer about the input data structure. High values in the
U-matrix represent a frontier region between clusters, and low values
represent a high degree of similarities among neurons on that region,
clusters .

SOMs (and U-Matrix) are usually applied to visualise natural
structures in the data and their relations, as well as the natural
groupings that could be among them. In addition, SOM makes easy the
estimation of the variables that have more influence on these
groupings. Other statistical and soft computing tools can also be used
for this purpose, but since Kohonen's SOMs offers a visual way of
doing it, it is much more intuitive, and takes advantage of the
capabilities of the human brain as a pattern recogniser. 
%FERGUSON: por qué se menciona aquí la palabra "Kohonen" y no antes?
% Antonio - arreglado 
% And this is going to be useful because... - JJ

This method is usually applied to analyse data as similar items tend to be 
mapped close together, while those items dissimilar, are mapped apart.
Also, the SOM graph represents and highlight very clearly those
regions (clusters) with high training sample concentration and fewer
where the samples are scarce.  
%FERGU: mencionarlo en la intro
% Which is interesting because... JJ

As far as the feature selection methods are concerned, Weka software separates the process into two parts:
%FERGU: por qué mencionamos a Weka aquí y no antes?
% Antonio - se ha mencionado, pero sí, habría que destacarlo un poco más desde la intro quizá.
\begin{itemize}
  \item Attribute evaluator: This is the method by which the feature subsets are assessed. In this work, both a correlation-based feature subset evaluation method ({\tt CfsSubsetEval}) and an adaptation of relief for attribute estimation ({\tt ReliefFAttributeEval}) evaluation method are used. 
  
  The Correlation Feature Selection (CFS) method evaluates subsets of features/variables on the basis of the hypothesis made by Hall in \cite{Hall1998}, ``A  good  feature  subset  is  one  that  contains  features  highly  correlated  with
(predictive of) the class, yet uncorrelated with (not predictive of) each other''. This evaluator needs the numeric features to be transformed to nominal features first, by being discretised. The CFS evaluation function measures the ``merit'' of the subsets, which depend on the mean feature-class correlation, and the average feature-feature intercorrelation. In addition, the correlation is measured by three variations: the Minimum Description Length (MDL), the symmetrical uncertainty, and the ``relief''.

``Relief'' is the second evaluator used in this work. It is important to note that the relief method implemented in Weka is an updated version \cite{RobnikSikonja1997}, called RRELIEFF, of the original by Kira \& Rendell \cite{Kira1992}. The algorithm works by evaluating single attributes, and not whole subsets of attributes as CFS does. Also, it is applicable even working with both nominal and numeric features, without the need for transformations. For this algorithm, the weight of an attribute depends on the difference of an attribute value inside an instance with its neighbours, being these the nearest instance of the same class, and the nearest of the different class. Therefore, a good attribute is that which gives similar values for instances of the same class, and different values for instances of different class.
  \item Search method: This describes the structured way in which the set of possible feature subsets is studied, based on the results of the evaluator. With regard to the used search criteria, it is important to take into account that CFS evaluates attribute subsets, and Relief evaluates attributes separately.

For this reason, the search method used with CFS has been BestFirst, that searches the space of feature subsets by greedy hill-climbing augmented with a backtracking facility. On the other hand, Ranker is the searching criteria used with the Relief evaluator. This method ranks features by their individual evaluations.
\end{itemize}

Finally, Section \ref{sec:experiments} provides a comparison between the feature reduction through SOM, and the feature selection techniques that have been described.

%-------------------------------------------------
\subsection{Selecting an appropriate forecasting method}

As previously stated, there are many prediction methods available in the 
literature, each one with a different parameter set that may affect the 
obtained results.

However, as in the case of new books only some descriptive data are available 
(compared to those cases in which historical data on sales of published books 
is available), not all forecasting methods can be used in this specific problem.

Thus, in this research five forecasting methods, based on the literature 
review in the introduction 
\cite{Madsen2008,ChingChin2010,Thomassey2012,Xia2012,SThomassey2014}, have been used. 
Well known forecasting methods implemented in the Weka tool
\cite{Hall2009,Witten2011} have been chosen, as these methods are
widely known, and could be very helpful for practitioners to reproduce
experiments and even to solve similar sales prediction problems using
these methods. % Why these and not others? Exploratory data analysis?
               % - JJ

Specifically, the following forecasting methods are proposed: 
        % Proposed why? - JJ
        % because those methods were those that could lead with the problem data - pedro
\begin{itemize}

 % weka.classifiers.trees.M5P
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/M5P.html
 \item \emph{M5 Model trees}: 
A decision tree consists of answer-nodes, that indicate a class, and decision-nodes, 
that contain an attribute name and branches to other sub-decision trees.
Building a decision tree can be done using many algorithms, i.e. ID3 and C4.5 \cite{Quinlan1986}.
However, in order to use this model in regression problems, some authors have 
extended the model using methods such as the M5 model tree \cite{Quinlan1986,Quinlan1992,Wang1997,WittenFrank2000}, 
by combining a conventional decision tree and generating linear regression 
functions at the nodes.

The construction of a model tree is similar to that of classical decision 
trees \cite{Solomatine2004}:
The process breaks the input space of the training data through decision points 
(nodes) to assign a linear model suitable for that sub-area of the input space. 
This process may lead to a complex model tree.

Model tree models can learn and tackle tasks with high dimensionality (up to 
hundreds of attributes) and generate reproducible and comprehensible representations, 
what makes them potentially more successful in the eyes of decision makers.

The final model consists of the collection of linear sub-models that brings the 
required non-linearity in dealing with the regression problem
and both the predicted values at the answer-nodes along with the path from the 
root to that node is given as a result.


 % weka.classifiers.lazy.IBk 
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/lazy/IBk.html
 \item \emph{k-Nearest Neighbours (kNN)}: 
The kNN \cite{Aha1991,Mitchell1997} method is an instance-based classifier in 
which the unknown instances are classified by relating them to the known 
instances using a distance measurement/function (Euclidean, Minkowsky or minimax).

The main idea behind the algorithm is that two instances far enough in the space, 
taking into account the distance function, are less likely to belong to the 
same class than two closely situated instances.

The classification algorithm locates the nearest neighbour in instance space and 
assigns the class of that neighbour to the unknown instance.
In order to improve the robustness of the model, several neighbours can be 
located, assigning the resulting class to an unclassified vector using the 
closest $k$ vectors found in the training set by majority vote.


 \item \emph{Random Forest}: This method is based in the construction of a set of decision trees using a stochastic process over the basis of C4.5 algorithm. It was proposed by Breiman \cite{Breiman2001} and aims to create independent and uncorrelated trees based on different and random input vectors, following the same distribution.
The result is the average of the generated trees during the process.


 % weka.classifiers.functions.LinearRegression
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/LinearRegression.html
 \item \emph{Linear Regression}:
 Linear regression \cite{Cohen2003,Yan2009,Rencher2102} method models the 
relationship between a scalar dependent variable (outcome) and several independent 
variables (predictor) over the range of values in the dataset. 
In this statistical technique, data are modelled using linear functions to estimate unknown model parameters, examining the linear correlations between independent and dependent variables.

This method of regression provides an adequate and interpretable description of 
how the input affects the output as it models the dependent variable as a linear 
function of the independent variables. Moreover, the linear relation can be 
solved using the least squares method, that minimises the error between the 
actual data and the regression line \cite{McClendon2015}.


 % weka.classifiers.functions.SMOreg
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/SMOreg.html
 \item \emph{Support Vector Machine for Regression (SVM)}:
Support vector machine (SVM) \cite{Cortes1995,Shevade1999} method is based on  
statistical learning theory and has successfully used in classification and  
regression problems \cite{Cao2003,Jari2008}.

In classification problems the algorithm searches for an optimal hyperplane 
that separates two classes, maximising the margin between two classes. 
In the case of regression the algorithm chooses a hyperplane close to as many 
of the data points as possible, minimising the sum of the distances from 
the data points to the hyperplane. 
In both cases, the hyperplane is defined by a subset of training set samples 
(called support vectors).
%This method works well even if the space is highly dimensional and the problem is not linearly separable.


% Antonio - MLP no se ha aplicado al final...

 % weka.classifiers.functions.MultilayerPerceptron
 %		http://weka.sourceforge.net/doc.dev/weka/classifiers/functions/MultilayerPerceptron.html
% \item \emph{Multilayer Perceptron}:
%A Multilayer Perceptron (MLP) \cite{Rosenblatt1962,Widrow1990} is a feedforward 
%artificial neural network model that maps the input data onto an appropriate output. 
%It is an artificial neural network generally used for classification or 
%approximation problems.
%
%This model is a generalisation of the standard linear perceptron that uses several 
%layers of nodes (called neurons) and that is able to solve linearly inseparable 
%problems \cite{SteinwenderBitzer2003}.
%Each neuron consists of a linear combination of weighted inputs which is passed 
%though a non-linear activation function to produce its output.
%
%This kind of artificial neural network is trained using a supervised learning 
%technique called back-propagation.
%This training method was developed independently by Werbos \cite{Werbos1974}, 
%Parker \cite{Parker1985} and Rumelhart et al. \cite{Rumelhart1985}, and consists
%in updating the weights of the output layer neurons once the erroneous output 
%has been obtained, and then, propagating the successive weight layers back to 
%the input layer.
%
%A key issue when designing an MLP is the number of hidden layers of neurons.
%Lippmann proved in \cite{Lippmann1987} that two hidden layers are enough to 
%create classifying regions of any kind. This result was then confirmed in later 
%works by Bishop \cite{Bishop1996} and Reed et al. \cite{Reed1999}.

\end{itemize}

The objective is finding the most appropriate forecasting method that minimises 
the error between the obtained forecast and the actual sales for each new book 
published.


%********************************************************************************
\section{Experiments and Results}
\label{sec:experiments}

% An intro should go here - JJ
% Antonio - done

This section presents the experiments conducted over the dataset considering the pre-sales variables forecasting new book sales.
Feature selection techniques have been also applied in order to test their utility by comparing the obtained results.

We have analysed the whole dataset (all the editorials), but also performed some experiments considering specific editorials as example cases.
The obtained decision trees have been plotted to show their simplicity and their real value as a decision-aid tool for editors.

%--------------------------------------------------------------------------

\subsection{Experimental setup}

In most data mining and machine learning algorithms, the values of
some parameters have a high influence on its performance. In our
experiments, the best value of $k$ in the $k$-nearest neighbour was
empirically found at $k=3$. % What???? - JJ
 For SVM, the widely used Radial Basis
Function (RBF) kernel is selected. As reported in several studies, RBF
kernel has a reliable performance and can analyse higher dimensional
data \cite{yu2004ec,chao2015construction,huang2006ga}. Two important
parameters should be tuned in SVM: the Cost $(C)$ and $(\gamma)$ of
the RBF kernel. We apply grid search which is the most common approach
to find $C$ and $\gamma$. The best performance of SVM was found at
$C=200$ and Gamma $\gamma = 0.01$. For MLP, we apply it with one
hidden layer since it was proven in the literature that an MLP with
one hidden layer can approximate any continuous or discontinuous
function \cite{hornik1989multilayer,mirjalili2014let}. However, there
is no rule of thumb on selecting the best number of neurons in the
hidden layer. One suggested method that is followed in this work is to
set the number of neurons in the hidden layer to $2 \times I + 1$
where $I$ is the number of inputs in the dataset
\cite{wdaa2008differential,mirjalili2014let,mirjalili2015effective}. 

All experiments are performed using 10-fold cross validation \cite{Geisser1993,Kohavi1995,Devijver1982}. The dataset is partitioned into 10 parts where the training process is carried out on 9 parts then the resulted model is tested on the left 10th part. This process is repeated 10 times, each time the model is trained using different 9 parts. At the end of the process the 10 evaluation results on the testing parts are averaged. 
This technique is used to estimate how accurately the predictive model will perform in practise, limiting, at the same time, the overfitting problem.

In addition, due to the stochastic component present in some of the methods and also in order to evaluate the running time, 30 repetitions of each experiment have been done per method. Then the average and standard deviation have been computed.

***TODO***: talking about the machines used

%Experiments have been conducted on an Intel(R) Core(TM) i5-4430 CPU at 3.00GHz 
%(4 cores) and 16 GB RAM, running Ubuntu Linux 14.04.1 LTS and Java JRE\_1.7.0\_72.


%--------------------------------------------------------------------------

\subsection{Results for all the editors}
\label{subsec:results_all_editors}

In the first experiment, forecasting methods have been applied to the whole dataset. All the pre-sales features have been considered, also including the categorical ones, which make harder to process the instances and build the different models. SVM for instance, must previously transform these categories into numbers in order to deal with the data, which means the creation of a different (and new) variable per possible value. Thus, in the case of editorial or collection this would imply the definition of hundreds of new variables.
% Antonio - Sam, verify this, please

Correlation coefficient, error measures and time to build the model have been computed for every method. Obtained results are presented in Table \ref{tab:all_editors_no_fs}.

\begin{table}
\caption{Predicting Total sales for all editors (all the features). 30 repetitions, 10-fold cross validation.
\label{tab:all_editors_no_fs}}
\centering{}%
%{\scriptsize
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
 & Corr. coeffic  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 0.890 $\pm$ 0.09 & 214.365 $\pm$ 22.96 & 634.229 $\pm$ 307.76 & 37.457 $\pm$ 3.12 & 48.324 $\pm$ 25.16 & 18.845 $\pm$ 0.61\\
\hline
K-NN & 0.787 $\pm$ 0.08 & 272.294 $\pm$ 32.38 & 840.594 $\pm$ 240.11 & 47.475 $\pm$ 3.39 & 63.072 $\pm$ 10.29 & 0.001 $\pm$ 0.01\\
\hline 
Random Forest & 0.840 $\pm$ 0.05 & 253.365 $\pm$ 29.43 & 788.937 $\pm$ 282.01 & 44.164 $\pm$ 2.76 & 57.766 $\pm$ 8.15 & 4.906 $\pm$ 0.12\\
\hline 
Linear Regression & 0.872 $\pm$ 0.07 & 308.679 $\pm$ 25.44 & 680.905 $\pm$ 226.41 & 54.053 $\pm$ 4.38 & 52.130 $\pm$ 18.29 & 984.687 $\pm$ 128.21\\
\hline 
SVM & 0.876 $\pm$ 0.04 & 248.763 $\pm$ 27.49 & 732.641 $\pm$ 260.56 & 43.371 $\pm$ 2.35 & 53.471 $\pm$ 6.29 & 6169.239 $\pm$ 281.49\\
\hline 
\end{tabular}
%}
\end{table}

***TODO***: Revise and fix the following text...

As far as the different methods is concerned, M5P model obtained good results, 
both in time and error, and as stated before, this method can learn and tackle 
tasks with high dimensionality and it generates reproducible and comprehensible 
representations that provide information on the decision process and does not 
require human intervention either for the operation or for interpretation. 
Thus, it is very suitable in order to be incorporated in a publishing company 
processes.

The Linear Regression model outperforms some other complex methods, both in time 
(as it is a simple method to train) and in obtained errors.
However not in all cases, as more powerful models that obtain better results 
can be obtained in comparable time.

Taking into account the SVM model obtained results, this method obtains comparable 
errors to those of M5P. Even when used using the default parameter values it exhibit 
a good generalisation performance.
However, it is much slower than the others to build the model, not only when 
using the dataset with categorical input variables, but also when using the 
datasets with numerical variables.

kNN is the faster method used in this study, as it is the simpler method.
However, the model is slow when classifying (as there are a large number of 
training examples), and it exhibit a poor generalisation ability, as it does not 
learn anything from the training data.

Moreover, in these experiments it has been seen the difficulty to process 
information related to categorical variables, such as \emph{editorial} and 
\emph{collection}.

%[Pedro] ¿aplicamos tests estadísticos para ver si hay diferencias entre métodos?

Finally, paying attention to the cost in time to obtain the forecasting models 
and their accuracy, the M5P model is the most adequate to solve the problem of 
how many books should be printed when a new book is published.
This forecasting method, as an additional advantage, can be used for products 
with historical sales data and for new products with little or no historical 
data available.



%--------------------------------------------------------------------------

\subsection{Feature selection}
\label{subsec:feature_selection}

In this second experiment, different feature selection processes have been applied.

***TODO***: The aim of these techniques is to reduce the number of features to consider...
% Antonio - TODO: explain the aim of FS in these experiments.

We have used both the {\tt CfsSubsetEval} \cite{Hall1998} (a correlation-based feature subset evaluation method) and the ReliefFAttributeEval \cite{RobnikSikonja1997} (an adaptation of relief for attribute estimation) evaluation methods, implemented in the Weka tool and described in Section \ref{subsec:data_preproces}.

Then, the dataset has been projected using SOM, and a visual analysis of the obtained results has been carried out in order to determine the best variables to consider.
An important preprocessing step has been carried out before conducting the SOM analysis, so the two categorical features Editorial and Collection have been
removed from the analysis since SOM cannot deal with them without a previous transformation into numerical ones. However, since they have a large number of unique different values this would be finally impracticable and useless. 


Moreover,
four features (i.e; Subject1, Subject2, Subject3 and Book-binding)
have more than 10\% missing values so they are removed also.

***TODO***: describe the parameters of SOM (size, neighbourhood shape, etc) and obtained quantisation and topographical errors.

% The SOM algorithm is applied with a large map size of $26 \times 16$  in order to represent the data. The best SOM training results are obtained with a final quantisation error of 0.691 and topographic error of 0.046. 


Figure \ref{fig:componentplanes} shows the U-matrix and component
variables resulted from SOM training. The term component is used in
SOM to refer to the features. The component planes in Figure
\ref{fig:componentplanes} show the trend of values of the prototype
vectors of the SOM map units. These values are represented as
colours. The colorbar on the right of each component plane shows the
indication of each colour. On the other side, the U-matrix on the top
left corner shows the distances between the adjacent units. It is
important to note that high values in the matrix indicates large
distance between the units of the map. It can be noticed also that
there are more hexagons in the U-matrix than component planes since
the distances between units is also represented as hexagons. 

***TODO***: Justify the selected features by SOM (why have we chosen them?).

\begin{figure*}[ht]
\begin{center}
\includegraphics[scale=0.4]{som_planes.eps}
\end{center}
\caption{U-matrix and component planes.}
\label{fig:componentplanes}
\end{figure*}

%\begin{figure*}[http] 
%\begin{center} 
%\begin{tabular}{cc}
%\includegraphics[scale=0.55]{Umatrix.eps}  &
%\includegraphics[scale=0.55]{Labels.eps}  \\
%U-matrix & Labels 
%\end{tabular}
%\end{center} 
%\caption{Labeling the U-matrix using the total-sales variable.}
%\label{fig:Umatrix} 
%\end{figure*}

The selected features per method are shown in Table \ref{tab:features_selected}. Logically, just the independent or input variables are considered.

\begin{table}
\caption{Input features/variables selected by each method
\label{tab:features_selected}}
\centering{}%
\begin{tabular}{|c|c|c|c|c|}
\hline 
No. & Variable & ReliefFAttributeEval & CfsSubsetEval & SOM\\
\hline 
1 & \texttt{ret\_price} & $\checkmark$ &  & $\checkmark$\\
\hline 
2 & \texttt{subject1} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\hline 
3 &  \texttt{collection} &  &  & \\
\hline 
4 & \texttt{binding} &  &  & \\
\hline 
5 & \texttt{gifts} & $\checkmark$ &  & $\checkmark$\\
\hline 
6 & \texttt{distrib\_novelty} & $\checkmark$ &  & \\
\hline 
7 & \texttt{tot\_points\_sale} & $\checkmark$ &  & $\checkmark$\\
\hline 
8 & \texttt{tot\_points\_sale\_1st\_year} & $\checkmark$ &  & \\
\hline 
9 & \texttt{weeks\_sale} & $\checkmark$ &  & $\checkmark$\\
\hline 
10 & \texttt{print\_run} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
\hline 
\end{tabular}
\end{table}


Thus, we have run the same experiments as in Section \ref{subsec:results_all_editors} with the whole dataset (all the editors), but considering the selected features by each method. The results are shown in Table

\begin{table}

\centering{}%
\caption{Predicting Total sales for all editors based on features selected
by each method.
\label{tab:results_all_editors_fs}}

% RELIEFATTRIBUTEEVAL
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
\multicolumn{7}{|c|}{\textit{ReliefAttributeEval}}\\
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 0.892 $\pm$0.102 & 211.147 $\pm$22.41 & 616.347 $\pm$321.254 & 36.928 $\pm$3.414 & 47.299 $\pm$26.827 & 0.4906 $\pm$0.0223\\
\hline 
K-NN & 0.869 $\pm$0.087 & 231.782 $\pm$24.246 & 668.028 $\pm$205.135 & 40.483 $\pm$3.058 & 51.276 $\pm$16.807 & 0.0015 $\pm$0.0042\\
\hline 
Random Forest & 0.91 $\pm$0.048 & 198.16 $\pm$19.499 & 589.353 $\pm$195.801 & 34.61 $\pm$2.27 & 44.209 $\pm$11.634 & 3.633 $\pm$0.2039\\
\hline 
Linear Regression & 0.864 $\pm$0.081 & 346.875 $\pm$25.395 & 695.897 $\pm$242.49 & 60.754 $\pm$4.549 & 53.164 $\pm$19.432 & 0.0124 $\pm$0.0072\\
\hline 
SVM & 0.842 $\pm$0.039 & 252.868 $\pm$33.384 & 858.26 $\pm$318.225 & 44.012 $\pm$3.038 & 62.22 $\pm$7.148 & 38.9667 $\pm$5.7518\\
\hline 
\end{tabular}

% CFSSUBEVAL
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
\multicolumn{7}{|c|}{\textit{CfsSubEval}}\\
\hline
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 0.873 $\pm$0.09 & 266.777 $\pm$23.117 & 662.319 $\pm$245.272 & 46.66 $\pm$3.247 & 50.9 $\pm$20.35 & 0.4103 $\pm$0.0428\\
\hline 
K-NN & 0.84 $\pm$0.099 & 289.092 $\pm$22.834 & 743.917 $\pm$260.45 & 50.591 $\pm$3.496 & 57.301 $\pm$22.899 & 0.001 $\pm$0.0006\\
\hline 
Random Forest & 0.85 $\pm$0.106 & 278.417 $\pm$23.073 & 714.134 $\pm$278.654 & 48.713 $\pm$3.434 & 55.264 $\pm$24.412 & 2.479 $\pm$0.1264\\
\hline 
Linear Regression & 0.859 $\pm$0.062 & 356.35 $\pm$22.43 & 708.056 $\pm$195.145 & 62.423 $\pm$4.216 & 53.974 $\pm$14.053 & 0.0067 $\pm$0.0009\\
\hline 
SVM & 0.86 $\pm$0.063 & 288.474 $\pm$30.322 & 817.562 $\pm$253.237 & 50.302 $\pm$2.375 & 60.266 $\pm$5.675 & 32.7252 $\pm$5.7438\\
\hline
\end{tabular}\\

% SOM
\begin{tabular}{|c|l|l|l|l|l|l|}
\hline 
\multicolumn{7}{|c|}{\textit{SOM}}\\
\hline
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\\
\hline 
M5P & 0.866 $\pm$0.104 & 234.091 $\pm$23.186 & 699.563 $\pm$317.196 & 40.924 $\pm$3.248 & 53.698 $\pm$26.867 & 0.4754 $\pm$0.1228\\
\hline 
K-NN & 0.868 $\pm$0.086 & 237.91 $\pm$25.344 & 665.639 $\pm$212.05 & 41.547 $\pm$3.141 & 50.96 $\pm$16.721 & 0.0009 $\pm$0.0035\\
\hline 
Random Forest & 0.898 $\pm$0.051 & 211.867 $\pm$20.072 & 617.776 $\pm$187.155 & 37.017 $\pm$2.461 & 46.581 $\pm$11.144 & 2.932 $\pm$0.1083\\
\hline 
Linear Regression & 0.855 $\pm$0.075 & 351.993 $\pm$23.72 & 714.756 $\pm$214.213 & 61.65 $\pm$4.278 & 54.665 $\pm$16.881 & 0.0113 $\pm$0.0074\\
\hline 
SVM & 0.845 $\pm$0.035 & 267.706 $\pm$33.819 & 888.331 $\pm$308.932 & 46.604 $\pm$2.865 & 64.62 $\pm$5.445 & 32.2689 $\pm$4.7404\\
\hline 
\end{tabular}

\end{table}


% ***TO DO***: comment the results


% ***TO DO***: Think about including the graphs (scatter plots).

%In Figures \ref{conv2}, \ref{conv2ReliefAttributeEval}, \ref{conv2CfsSubsetEval} and \ref{conv2SOM} we depict scatter plots for the values of the actual total sales values against the predicted ones obtained by the developed models. 
%Those figures also show the correlation between the two variables and the proportion of variation in actual total sales that is explained by the predicted ones. 
%Figure \ref{conv2} corresponds to the results obtained using all available features without performing any feature selection method. In this figure, it can be noticed that SVM achieved the best results by explaining 88.2\% of the actual values while $k$-NN showed very poor results with only 49.8\%. 
%Figures \ref{conv2ReliefAttributeEval}, \ref{conv2CfsSubsetEval} and \ref{conv2SOM} show same plots after performing ReliefAttributeEval, CfsSubsetEval and SOM feature selection methods, respectively. 
%According to these figures, we can notice that there wasn't almost any significant degradation in the performance of the M5P, LR, SVM and MLP. 
%On the other side, the performance of $k$-NN was dramatically improved after the feature selection process by explaining 84.5\% of variation on average.
%
%
%\begin{figure*}[http] 
%\begin{center} 
%\begin{tabular}{ccc}
%\includegraphics[scale=0.22]{m5p.eps}  &
%\includegraphics[scale=0.22]{lr.eps}   &
%\includegraphics[scale=0.22]{svm.eps}  \\
%M5P & LR & SVM 
%\end{tabular}
%\begin{tabular}{cc}
%\includegraphics[scale=0.22]{knn.eps}  &
%\includegraphics[scale=0.22]{mlp.eps}  \\
%k-NN & MLP
%\end{tabular}
%\end{center} 
%\caption{Actual vs. predicted sales for M5P, LR, SVM, k-NN, MLP
%  models.}
% You should use a log-log scale here to see it more clearly. Besides,
% please say in a comment the script you are using to generate it, we
% are going to need it later on - JJ
%\label{conv2}
%\end{figure*}
%
%
%\begin{figure*}[http] 
%\begin{center} 
%\begin{tabular}{ccc}
%\includegraphics[scale=0.22]{m5p_RAE.eps}  &
%\includegraphics[scale=0.22]{lr_RAE.eps}   &
%\includegraphics[scale=0.22]{svm_RAE.eps}  \\
%M5P & LR & SVM 
%\end{tabular}
%\begin{tabular}{cc}
%\includegraphics[scale=0.22]{knn_RAE.eps}  &
%\includegraphics[scale=0.22]{mlp_RAE.eps}  \\
%k-NN & MLP
%\end{tabular}
%\end{center} 
%\caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using ReliefAttributeEval.}
%\label{conv2ReliefAttributeEval}
%\end{figure*}
%
%
%\begin{figure*}[http] 
%\begin{center} 
%\begin{tabular}{ccc}
%\includegraphics[scale=0.22]{m5p_CFS.eps}  &
%\includegraphics[scale=0.22]{lr_CFS.eps}   &
%\includegraphics[scale=0.22]{svm_CFS.eps}  \\
%M5P & LR & SVM 
%\end{tabular}
%\begin{tabular}{cc}
%\includegraphics[scale=0.22]{knn_CFS.eps}  &
%\includegraphics[scale=0.22]{mlp_CFS.eps}  \\
%k-NN & MLP
%\end{tabular}
%\end{center} 
%\caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using CfsSubsetEval.}
%\label{conv2CfsSubsetEval}
%\end{figure*}
%
%
%\begin{figure*}[http] 
%\begin{center} 
%\begin{tabular}{ccc}
%\includegraphics[scale=0.22]{m5p_SOM.eps}  &
%\includegraphics[scale=0.22]{lr_SOM.eps}   &
%\includegraphics[scale=0.22]{svm_SOM.eps}  \\
%M5P & LR & SVM 
%\end{tabular}
%\begin{tabular}{cc}
%\includegraphics[scale=0.22]{knn_SOM.eps}  &
%\includegraphics[scale=0.22]{mlp_SOM.eps}  \\
%k-NN & MLP
%\end{tabular}
%\end{center} 
%\caption{Actual VS predicted sale for M5P, LR, SVM, k-NN, MLP models applied on the selected features using SOM analysis.}
%\label{conv2SOM}
%\end{figure*}


%\begin{figure}[!ht] 
%\begin{center}
%  \epsfig{file=fig_time.eps,width=11cm}
%\caption{\small{Running time in seconds, obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). Plot shows that using categorical variables as part of the dataset makes the problem hard to solve. As expected, simpler methods are much faster to build the model than the complex ones.}}
%\label{fig:time}
%\end{center}
%\end{figure}

%\begin{figure}[!ht] 
%\begin{center}
%  \epsfig{file=fig_rae.eps,width=11cm}
%\caption{\small{Relative absolute error (RAE $\%$) obtained after applying the five methods to the reduced datasets (16, 8, 7 and 4 features each one). The M5P model obtains the lowest errors, whatever the dataset used.}}
%\label{fig:rae}
%\end{center}
%\end{figure}


% - indicar que el uso de las dos variables categóricas hace más difícil de aprender el conjunto
%Paying attention to the obtained results shown in tables \ref{tabla:resultsT}, 
%\ref{tabla:resultsSOM}, \ref{tabla:results9} and \ref{tabla:results4},
%using those categorical variables as part of a big dataset makes the problem 
%hard to solve, both in required time to build the forecasting models as well as 
%in the obtained errors.

%Moreover, some methods are unable to build the models in a reasonable time.
%However, as stated before, Trevenque expert sales manager, taking into account 
%his knowledge in industry experience and their perceptions, decided that 
%\emph{EDITORIAL} and \emph{SUBJECT} should be taken into account.


% - indicar pros y contras de los diferentes métodos
% - indicar qué método da mejores resultados
% - indicar qué método es tipo "caja-negra" y cuáles ofrecen modelos "explicables" 


%********************************************************************************

\section{Example Cases}
\label{example_cases}

This section will present some experiments conducted on the data concerning four different example cases, namely:
\begin{itemize}
    \item best seller editorial
    \item medium seller editorial
    \item best varied editorial
    \item medium varied editorial
\end{itemize}

% ***TO DO ***: describe better  (number of instances) and justify the special cases (why we have chosen them).

%--------------------------------------------------------------------------

\subsection{Sales forecasting with all pre-sales features}
\label{subsec:case_sales_all_features}

Case 1-A: Developing prediction models to predict Total sales for
Editor704 (An editor with the most number of different books sold) - 293
instances.

\begin{table}
\caption{Predicting Total sales for Editor704}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.889 $\pm$0.074 & 71.258 $\pm$21.291 & 125.19 $\pm$50.085 & 39.251 $\pm$11.769 & 47.495 $\pm$15.933 & 0.046 $\pm$0.0278\tabularnewline
\hline 
K-NN & 0.826 $\pm$0.1 & 87.602 $\pm$23.206 & 158.629 $\pm$54.164 & 47.621 $\pm$10.32 & 59.168 $\pm$14.428 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.871 $\pm$0.082 & 74.634 $\pm$22.166 & 139.743 $\pm$54.357 & 40.618 $\pm$10.214 & 52.219 $\pm$16.267 & 0.1069 $\pm$0.0053\tabularnewline
\hline 
Linear Regression & 0.838 $\pm$0.089 & 97.513 $\pm$18.874 & 155.273 $\pm$45.562 & 54.172 $\pm$13.598 & 60.314 $\pm$21.168 & 0.0069 $\pm$0.0012\tabularnewline
\hline 
SVM & 0.874 $\pm$0.066 & 78.743 $\pm$22.254 & 147.295 $\pm$57.99 & 42.428 $\pm$7.826 & 53.286 $\pm$9.351 & 0.0598 $\pm$0.0157\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-B: Developing prediction models to predict Total sales for
Editor-UDLU11 (An editor with a medium number of different books sold) - 180
instances.

\begin{table}
\caption{Predicting Total sales for Editor-UDLU11}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.775 $\pm$0.15 & 573.405 $\pm$192.219 & 860.313 $\pm$456.217 & 58.26 $\pm$15.867 & 65.997 $\pm$29.447 & 0.0238 $\pm$0.0018\tabularnewline
\hline 
K-NN & 0.732 $\pm$0.163 & 596.615 $\pm$205.368 & 933.058 $\pm$356.108 & 60.619 $\pm$17.854 & 71.363 $\pm$22.336 & 0 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.867 $\pm$0.073 & 449.161 $\pm$127.796 & 671.758 $\pm$212.739 & 45.575 $\pm$9.8 & 51.372 $\pm$11.159 & 0.0606 $\pm$0.005\tabularnewline
\hline 
Linear Regression & 0.733 $\pm$0.195 & 680.157 $\pm$263.788 & 1103.971 $\pm$850.685 & 69.775 $\pm$27.238 & 86.106 $\pm$71.316 & 0.0024 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.804 $\pm$0.122 & 529.349 $\pm$172.496 & 846.013 $\pm$341.117 & 53.26 $\pm$11.915 & 63.539 $\pm$18.247 & 0.029 $\pm$0.0151\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-A: Developing prediction models to predict Total sales for
Editor-UDL929 (The best selling editor) - 216 instances.

\begin{table}
\caption{Predicting Total sales for Editor-UDL929 (460752 sales) }


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.9 $\pm$0.098 & 670.244 $\pm$181.047 & 1065.337 $\pm$413.835 & 40.833 $\pm$10.858 & 43.561 $\pm$19.866 & 0.0302 $\pm$0.0016\tabularnewline
\hline 
K-NN & 0.733 $\pm$0.192 & 1057.212 $\pm$385.036 & 1875.849 $\pm$916.958 & 61.879 $\pm$12.366 & 68.747 $\pm$15.978 & 0 $\pm$0.0001\tabularnewline
\hline 
Random Forest & 0.876 $\pm$0.108 & 771.825 $\pm$326.339 & 1490.492 $\pm$930.576 & 45.399 $\pm$14.137 & 55.912 $\pm$31.703 & 0.0777 $\pm$0.0041\tabularnewline
\hline 
Linear Regression & 0.863 $\pm$0.118 & 922.334 $\pm$289.01 & 1399.844 $\pm$619.623 & 56.107 $\pm$16.171 & 56.425 $\pm$25.091 & 0.0045 $\pm$0.0006\tabularnewline
\hline 
SVM & 0.867 $\pm$0.1 & 873.222 $\pm$345.164 & 1621.478 $\pm$985.241 & 50.847 $\pm$11.83 & 57.232 $\pm$20.404 & 0.0423 $\pm$0.016\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-B: Developing prediction models to predict Total sales for
Editor-UDLW12 (The median selling editor)- 97 instances.

\begin{table}
\caption{Predicting Total sales for Editor-UDLW12 (2635 sales) }

\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.832 $\pm$0.24 & 14.179 $\pm$6.615 & 23.82 $\pm$11.86 & 43.405 $\pm$20.387 & 55.787 $\pm$30.474 & 0.0089 $\pm$0.0016\tabularnewline
\hline 
K-NN & 0.793 $\pm$0.236 & 15.34 $\pm$8.706 & 26.571 $\pm$16.635 & 46.2 $\pm$23.768 & 60.09 $\pm$35.205 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.858 $\pm$0.172 & 14.185 $\pm$6.692 & 23.569 $\pm$11.831 & 42.534 $\pm$16.799 & 53.295 $\pm$23.408 & 0.0321 $\pm$0.0044\tabularnewline
\hline 
Linear Regression & 0.821 $\pm$0.252 & 14.62 $\pm$7.917 & 24.793 $\pm$15.076 & 44.573 $\pm$24.265 & 56.854 $\pm$35.071 & 0.0019 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.835 $\pm$0.21 & 14.01 $\pm$7.608 & 25.612 $\pm$15.297 & 40.663 $\pm$16.026 & 53.276 $\pm$18.45 & 0.0198 $\pm$0.0146\tabularnewline
\hline 
\end{tabular}
\end{table}


%--------------------------------------------------------------------------

\subsection{Sales forecasting with feature selection}
\label{subsec:case_sales_feature_selection}

***TO DO***: choose the best Feature selection method and apply just this one (ReliefAttributeEval???). There are too many tables I think.
We could add an appendix or so with the rest of results if you want.


Case 1-A: Developing prediction models to predict Total sales for
Editor704 with ReliefFAttributeEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor704 (ReliefFAttributeEval-FS) }


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.893 $\pm$0.075 & 69.699 $\pm$19.853 & 122.48 $\pm$47.284 & 38.486 $\pm$11.531 & 46.677 $\pm$16.018 & 0.0275 $\pm$0.003\tabularnewline
\hline 
K-NN & 0.856 $\pm$0.101 & 78.393 $\pm$24.118 & 147.075 $\pm$58.023 & 42.577 $\pm$10.988 & 54.416 $\pm$15.542 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.877 $\pm$0.079 & 73.134 $\pm$22.949 & 137.604 $\pm$56.143 & 39.785 $\pm$10.689 & 51.348 $\pm$16.98 & 0.1282 $\pm$0.0137\tabularnewline
\hline 
Linear Regression & 0.873 $\pm$0.06 & 89.719 $\pm$18.306 & 140.653 $\pm$47.12 & 49.452 $\pm$10.495 & 53.196 $\pm$14.156 & 0.0008 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.877 $\pm$0.069 & 77.301 $\pm$22.249 & 146.462 $\pm$57.679 & 41.665 $\pm$7.994 & 53.098 $\pm$9.505 & 0.045 $\pm$0.0159\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-A: Developing prediction models to predict Total sales for
Editor704 with CfsSubsetEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor704 (CfsSubsetEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.801 $\pm$0.092 & 104.874 $\pm$21.376 & 174.571 $\pm$47.332 & 58.111 $\pm$14.731 & 67.577 $\pm$22.612 & 0.0178 $\pm$0.0028\tabularnewline
\hline 
K-NN & 0.782 $\pm$0.116 & 99.45 $\pm$24.305 & 170.827 $\pm$46.342 & 54.529 $\pm$12.277 & 65.864 $\pm$18.338 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.776 $\pm$0.128 & 101.933 $\pm$23.664 & 180.557 $\pm$51.203 & 56.098 $\pm$13.624 & 69.862 $\pm$23.274 & 0.0738 $\pm$0.003\tabularnewline
\hline 
Linear Regression & 0.801 $\pm$0.096 & 106.617 $\pm$21.523 & 173.179 $\pm$46.584 & 59.109 $\pm$14.958 & 67.527 $\pm$24.529 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.818 $\pm$0.075 & 94.285 $\pm$25.516 & 180.536 $\pm$62.67 & 50.792 $\pm$8.477 & 65.624 $\pm$6.245 & 0.0382 $\pm$0.0183\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-A: Developing prediction models to predict Total sales for
Editor704 with SOM feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor704 (SOM-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.892 $\pm$0.066 & 71.536 $\pm$18.953 & 123.061 $\pm$42.132 & 39.426 $\pm$10.597 & 47.204 $\pm$15.159 & 0.0307 $\pm$0.0067\tabularnewline
\hline 
K-NN & 0.858 $\pm$0.093 & 79.422 $\pm$21.473 & 140.948 $\pm$44.928 & 43.342 $\pm$10.072 & 53.313 $\pm$14.374 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.877 $\pm$0.08 & 73.981 $\pm$20.909 & 134.107 $\pm$51.698 & 40.475 $\pm$10.512 & 50.413 $\pm$16.378 & 0.1035 $\pm$0.0076\tabularnewline
\hline 
Linear Regression & 0.875 $\pm$0.062 & 88.697 $\pm$16.338 & 135.029 $\pm$35.176 & 49.076 $\pm$10.471 & 51.972 $\pm$13.648 & 0.0006 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.878 $\pm$0.057 & 81.087 $\pm$22.477 & 154.823 $\pm$59.93 & 43.651 $\pm$7.364 & 55.716 $\pm$7.153 & 0.0309 $\pm$0.0033\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-B: Developing prediction models to predict Total sales for
Editor-UDLU11 with ReliefFAttributeEval feature selection

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLU11 (ReliefFAttributeEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.766 $\pm$0.168 & 592.493 $\pm$209.249 & 894.757 $\pm$508.14 & 60.206 $\pm$18.221 & 69.138 $\pm$37.353 & 0.0166 $\pm$0.0011\tabularnewline
\hline 
K-NN & 0.827 $\pm$0.093 & 486.96 $\pm$140.273 & 741.808 $\pm$238.557 & 49.268 $\pm$10.504 & 56.62 $\pm$12.591 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.853 $\pm$0.089 & 455.7 $\pm$123.958 & 686.536 $\pm$198.157 & 46.319 $\pm$10.195 & 53.029 $\pm$13.054 & 0.07 $\pm$0.0028\tabularnewline
\hline 
Linear Regression & 0.644 $\pm$0.202 & 817.214 $\pm$331.164 & 1324.047 $\pm$1090.851 & 83.217 $\pm$32.006 & 101.895 $\pm$87.829 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.714 $\pm$0.142 & 644.424 $\pm$203.425 & 1009.656 $\pm$371.828 & 64.474 $\pm$11.701 & 75.542 $\pm$15.994 & 0.0182 $\pm$0.0012\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-B: Developing prediction models to predict Total sales for
Editor-UDLU11 with CfsSubsetEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLU11 (CfsSubsetEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.64 $\pm$0.232 & 777.935 $\pm$342.04 & 1305.28 $\pm$1075.291 & 78.373 $\pm$30.4 & 99.857 $\pm$85.431 & 0.0134 $\pm$0.0012\tabularnewline
\hline 
K-NN & 0.648 $\pm$0.181 & 702.701 $\pm$181.407 & 1008.391 $\pm$305.871 & 71.407 $\pm$13.951 & 78.469 $\pm$22.695 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.653 $\pm$0.193 & 685.556 $\pm$195.277 & 1021.374 $\pm$352.882 & 69.74 $\pm$16.956 & 79.373 $\pm$26.977 & 0.0466 $\pm$0.0023\tabularnewline
\hline 
Linear Regression & 0.67 $\pm$0.234 & 857.222 $\pm$308.686 & 1352.26 $\pm$945.314 & 86.095 $\pm$24.253 & 102.893 $\pm$74.43 & 0.0003 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.648 $\pm$0.23 & 879.912 $\pm$280.326 & 1402.822 $\pm$593.437 & 87.739 $\pm$15.273 & 105.402 $\pm$37.729 & 0.0168 $\pm$0.001\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 1-B: Developing prediction models to predict Total sales for
Editor-UDLU11 with SOM feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLU11 (SOM-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.79 $\pm$0.138 & 573.246 $\pm$167.882 & 827.554 $\pm$388.162 & 58.421 $\pm$14.138 & 63.975 $\pm$25.632 & 0.0153 $\pm$0.0015\tabularnewline
\hline 
K-NN & 0.836 $\pm$0.098 & 492.179 $\pm$126.856 & 727.326 $\pm$209.877 & 50.117 $\pm$10.688 & 56.136 $\pm$12.894 & 0 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.856 $\pm$0.084 & 446.245 $\pm$119.79 & 673.455 $\pm$189.06 & 45.384 $\pm$9.778 & 52.236 $\pm$13.088 & 0.0594 $\pm$0.0049\tabularnewline
\hline 
Linear Regression & 0.71 $\pm$0.188 & 723.735 $\pm$247.959 & 1108.74 $\pm$736.165 & 74.136 $\pm$24.906 & 85.956 $\pm$59.994 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.716 $\pm$0.147 & 640.136 $\pm$206.459 & 1017.018 $\pm$376.134 & 63.961 $\pm$11.873 & 76.103 $\pm$16.796 & 0.0189 $\pm$0.0014\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-A: Developing prediction models to predict Total sales for
Editor-UDL929 with ReliefFAttributeEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDL929 (ReliefFAttributeEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.905 $\pm$0.1 & 661.81 $\pm$186.171 & 1070.235 $\pm$425.658 & 40.176 $\pm$10.502 & 43.555 $\pm$19.769 & 0.0198 $\pm$0.0018\tabularnewline
\hline 
K-NN & 0.878 $\pm$0.114 & 751.542 $\pm$270.905 & 1257.821 $\pm$615.656 & 44.571 $\pm$10.886 & 48.616 $\pm$18.727 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.883 $\pm$0.11 & 735.044 $\pm$326.9 & 1420.43 $\pm$941.619 & 43.561 $\pm$15.883 & 54.357 $\pm$35.307 & 0.095 $\pm$0.0086\tabularnewline
\hline 
Linear Regression & 0.884 $\pm$0.113 & 868.179 $\pm$208.648 & 1231.227 $\pm$454.862 & 53.383 $\pm$14.672 & 50.598 $\pm$22.037 & 0.0006 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.876 $\pm$0.099 & 860.485 $\pm$334.669 & 1560.048 $\pm$962.637 & 49.978 $\pm$9.607 & 55.069 $\pm$17.762 & 0.0232 $\pm$0.0028\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-A: Developing prediction models to predict Total sales for
Editor-UDL929 with CfsSubsetEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDL929 (CfsSubsetEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.88 $\pm$0.126 & 738.28 $\pm$188.534 & 1141.786 $\pm$472.025 & 45.452 $\pm$13.622 & 47.58 $\pm$24.835 & 0.0169 $\pm$0.0016\tabularnewline
\hline 
K-NN & 0.853 $\pm$0.145 & 825.191 $\pm$235.337 & 1280.334 $\pm$523.058 & 50.228 $\pm$14.065 & 51.419 $\pm$21.24 & 0.0001 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.864 $\pm$0.138 & 790.09 $\pm$225.858 & 1245.227 $\pm$584.231 & 48.216 $\pm$13.792 & 50.01 $\pm$21.97 & 0.063 $\pm$0.0039\tabularnewline
\hline 
Linear Regression & 0.866 $\pm$0.123 & 936.557 $\pm$204.458 & 1329.237 $\pm$480.786 & 58.037 $\pm$16.921 & 55.408 $\pm$25.623 & 0.0004 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.868 $\pm$0.12 & 940.386 $\pm$344.161 & 1648.503 $\pm$1026.038 & 54.498 $\pm$7.122 & 56.166 $\pm$7.8 & 0.0201 $\pm$0.0021\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-A: Developing prediction models to predict Total sales for
Editor-UDL929 with SOM feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDL929 (SOM-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.897 $\pm$0.098 & 685.667 $\pm$185.988 & 1116.42 $\pm$478.062 & 42.092 $\pm$12.822 & 46.308 $\pm$24.612 & 0.02 $\pm$0.0035\tabularnewline
\hline 
K-NN & 0.89 $\pm$0.101 & 746.448 $\pm$250.762 & 1203.407 $\pm$564.836 & 44.361 $\pm$9.734 & 46.251 $\pm$14.413 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.895 $\pm$0.094 & 708.244 $\pm$264.518 & 1270.262 $\pm$708.817 & 42.135 $\pm$12.446 & 49.066 $\pm$26.594 & 0.0801 $\pm$0.0058\tabularnewline
\hline 
Linear Regression & 0.866 $\pm$0.129 & 894.084 $\pm$206.603 & 1282.833 $\pm$451.866 & 55.287 $\pm$15.923 & 53.738 $\pm$24.698 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.871 $\pm$0.101 & 908.562 $\pm$348.195 & 1608.767 $\pm$972.552 & 52.632 $\pm$8.746 & 55.945 $\pm$12.493 & 0.0221 $\pm$0.0024\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-B: Developing prediction models to predict Total sales for
Editor-UDLW12 with ReliefFAttributeEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLW12 (ReliefFAttributeEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.837 $\pm$0.241 & 13.853 $\pm$6.557 & 23.189 $\pm$11.795 & 42.353 $\pm$20.326 & 54.078 $\pm$30.055 & 0.0063 $\pm$0.0014\tabularnewline
\hline 
K-NN & 0.772 $\pm$0.296 & 14.178 $\pm$6.631 & 23.645 $\pm$11.029 & 43.367 $\pm$19.229 & 56.996 $\pm$30.59 & 0 $\pm$0.0001\tabularnewline
\hline 
Random Forest & 0.867 $\pm$0.161 & 13.628 $\pm$6.673 & 22.797 $\pm$11.806 & 41.066 $\pm$17.705 & 52.431 $\pm$26.232 & 0.0304 $\pm$0.0025\tabularnewline
\hline 
Linear Regression & 0.844 $\pm$0.246 & 12.789 $\pm$6.645 & 21.521 $\pm$12.543 & 39.467 $\pm$23.345 & 50.14 $\pm$35.261 & 0.0005 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.852 $\pm$0.187 & 13.217 $\pm$7.386 & 24.407 $\pm$15.087 & 38.24 $\pm$15.41 & 50.463 $\pm$17.973 & 0.0139 $\pm$0.0016\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-B: Developing prediction models to predict Total sales for
Editor-UDLW12 with CfsSubsetEval feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLW12 (CfsSubsetEval-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.816 $\pm$0.219 & 15.671 $\pm$6.584 & 25.632 $\pm$11.838 & 47.931 $\pm$18.812 & 60.719 $\pm$31.747 & 0.0059 $\pm$0.0015\tabularnewline
\hline 
K-NN & 0.752 $\pm$0.267 & 15.732 $\pm$7.726 & 26.68 $\pm$14.816 & 48.578 $\pm$24.769 & 66.231 $\pm$48.588 & 0.0001 $\pm$0.0003\tabularnewline
\hline 
Random Forest & 0.751 $\pm$0.264 & 15.61 $\pm$7.191 & 26.831 $\pm$13.355 & 47.381 $\pm$20.099 & 64.652 $\pm$38.664 & 0.0224 $\pm$0.0027\tabularnewline
\hline 
Linear Regression & 0.816 $\pm$0.218 & 15.674 $\pm$6.546 & 25.492 $\pm$11.628 & 47.97 $\pm$18.85 & 60.276 $\pm$30.617 & 0.0002 $\pm$0.0004\tabularnewline
\hline 
SVM & 0.818 $\pm$0.211 & 17.711 $\pm$9.704 & 31.951 $\pm$19.261 & 50.073 $\pm$14.926 & 65.121 $\pm$16.362 & 0.0129 $\pm$0.0009\tabularnewline
\hline 
\end{tabular}
\end{table}


Case 2-B: Developing prediction models to predict Total sales for
Editor-UDLW12 with SOM feature selection.

\begin{table}[h!]
\caption{Predicting Total sales for Editor-UDLW12 (SOM-FS)}


\centering{}%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline 
 & Corr. coefficient  & MAE & RMSE & RAE (\%) & RRSE (\%) & Time (s)\tabularnewline
\hline 
\hline 
M5P & 0.86 $\pm$0.188 & 13.84 $\pm$6.753 & 23.94 $\pm$11.638 & 41.65 $\pm$17.557 & 55.439 $\pm$27.128 & 0.006 $\pm$0.0012\tabularnewline
\hline 
K-NN & 0.74 $\pm$0.293 & 15.926 $\pm$8.521 & 28.281 $\pm$16.283 & 48.827 $\pm$25.885 & 68.75 $\pm$48.849 & 0 $\pm$0.0002\tabularnewline
\hline 
Random Forest & 0.855 $\pm$0.187 & 13.661 $\pm$6.469 & 22.51 $\pm$11.026 & 41.378 $\pm$17.87 & 52.198 $\pm$27.088 & 0.0262 $\pm$0.0019\tabularnewline
\hline 
Linear Regression & 0.854 $\pm$0.214 & 14.586 $\pm$7.129 & 24.404 $\pm$12.87 & 44.813 $\pm$23.647 & 57.727 $\pm$38.631 & 0.0003 $\pm$0.0005\tabularnewline
\hline 
SVM & 0.844 $\pm$0.184 & 14.177 $\pm$7.959 & 26.487 $\pm$16.539 & 40.931 $\pm$16.652 & 54.366 $\pm$20.004 & 0.0133 $\pm$0.0013\tabularnewline
\hline 
\end{tabular}
\end{table}


%--------------------------------------------------------------------------

\subsection{Decision trees for best seller}
\label{subsec:decision_trees}

In this section we show the output of one of the models built during the regression, namely we present one of the decision trees generated by M5P method, to demonstrate the value of the proposal to generate an useful decision-aid tool for and expert (editor) to see what sales can be expected for the different values of the input variables (including the print-run).

***TO DO:*** better explain this

We have considered the best seller editorial as a good example case to show.

Figure \ref{fig:decision_tree_best_seller} depicts the obtained decision tree for the best seller editorial (UDL929). The leaves correspond to linear regression models that can be computed (easily) to get the final predicted value for book sales. This tree has been created considering the 10 pre-sales features.

\begin{figure}[!ht] 
\begin{center}
  \epsfig{file=decision_tree_no_fs.eps,width=12cm}
\caption{Decision Tree built by the M5P method for the best seller editorial with all the pre-sales features.}
\label{fig:decision_tree_best_seller}
\end{center}
\end{figure}

***TO DO***: comments on the figure\\

Figure \ref{fig:decision_tree_best_seller_fs} shows the decision tree obtained for the same editorial (the same data, but just considering the selected features by ReliefAttributeEval method, since it is the one which yields the best results.

***TO DO***: better justify this\\

\begin{figure}[!ht] 
\begin{center}
  \epsfig{file=decision_tree_fs.eps,width=12cm}
\caption{Decision Tree built by the M5P method for the best seller editorial with the features selected by the ReliefAttributeEval method.}
\label{fig:decision_tree_best_seller_fs}
\end{center}
\end{figure}

***TO DO***: comments on the figure\\

%********************************************************************************
\section{Conclusions and Future Work}
\label{sec:conclusionsAndFutureWork}

***TO DO***: rewrite these conclusions with the new experiments, examples and results\\

In this paper, the problem of how many books should be printed 
when a new book is published is faced using several data-mining and
forecasting methods.

This is a very challenging problem in this industry, because printing a higher 
number of volumes than those finally sold will lead to losses, 
while printing an adequate number of copies will optimise sales and company earnings.
In addition, there are several difficulties inherent to the new book sales forecasting, 
such as dealing with limited data, finding adequate forecasting methods, and 
selecting the best method to use.

%La investigación descrita en este trabajo se ha llevado a cabo conjuntamente con una empresa editorial que ha facilitado un conjunto de datos con los que trabajar, así como con la ayuda de un experto en ventas de este campo para validar los resultados obtenidos.
A dataset consisting of 3159 books provided by the Spanish %FERGU: mover este párrafo debajo del siguiente
publishing company Trevenque Editorial S.L. has been used.

In this paper, a data visualisation method has been used to find out what are 
the relevant variables describing a book in the prediction of book sales.
Then, several standard data mining models for sales forecasting have been used 
to forecast new book sales, and from them deduce print-run, based on those 
variables.

The most relevant variables when predicting sales have been determined, 
while unrelated or superfluous ones have been removed, 
and sales predictions have been obtained using five different forecasting 
standard methods.
Not only the obtained forecasting error has been taking into account to evaluate
those methods, but also the cost of model building as parameter setting and 
the required time to train the model.

In this sense, the decision trees proved to be a suitable model because of the 
ease to explain how the prediction is obtained from the variables that describe 
the new book. 
This, compared to the black-box models such as the artificial neural network, 
was decisive from the point of view of the needs of experts and Trevenque company.
Moreover, proposed method was validated by the company and incorporated in a 
business intelligence tool for publishing houses. %FERGU: no hay más info de esto? estaría guay describirlo

%FERGU: no mencionais el SOM en las conclusiones

Overall, this study contributes to the literature by proposing the use of different 
soft-computing standard techniques to solve a new challenging problem.
Moreover, proposed method could be implemented, not only in the editorial 
industry, but also in other domains where the specificity of products is similar, 
and might be of interest to other academic researches and industrial practitioners.


%---------- trabajo futuro

As future work, it would be interesting to compare the obtained forecasting results 
with other methods based on soft-computing, such as genetic programming or fuzzy logic.
Moreover, as default parameters have been used for proposed forecasting methods,
it would be of interest carrying out some parameter tuning procedure in order to
improve obtained results.

Furthermore, since there are some characteristics that may affect product sales 
significantly, another way to improve sales predictions could be making a thorough 
study of the effect of the application of discounts and promotions. 

Finally, there are some variables over which the published has some
control once the book has been published, such as the initial print
run or the books given out as gifts to reviewers or what proportion is
going to be sent to each possible point of sale. This can be
incorporated into an optimisation model that optimises those values to
obtain desired sales. This is left as future work.


%********************************************************************************
\section*{Acknowledgements}

***TO DO***: revise the projects\\

This work has been supported in part by projects 
PreTEL (PRM Consultores - Trevenque S.L.),
TIN2014-56494-C4-3-P (Spanish Ministry of Economy and Competitiveness), 
SPIP2014-01437 (Direcci{\'o}n General de Tr{\'a}fico), 
PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de Investigaci{\'o}n), 
PROY-PP2015-06 (Plan Propio 2015 UGR), 
and project CEI2015-MP-V17 of the Microprojects program 2015 from CEI BioTIC Granada.

%********************************************************************************
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% End:
